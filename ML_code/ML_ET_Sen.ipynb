{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import defaultdict \n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is to train a physics based machine learning model on Volk et al benchmark ET but with a twist. \n",
    "We are using a merging of SEBAL and Sentinel-1 radar backscatter data\n",
    "- We will first qulaity control the data based on the model outputs \n",
    "- In this model we will not use H, LE and G as inputs because they contain the most amount of errors(We can add them if required because the number of datapoints has drastically reduced) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the datasets, remoeve shady values and perform a random forest \n",
    "os.chdir(\"D:\\\\Backup\\\\Rouhin_Lenovo\\\\US_project\\\\Untitled_Folder\\\\Data\\\\Volk_merged\\\\ML_datasets\\\\Volk_SEBLA_Sen5\\\\\")\n",
    "file_list=os.listdir()\n",
    "et_data=[]\n",
    "for i in range(len(file_list)):\n",
    "    et_data.append(pd.read_csv(file_list[i],parse_dates=[\"Date\"]))\n",
    "    et_data[i]=et_data[i][et_data[i][\"Hinst\"]<1500]\n",
    "    et_data[i]=et_data[i][et_data[i][\"LEinst\"]>=0]\n",
    "    et_data[i][\"Gap\"]=et_data[i][\"Gap\"].replace(np.NAN,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2878"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat(et_data).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29, 3, 3, 4, 12, 8, 1, 10, 5, 3]\n"
     ]
    }
   ],
   "source": [
    "## Instead on concatenating the data we will split the data clustered on landcover (This has an issue that we might be using spatial autocorrelated datasets for predction)\n",
    "stations_lc=[]\n",
    "count=[]\n",
    "for i in pd.concat(et_data)[\"Land_cover_volk\"].unique():\n",
    "    counter=0\n",
    "    for st in range(len(et_data)):\n",
    "        if i==et_data[st][\"Land_cover_volk\"].iloc[0]:\n",
    "            counter=counter+1\n",
    "    count.append(counter)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before shuffling\n",
      "defaultdict(<class 'list'>, {'Annual crops': ['ALARC2_Smith6', 'Ellendale', 'JPL1_Smith5', 'LYS_NE', 'LYS_NW', 'LYS_SE', 'LYS_SW', 'manilacotton', 'S2', 'stonevillesoy', 'UA1_HartFarm', 'UA1_JV187', 'UA3_JV108', 'US-A74', 'US-ARM', 'US-Bi1', 'US-Bi2', 'US-IB1', 'US-KLS', 'US-MC1', 'US-OF1', 'US-OF2', 'US-Ro1', 'US-Ro2', 'US-Ro5', 'US-Ro6', 'US-Tw3', 'US-Twt', 'US-xSL'], 'Orchards': ['Almond_High', 'Almond_Low', 'Almond_Med'], 'Vineyards': ['BAR012', 'RIP760', 'SLM001'], 'Vegetable crops': ['JPL1_JV114', 'UA1_KN18', 'UA2_KN20', 'UA3_KN15'], 'Grasslands': ['US-A32', 'US-Hn2', 'US-IB2', 'US-KM4', 'US-Ro4', 'US-SRG', 'US-Var', 'US-Wkg', 'US-xAE', 'US-xDC', 'US-xDS', 'US-xNG'], 'Shrublands': ['US-ADR', 'US-Hn3', 'US-Jo2', 'US-Rwf', 'US-Rws', 'US-SRM', 'US-SRS', 'US-xJR'], 'Riparian': ['US-CMW'], 'Evergreen Forests': ['US-CZ3', 'US-GLE', 'US-Me2', 'US-Me6', 'US-NC2', 'US-NC3', 'US-NR1', 'US-xRM', 'US-xSB', 'US-xYE'], 'Mixed Forests': ['US-MOz', 'US-WCr', 'US-xDL', 'US-xST', 'US-xUN'], 'Wetlands': ['US-NC4', 'US-Sne', 'US-Srr']})\n",
      "defaultdict(<class 'list'>, {'Annual crops': ['US-Bi2', 'US-MC1', 'US-IB1', 'US-xSL', 'LYS_SE', 'US-ARM', 'US-A74', 'US-Ro1', 'stonevillesoy', 'US-Ro2', 'LYS_SW', 'LYS_NE', 'UA3_JV108', 'manilacotton', 'US-OF1', 'US-OF2', 'UA1_HartFarm', 'UA1_JV187', 'US-Ro5', 'US-Twt', 'LYS_NW', 'ALARC2_Smith6', 'Ellendale', 'S2', 'US-Bi1', 'JPL1_Smith5', 'US-Tw3', 'US-KLS', 'US-Ro6'], 'Orchards': ['Almond_High', 'Almond_Med', 'Almond_Low'], 'Vineyards': ['SLM001', 'RIP760', 'BAR012'], 'Vegetable crops': ['JPL1_JV114', 'UA2_KN20', 'UA1_KN18', 'UA3_KN15'], 'Grasslands': ['US-xDC', 'US-Var', 'US-SRG', 'US-IB2', 'US-Wkg', 'US-xAE', 'US-A32', 'US-xDS', 'US-Ro4', 'US-KM4', 'US-Hn2', 'US-xNG'], 'Shrublands': ['US-xJR', 'US-SRS', 'US-Rws', 'US-Rwf', 'US-Hn3', 'US-Jo2', 'US-SRM', 'US-ADR'], 'Riparian': ['US-CMW'], 'Evergreen Forests': ['US-Me2', 'US-Me6', 'US-CZ3', 'US-xRM', 'US-NR1', 'US-NC3', 'US-NC2', 'US-xSB', 'US-GLE', 'US-xYE'], 'Mixed Forests': ['US-MOz', 'US-xUN', 'US-xST', 'US-WCr', 'US-xDL'], 'Wetlands': ['US-Sne', 'US-Srr', 'US-NC4']})\n"
     ]
    }
   ],
   "source": [
    "## Now let's try to create a dictionary of Site ID and land cover so that we can trunce around the data \n",
    "site_id=[]\n",
    "landcover=[]\n",
    "for i in range(len(et_data)):\n",
    "    site_id.append(et_data[i][\"Site_name\"].iloc[0])\n",
    "    landcover.append(et_data[i][\"Land_cover_volk\"].iloc[0])\n",
    "station_landcover_dict  = {k: v for k, v in zip(site_id, landcover)}\n",
    "# Group stations by landcover\n",
    "landcover_groups = defaultdict(list)\n",
    "for station, landcover in station_landcover_dict.items():\n",
    "    landcover_groups[landcover].append(station)\n",
    "print(\"Before shuffling\")\n",
    "print(landcover_groups)\n",
    "## Set random seed\n",
    "random.seed(6)\n",
    "for landcover, stations in landcover_groups.items():\n",
    "    random.shuffle(stations)\n",
    "print(landcover_groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Val stations= 16\n",
      "No of Train stations= 62\n",
      "no of training points= 2164\n",
      "no of val points= 714\n"
     ]
    }
   ],
   "source": [
    "# Initialize the final training and validation sets\n",
    "train_stations = []\n",
    "val_stations = []\n",
    "\n",
    "# Include at least one station of each landcover in the validation set\n",
    "for landcover, stations in landcover_groups.items():\n",
    "    val_stations.extend(stations[:len(stations)//4])  # Add 25% of stations to validation set\n",
    "    train_stations.extend(stations[len(stations)//4:]) # Add the rest to training set\n",
    "\n",
    "# Shuffle the final training and validation sets\n",
    "# random.shuffle(train_stations)\n",
    "# random.shuffle(val_stations)\n",
    "print(\"No of Val stations=\",len(val_stations))\n",
    "print(\"No of Train stations=\",len(train_stations))\n",
    "## Create workflow to re read the train and validation datasets \n",
    "train,val=[],[]\n",
    "for i in range(len(et_data)):\n",
    "    for j in train_stations:\n",
    "        if et_data[i][\"Site_name\"].iloc[0]==j:\n",
    "            train.append(et_data[i])\n",
    "for i in range(len(et_data)):\n",
    "    for j in val_stations:\n",
    "        if et_data[i][\"Site_name\"].iloc[0]==j:\n",
    "            val.append(et_data[i])\n",
    "print(\"no of training points=\",pd.concat(train).shape[0])\n",
    "print(\"no of val points=\",pd.concat(val).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[5].Gap.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Unnamed: 0_x', 'Date', 'B', 'R', 'GR', 'NIR', 'SWIR_1',\n",
       "       'SWIR_2', 'ST_B10', 'NDVI', 'NDWI', 'ALFA', 'Tao_sw_1', 'Rs_down',\n",
       "       'Rl_down', 'Rl_up', 'Rn', 'Ginst', 'Hinst', 'LEinst', 'LE_closed',\n",
       "       'LE_Daily_model', 'Rn24h_G', 'AirT_G', 'RH_G', 'ux_G', 'Lat_volk',\n",
       "       'Lon_volk', 'Elev_volk', 'Land_cover_volk', 'Site_name', 'Unnamed: 0_y',\n",
       "       'longitude', 'latitude', 'date', 'VV', 'VH', 'angle', 'Gap', 'VV_norm',\n",
       "       'VH_norm', 'VV_red', 'VV_green', 'VV_blue', 'VV_NIR', 'VV_SWIR1',\n",
       "       'VV_SWIR2', 'VV_NDVI', 'VV_NDWI', 'VH_red', 'VH_green', 'VH_blue',\n",
       "       'VH_NIR', 'VH_SWIR1', 'VH_SWIR2', 'VH_NDVI', 'VH_NDWI', 'VH_LST',\n",
       "       'VV_LST', 'VH-VV'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(714,)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "## Split data into features and labels\n",
    "features=['ALFA', 'Tao_sw_1', 'Rs_down',\n",
    "       'Rl_down', 'Rl_up', 'Rn','VV_norm', \n",
    "       'VH_norm', 'VV_red', 'VV_green', 'VV_blue', 'VV_NIR', 'VV_SWIR1',\n",
    "       'VV_SWIR2', 'VV_NDVI', 'VV_NDWI', 'VH_red', 'VH_green', 'VH_blue',\n",
    "       'VH_NIR', 'VH_SWIR1', 'VH_SWIR2', 'VH_NDVI', 'VH_NDWI', 'VH_LST',\n",
    "       'VV_LST', 'VH-VV',\"Ginst\",\"Hinst\",\"LE_Daily_model\"]\n",
    "labels=[\"LE_closed\"]\n",
    "## Create a train set to include the geesebal output \n",
    "train_features_all=pd.concat(train)[features].dropna()\n",
    "test_features_all=pd.concat(val)[features].dropna()\n",
    "train_features=pd.concat(train)[features].dropna().drop(columns=\"LE_Daily_model\")\n",
    "train_features\n",
    "test_features=pd.concat(val)[features].dropna().drop(columns=\"LE_Daily_model\")\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_features)\n",
    "X_test = scaler.transform(test_features)\n",
    "# ## labels\n",
    "train_labels=np.array(pd.concat(train)[labels])\n",
    "test_labels=np.array(pd.concat(val)[labels]).squeeze()\n",
    "\n",
    "train_features_all.shape\n",
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(714, 30)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_all.shape\n",
    "test_features_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7986345210873578"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import root_mean_squared_error \n",
    "def rmse(Y_actual,Y_Predicted):\n",
    "    return root_mean_squared_error(Y_actual,Y_Predicted)\n",
    "def MAPE(Y_actual,Y_Predicted):\n",
    "    mape = np.mean(( Y_Predicted-Y_actual))*100/np.mean(Y_actual)\n",
    "    return mape\n",
    "rmse(pd.concat(train)[(pd.concat(train)[\"LEinst\"].notna())][\"LE_closed\"],pd.concat(train)[(pd.concat(train)[\"LEinst\"].notna())][\"LE_Daily_model\"])/28.36\n",
    "# MAPE(ml_df[(ml_df[\"LEinst\"].notna())][\"LE_inst_af\"],ml_df[(ml_df[\"LEinst\"].notna())][\"LEinst\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "11/11 [==============================] - 2s 4ms/step - loss: 12240.0137 - mean_squared_error: 12240.0137 - root_mean_squared_error: 110.6346\n",
      "Epoch 2/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 5322.2510 - mean_squared_error: 5322.2510 - root_mean_squared_error: 72.9538\n",
      "Epoch 3/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 3002.2170 - mean_squared_error: 3002.2170 - root_mean_squared_error: 54.7925\n",
      "Epoch 4/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 2296.7070 - mean_squared_error: 2296.7070 - root_mean_squared_error: 47.9240\n",
      "Epoch 5/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1964.3049 - mean_squared_error: 1964.3049 - root_mean_squared_error: 44.3205\n",
      "Epoch 6/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1682.9037 - mean_squared_error: 1682.9037 - root_mean_squared_error: 41.0232\n",
      "Epoch 7/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1481.3630 - mean_squared_error: 1481.3630 - root_mean_squared_error: 38.4885\n",
      "Epoch 8/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1376.0131 - mean_squared_error: 1376.0131 - root_mean_squared_error: 37.0947\n",
      "Epoch 9/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1283.5597 - mean_squared_error: 1283.5597 - root_mean_squared_error: 35.8268\n",
      "Epoch 10/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1178.5801 - mean_squared_error: 1178.5801 - root_mean_squared_error: 34.3305\n",
      "Epoch 11/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1153.2614 - mean_squared_error: 1153.2614 - root_mean_squared_error: 33.9597\n",
      "Epoch 12/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1133.2721 - mean_squared_error: 1133.2721 - root_mean_squared_error: 33.6641\n",
      "Epoch 13/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1046.4761 - mean_squared_error: 1046.4761 - root_mean_squared_error: 32.3493\n",
      "Epoch 14/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 987.0881 - mean_squared_error: 987.0881 - root_mean_squared_error: 31.4180\n",
      "Epoch 15/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 950.0862 - mean_squared_error: 950.0862 - root_mean_squared_error: 30.8235\n",
      "Epoch 16/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 904.7346 - mean_squared_error: 904.7346 - root_mean_squared_error: 30.0788\n",
      "Epoch 17/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 993.9591 - mean_squared_error: 993.9590 - root_mean_squared_error: 31.5271\n",
      "Epoch 18/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 857.0036 - mean_squared_error: 857.0036 - root_mean_squared_error: 29.2746\n",
      "Epoch 19/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 805.8126 - mean_squared_error: 805.8126 - root_mean_squared_error: 28.3868\n",
      "Epoch 20/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 801.1763 - mean_squared_error: 801.1763 - root_mean_squared_error: 28.3051\n",
      "Epoch 21/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 781.3069 - mean_squared_error: 781.3069 - root_mean_squared_error: 27.9519\n",
      "Epoch 22/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 786.4364 - mean_squared_error: 786.4364 - root_mean_squared_error: 28.0435\n",
      "Epoch 23/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 707.2096 - mean_squared_error: 707.2096 - root_mean_squared_error: 26.5934\n",
      "Epoch 24/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 732.2399 - mean_squared_error: 732.2399 - root_mean_squared_error: 27.0599\n",
      "Epoch 25/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 712.4837 - mean_squared_error: 712.4837 - root_mean_squared_error: 26.6924\n",
      "Epoch 26/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 679.9682 - mean_squared_error: 679.9682 - root_mean_squared_error: 26.0762\n",
      "Epoch 27/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 860.7319 - mean_squared_error: 860.7319 - root_mean_squared_error: 29.3382\n",
      "Epoch 28/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 627.9524 - mean_squared_error: 627.9524 - root_mean_squared_error: 25.0590\n",
      "Epoch 29/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 595.6353 - mean_squared_error: 595.6353 - root_mean_squared_error: 24.4056\n",
      "Epoch 30/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 620.1979 - mean_squared_error: 620.1979 - root_mean_squared_error: 24.9038\n",
      "Epoch 31/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 605.1588 - mean_squared_error: 605.1588 - root_mean_squared_error: 24.6000\n",
      "Epoch 32/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 648.9379 - mean_squared_error: 648.9379 - root_mean_squared_error: 25.4743\n",
      "Epoch 33/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 570.2166 - mean_squared_error: 570.2166 - root_mean_squared_error: 23.8792\n",
      "Epoch 34/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 519.4730 - mean_squared_error: 519.4730 - root_mean_squared_error: 22.7920\n",
      "Epoch 35/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 568.0364 - mean_squared_error: 568.0364 - root_mean_squared_error: 23.8335\n",
      "Epoch 36/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 497.4172 - mean_squared_error: 497.4172 - root_mean_squared_error: 22.3029\n",
      "Epoch 37/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 477.7838 - mean_squared_error: 477.7838 - root_mean_squared_error: 21.8583\n",
      "Epoch 38/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 493.8182 - mean_squared_error: 493.8182 - root_mean_squared_error: 22.2220\n",
      "Epoch 39/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 449.2523 - mean_squared_error: 449.2523 - root_mean_squared_error: 21.1956\n",
      "Epoch 40/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 400.7324 - mean_squared_error: 400.7323 - root_mean_squared_error: 20.0183\n",
      "Epoch 41/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 576.6326 - mean_squared_error: 576.6326 - root_mean_squared_error: 24.0132\n",
      "Epoch 42/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 516.9587 - mean_squared_error: 516.9587 - root_mean_squared_error: 22.7367\n",
      "Epoch 43/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 393.7464 - mean_squared_error: 393.7464 - root_mean_squared_error: 19.8430\n",
      "Epoch 44/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 429.2005 - mean_squared_error: 429.2005 - root_mean_squared_error: 20.7172\n",
      "Epoch 45/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 377.1118 - mean_squared_error: 377.1118 - root_mean_squared_error: 19.4194\n",
      "Epoch 46/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 453.2863 - mean_squared_error: 453.2863 - root_mean_squared_error: 21.2905\n",
      "Epoch 47/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 338.0158 - mean_squared_error: 338.0158 - root_mean_squared_error: 18.3852\n",
      "Epoch 48/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 386.3808 - mean_squared_error: 386.3808 - root_mean_squared_error: 19.6566\n",
      "Epoch 49/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 307.2914 - mean_squared_error: 307.2914 - root_mean_squared_error: 17.5297\n",
      "Epoch 50/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 341.4819 - mean_squared_error: 341.4819 - root_mean_squared_error: 18.4792\n",
      "Epoch 51/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 381.0393 - mean_squared_error: 381.0393 - root_mean_squared_error: 19.5202\n",
      "Epoch 52/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 419.1100 - mean_squared_error: 419.1100 - root_mean_squared_error: 20.4722\n",
      "Epoch 53/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 300.8713 - mean_squared_error: 300.8713 - root_mean_squared_error: 17.3456\n",
      "Epoch 54/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 305.7511 - mean_squared_error: 305.7511 - root_mean_squared_error: 17.4857\n",
      "Epoch 55/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 321.4123 - mean_squared_error: 321.4123 - root_mean_squared_error: 17.9280\n",
      "Epoch 56/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 323.4282 - mean_squared_error: 323.4282 - root_mean_squared_error: 17.9841\n",
      "Epoch 57/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 257.7867 - mean_squared_error: 257.7867 - root_mean_squared_error: 16.0557\n",
      "Epoch 58/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 230.8171 - mean_squared_error: 230.8171 - root_mean_squared_error: 15.1927\n",
      "Epoch 59/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 335.8055 - mean_squared_error: 335.8055 - root_mean_squared_error: 18.3250\n",
      "Epoch 60/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 265.0112 - mean_squared_error: 265.0112 - root_mean_squared_error: 16.2792\n",
      "Epoch 61/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 330.2714 - mean_squared_error: 330.2714 - root_mean_squared_error: 18.1734\n",
      "Epoch 62/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 285.6929 - mean_squared_error: 285.6929 - root_mean_squared_error: 16.9025\n",
      "Epoch 63/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 258.5607 - mean_squared_error: 258.5607 - root_mean_squared_error: 16.0798\n",
      "Epoch 64/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 253.0995 - mean_squared_error: 253.0995 - root_mean_squared_error: 15.9091\n",
      "Epoch 65/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 201.9637 - mean_squared_error: 201.9637 - root_mean_squared_error: 14.2114\n",
      "Epoch 66/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 254.1255 - mean_squared_error: 254.1255 - root_mean_squared_error: 15.9413\n",
      "Epoch 67/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 193.2834 - mean_squared_error: 193.2834 - root_mean_squared_error: 13.9026\n",
      "Epoch 68/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 249.3096 - mean_squared_error: 249.3096 - root_mean_squared_error: 15.7895\n",
      "Epoch 69/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 195.8969 - mean_squared_error: 195.8969 - root_mean_squared_error: 13.9963\n",
      "Epoch 70/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 204.4148 - mean_squared_error: 204.4148 - root_mean_squared_error: 14.2974\n",
      "Epoch 71/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 153.1772 - mean_squared_error: 153.1772 - root_mean_squared_error: 12.3765\n",
      "Epoch 72/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 138.3634 - mean_squared_error: 138.3634 - root_mean_squared_error: 11.7628\n",
      "Epoch 73/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 294.3240 - mean_squared_error: 294.3240 - root_mean_squared_error: 17.1559\n",
      "Epoch 74/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 246.8025 - mean_squared_error: 246.8025 - root_mean_squared_error: 15.7099\n",
      "Epoch 75/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 225.5046 - mean_squared_error: 225.5046 - root_mean_squared_error: 15.0168\n",
      "Epoch 76/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 154.9567 - mean_squared_error: 154.9567 - root_mean_squared_error: 12.4482\n",
      "Epoch 77/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 223.9406 - mean_squared_error: 223.9406 - root_mean_squared_error: 14.9646\n",
      "Epoch 78/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 214.5235 - mean_squared_error: 214.5235 - root_mean_squared_error: 14.6466\n",
      "Epoch 79/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 132.4758 - mean_squared_error: 132.4758 - root_mean_squared_error: 11.5098\n",
      "Epoch 80/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 157.2562 - mean_squared_error: 157.2562 - root_mean_squared_error: 12.5402\n",
      "Epoch 81/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 148.0835 - mean_squared_error: 148.0835 - root_mean_squared_error: 12.1690\n",
      "Epoch 82/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 248.5166 - mean_squared_error: 248.5166 - root_mean_squared_error: 15.7644\n",
      "Epoch 83/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 128.8498 - mean_squared_error: 128.8498 - root_mean_squared_error: 11.3512\n",
      "Epoch 84/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 112.2669 - mean_squared_error: 112.2669 - root_mean_squared_error: 10.5956\n",
      "Epoch 85/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 106.9122 - mean_squared_error: 106.9122 - root_mean_squared_error: 10.3398\n",
      "Epoch 86/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 124.4475 - mean_squared_error: 124.4475 - root_mean_squared_error: 11.1556\n",
      "Epoch 87/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 106.9203 - mean_squared_error: 106.9203 - root_mean_squared_error: 10.3402\n",
      "Epoch 88/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 146.0259 - mean_squared_error: 146.0259 - root_mean_squared_error: 12.0841\n",
      "Epoch 89/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 352.6332 - mean_squared_error: 352.6332 - root_mean_squared_error: 18.7785\n",
      "Epoch 90/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 141.0584 - mean_squared_error: 141.0584 - root_mean_squared_error: 11.8768\n",
      "Epoch 91/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 125.9664 - mean_squared_error: 125.9664 - root_mean_squared_error: 11.2235\n",
      "Epoch 92/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 102.7694 - mean_squared_error: 102.7694 - root_mean_squared_error: 10.1375\n",
      "Epoch 93/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 128.8152 - mean_squared_error: 128.8152 - root_mean_squared_error: 11.3497\n",
      "Epoch 94/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 119.9139 - mean_squared_error: 119.9139 - root_mean_squared_error: 10.9505\n",
      "Epoch 95/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 81.5227 - mean_squared_error: 81.5227 - root_mean_squared_error: 9.0290\n",
      "Epoch 96/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 216.6056 - mean_squared_error: 216.6056 - root_mean_squared_error: 14.7175\n",
      "Epoch 97/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 104.2881 - mean_squared_error: 104.2881 - root_mean_squared_error: 10.2122\n",
      "Epoch 98/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 94.6433 - mean_squared_error: 94.6433 - root_mean_squared_error: 9.7285\n",
      "Epoch 99/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 130.7754 - mean_squared_error: 130.7754 - root_mean_squared_error: 11.4357\n",
      "Epoch 100/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 157.6841 - mean_squared_error: 157.6841 - root_mean_squared_error: 12.5572\n",
      "Epoch 101/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 79.0848 - mean_squared_error: 79.0848 - root_mean_squared_error: 8.8930\n",
      "Epoch 102/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 68.3810 - mean_squared_error: 68.3810 - root_mean_squared_error: 8.2693\n",
      "Epoch 103/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 194.6921 - mean_squared_error: 194.6921 - root_mean_squared_error: 13.9532\n",
      "Epoch 104/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 94.7410 - mean_squared_error: 94.7410 - root_mean_squared_error: 9.7335\n",
      "Epoch 105/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 106.0887 - mean_squared_error: 106.0887 - root_mean_squared_error: 10.2999\n",
      "Epoch 106/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 107.1393 - mean_squared_error: 107.1393 - root_mean_squared_error: 10.3508\n",
      "Epoch 107/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 93.5771 - mean_squared_error: 93.5771 - root_mean_squared_error: 9.6735\n",
      "Epoch 108/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 68.5212 - mean_squared_error: 68.5212 - root_mean_squared_error: 8.2778\n",
      "Epoch 109/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 186.5915 - mean_squared_error: 186.5915 - root_mean_squared_error: 13.6598\n",
      "Epoch 110/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 73.0706 - mean_squared_error: 73.0706 - root_mean_squared_error: 8.5481\n",
      "Epoch 111/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 66.2939 - mean_squared_error: 66.2939 - root_mean_squared_error: 8.1421\n",
      "Epoch 112/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 61.8439 - mean_squared_error: 61.8439 - root_mean_squared_error: 7.8641\n",
      "Epoch 113/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 72.1953 - mean_squared_error: 72.1953 - root_mean_squared_error: 8.4968\n",
      "Epoch 114/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 233.2971 - mean_squared_error: 233.2971 - root_mean_squared_error: 15.2741\n",
      "Epoch 115/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 82.6241 - mean_squared_error: 82.6241 - root_mean_squared_error: 9.0898\n",
      "Epoch 116/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 69.7947 - mean_squared_error: 69.7947 - root_mean_squared_error: 8.3543\n",
      "Epoch 117/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 78.3902 - mean_squared_error: 78.3902 - root_mean_squared_error: 8.8538\n",
      "Epoch 118/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 94.5489 - mean_squared_error: 94.5489 - root_mean_squared_error: 9.7236\n",
      "Epoch 119/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 53.4948 - mean_squared_error: 53.4948 - root_mean_squared_error: 7.3140\n",
      "Epoch 120/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 158.7068 - mean_squared_error: 158.7068 - root_mean_squared_error: 12.5979\n",
      "Epoch 121/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 96.2148 - mean_squared_error: 96.2148 - root_mean_squared_error: 9.8089\n",
      "Epoch 122/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 93.0018 - mean_squared_error: 93.0018 - root_mean_squared_error: 9.6437\n",
      "Epoch 123/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 53.8404 - mean_squared_error: 53.8404 - root_mean_squared_error: 7.3376\n",
      "Epoch 124/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 42.3605 - mean_squared_error: 42.3605 - root_mean_squared_error: 6.5085\n",
      "Epoch 125/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 145.3959 - mean_squared_error: 145.3959 - root_mean_squared_error: 12.0580\n",
      "Epoch 126/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 55.6702 - mean_squared_error: 55.6702 - root_mean_squared_error: 7.4612\n",
      "Epoch 127/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 46.5389 - mean_squared_error: 46.5389 - root_mean_squared_error: 6.8219\n",
      "Epoch 128/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 168.3159 - mean_squared_error: 168.3159 - root_mean_squared_error: 12.9737\n",
      "Epoch 129/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 56.7544 - mean_squared_error: 56.7544 - root_mean_squared_error: 7.5336\n",
      "Epoch 130/500\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 48.8676 - mean_squared_error: 48.8676 - root_mean_squared_error: 6.9905\n",
      "Epoch 131/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 42.5371 - mean_squared_error: 42.5371 - root_mean_squared_error: 6.5220\n",
      "Epoch 132/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 131.6956 - mean_squared_error: 131.6956 - root_mean_squared_error: 11.4759\n",
      "Epoch 133/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 108.7131 - mean_squared_error: 108.7131 - root_mean_squared_error: 10.4266\n",
      "Epoch 134/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 59.3355 - mean_squared_error: 59.3356 - root_mean_squared_error: 7.7030\n",
      "Epoch 135/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 58.4745 - mean_squared_error: 58.4745 - root_mean_squared_error: 7.6469\n",
      "Epoch 136/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 37.7654 - mean_squared_error: 37.7654 - root_mean_squared_error: 6.1454\n",
      "Epoch 137/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 41.0792 - mean_squared_error: 41.0792 - root_mean_squared_error: 6.4093\n",
      "Epoch 138/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 31.9114 - mean_squared_error: 31.9114 - root_mean_squared_error: 5.6490\n",
      "Epoch 139/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 78.4744 - mean_squared_error: 78.4744 - root_mean_squared_error: 8.8586\n",
      "Epoch 140/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 182.3059 - mean_squared_error: 182.3059 - root_mean_squared_error: 13.5021\n",
      "Epoch 141/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 59.2051 - mean_squared_error: 59.2051 - root_mean_squared_error: 7.6945\n",
      "Epoch 142/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 209.5973 - mean_squared_error: 209.5973 - root_mean_squared_error: 14.4775\n",
      "Epoch 143/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 66.5777 - mean_squared_error: 66.5777 - root_mean_squared_error: 8.1595\n",
      "Epoch 144/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 43.5841 - mean_squared_error: 43.5841 - root_mean_squared_error: 6.6018\n",
      "Epoch 145/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 43.0885 - mean_squared_error: 43.0885 - root_mean_squared_error: 6.5642\n",
      "Epoch 146/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 41.5442 - mean_squared_error: 41.5442 - root_mean_squared_error: 6.4455\n",
      "Epoch 147/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 32.5662 - mean_squared_error: 32.5662 - root_mean_squared_error: 5.7067\n",
      "Epoch 148/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 38.3163 - mean_squared_error: 38.3163 - root_mean_squared_error: 6.1900\n",
      "Epoch 149/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 97.7579 - mean_squared_error: 97.7579 - root_mean_squared_error: 9.8873\n",
      "Epoch 150/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 75.7092 - mean_squared_error: 75.7092 - root_mean_squared_error: 8.7011\n",
      "Epoch 151/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 34.5642 - mean_squared_error: 34.5642 - root_mean_squared_error: 5.8791\n",
      "Epoch 152/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 41.5870 - mean_squared_error: 41.5870 - root_mean_squared_error: 6.4488\n",
      "Epoch 153/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 87.6973 - mean_squared_error: 87.6973 - root_mean_squared_error: 9.3647\n",
      "Epoch 154/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 61.1287 - mean_squared_error: 61.1287 - root_mean_squared_error: 7.8185\n",
      "Epoch 155/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 35.9168 - mean_squared_error: 35.9168 - root_mean_squared_error: 5.9931\n",
      "Epoch 156/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 40.3763 - mean_squared_error: 40.3763 - root_mean_squared_error: 6.3542\n",
      "Epoch 157/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 37.6376 - mean_squared_error: 37.6376 - root_mean_squared_error: 6.1349\n",
      "Epoch 158/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 32.5629 - mean_squared_error: 32.5629 - root_mean_squared_error: 5.7064\n",
      "Epoch 159/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 57.5014 - mean_squared_error: 57.5014 - root_mean_squared_error: 7.5830\n",
      "Epoch 160/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 140.4050 - mean_squared_error: 140.4050 - root_mean_squared_error: 11.8493\n",
      "Epoch 161/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 41.3214 - mean_squared_error: 41.3214 - root_mean_squared_error: 6.4282\n",
      "Epoch 162/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 25.7310 - mean_squared_error: 25.7310 - root_mean_squared_error: 5.0726\n",
      "Epoch 163/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 43.5311 - mean_squared_error: 43.5311 - root_mean_squared_error: 6.5978\n",
      "Epoch 164/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 52.5886 - mean_squared_error: 52.5886 - root_mean_squared_error: 7.2518\n",
      "Epoch 165/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 117.6578 - mean_squared_error: 117.6578 - root_mean_squared_error: 10.8470\n",
      "Epoch 166/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 49.7091 - mean_squared_error: 49.7091 - root_mean_squared_error: 7.0505\n",
      "Epoch 167/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 30.8686 - mean_squared_error: 30.8686 - root_mean_squared_error: 5.5560\n",
      "Epoch 168/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 23.4415 - mean_squared_error: 23.4415 - root_mean_squared_error: 4.8416\n",
      "Epoch 169/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 35.8228 - mean_squared_error: 35.8228 - root_mean_squared_error: 5.9852\n",
      "Epoch 170/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 38.9231 - mean_squared_error: 38.9231 - root_mean_squared_error: 6.2388\n",
      "Epoch 171/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 176.1742 - mean_squared_error: 176.1742 - root_mean_squared_error: 13.2731\n",
      "Epoch 172/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 48.7218 - mean_squared_error: 48.7218 - root_mean_squared_error: 6.9801\n",
      "Epoch 173/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 33.7824 - mean_squared_error: 33.7824 - root_mean_squared_error: 5.8123\n",
      "Epoch 174/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 33.8046 - mean_squared_error: 33.8046 - root_mean_squared_error: 5.8142\n",
      "Epoch 175/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 50.1793 - mean_squared_error: 50.1793 - root_mean_squared_error: 7.0837\n",
      "Epoch 176/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 24.0860 - mean_squared_error: 24.0860 - root_mean_squared_error: 4.9078\n",
      "Epoch 177/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 35.3303 - mean_squared_error: 35.3303 - root_mean_squared_error: 5.9439\n",
      "Epoch 178/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 40.6988 - mean_squared_error: 40.6988 - root_mean_squared_error: 6.3796\n",
      "Epoch 179/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 30.2912 - mean_squared_error: 30.2912 - root_mean_squared_error: 5.5037\n",
      "Epoch 180/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 37.4468 - mean_squared_error: 37.4468 - root_mean_squared_error: 6.1194\n",
      "Epoch 181/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 53.9265 - mean_squared_error: 53.9265 - root_mean_squared_error: 7.3435\n",
      "Epoch 182/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 49.2512 - mean_squared_error: 49.2512 - root_mean_squared_error: 7.0179\n",
      "Epoch 183/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 42.9525 - mean_squared_error: 42.9525 - root_mean_squared_error: 6.5538\n",
      "Epoch 184/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 28.3665 - mean_squared_error: 28.3665 - root_mean_squared_error: 5.3260\n",
      "Epoch 185/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 23.8784 - mean_squared_error: 23.8784 - root_mean_squared_error: 4.8866\n",
      "Epoch 186/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 63.8904 - mean_squared_error: 63.8904 - root_mean_squared_error: 7.9931\n",
      "Epoch 187/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 35.5890 - mean_squared_error: 35.5890 - root_mean_squared_error: 5.9657\n",
      "Epoch 188/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 35.6012 - mean_squared_error: 35.6012 - root_mean_squared_error: 5.9667\n",
      "Epoch 189/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.0927 - mean_squared_error: 17.0927 - root_mean_squared_error: 4.1343\n",
      "Epoch 190/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 13.8906 - mean_squared_error: 13.8906 - root_mean_squared_error: 3.7270\n",
      "Epoch 191/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 105.4428 - mean_squared_error: 105.4428 - root_mean_squared_error: 10.2685\n",
      "Epoch 192/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 78.1678 - mean_squared_error: 78.1678 - root_mean_squared_error: 8.8413\n",
      "Epoch 193/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 41.7911 - mean_squared_error: 41.7911 - root_mean_squared_error: 6.4646\n",
      "Epoch 194/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 39.7393 - mean_squared_error: 39.7393 - root_mean_squared_error: 6.3039\n",
      "Epoch 195/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 47.0042 - mean_squared_error: 47.0042 - root_mean_squared_error: 6.8560\n",
      "Epoch 196/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 25.6320 - mean_squared_error: 25.6320 - root_mean_squared_error: 5.0628\n",
      "Epoch 197/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 15.6687 - mean_squared_error: 15.6687 - root_mean_squared_error: 3.9584\n",
      "Epoch 198/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 35.7492 - mean_squared_error: 35.7492 - root_mean_squared_error: 5.9791\n",
      "Epoch 199/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 39.6649 - mean_squared_error: 39.6649 - root_mean_squared_error: 6.2980\n",
      "Epoch 200/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 17.8210 - mean_squared_error: 17.8210 - root_mean_squared_error: 4.2215\n",
      "Epoch 201/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 15.6146 - mean_squared_error: 15.6146 - root_mean_squared_error: 3.9515\n",
      "Epoch 202/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 24.0637 - mean_squared_error: 24.0637 - root_mean_squared_error: 4.9055\n",
      "Epoch 203/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.2667 - mean_squared_error: 19.2667 - root_mean_squared_error: 4.3894\n",
      "Epoch 204/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.9869 - mean_squared_error: 19.9869 - root_mean_squared_error: 4.4707\n",
      "Epoch 205/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 81.5120 - mean_squared_error: 81.5120 - root_mean_squared_error: 9.0284\n",
      "Epoch 206/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 180.5392 - mean_squared_error: 180.5392 - root_mean_squared_error: 13.4365\n",
      "Epoch 207/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 43.1627 - mean_squared_error: 43.1627 - root_mean_squared_error: 6.5698\n",
      "Epoch 208/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 26.1956 - mean_squared_error: 26.1956 - root_mean_squared_error: 5.1182\n",
      "Epoch 209/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.9009 - mean_squared_error: 17.9009 - root_mean_squared_error: 4.2310\n",
      "Epoch 210/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 36.5493 - mean_squared_error: 36.5493 - root_mean_squared_error: 6.0456\n",
      "Epoch 211/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 27.4889 - mean_squared_error: 27.4889 - root_mean_squared_error: 5.2430\n",
      "Epoch 212/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 29.1883 - mean_squared_error: 29.1883 - root_mean_squared_error: 5.4026\n",
      "Epoch 213/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 32.0152 - mean_squared_error: 32.0152 - root_mean_squared_error: 5.6582\n",
      "Epoch 214/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 74.1085 - mean_squared_error: 74.1085 - root_mean_squared_error: 8.6086\n",
      "Epoch 215/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 106.7400 - mean_squared_error: 106.7400 - root_mean_squared_error: 10.3315\n",
      "Epoch 216/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 31.6280 - mean_squared_error: 31.6280 - root_mean_squared_error: 5.6239\n",
      "Epoch 217/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 22.7719 - mean_squared_error: 22.7719 - root_mean_squared_error: 4.7720\n",
      "Epoch 218/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 38.7017 - mean_squared_error: 38.7017 - root_mean_squared_error: 6.2211\n",
      "Epoch 219/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.0984 - mean_squared_error: 18.0984 - root_mean_squared_error: 4.2542\n",
      "Epoch 220/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 15.2782 - mean_squared_error: 15.2782 - root_mean_squared_error: 3.9087\n",
      "Epoch 221/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 11.5724 - mean_squared_error: 11.5724 - root_mean_squared_error: 3.4018\n",
      "Epoch 222/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 60.7164 - mean_squared_error: 60.7164 - root_mean_squared_error: 7.7921\n",
      "Epoch 223/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 45.5791 - mean_squared_error: 45.5791 - root_mean_squared_error: 6.7512\n",
      "Epoch 224/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 25.7208 - mean_squared_error: 25.7208 - root_mean_squared_error: 5.0716\n",
      "Epoch 225/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 39.0475 - mean_squared_error: 39.0475 - root_mean_squared_error: 6.2488\n",
      "Epoch 226/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 32.6267 - mean_squared_error: 32.6267 - root_mean_squared_error: 5.7120\n",
      "Epoch 227/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 13.8175 - mean_squared_error: 13.8175 - root_mean_squared_error: 3.7172\n",
      "Epoch 228/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 15.7796 - mean_squared_error: 15.7796 - root_mean_squared_error: 3.9723\n",
      "Epoch 229/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 27.0839 - mean_squared_error: 27.0839 - root_mean_squared_error: 5.2042\n",
      "Epoch 230/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 22.9060 - mean_squared_error: 22.9060 - root_mean_squared_error: 4.7860\n",
      "Epoch 231/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 51.8177 - mean_squared_error: 51.8177 - root_mean_squared_error: 7.1985\n",
      "Epoch 232/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 25.7729 - mean_squared_error: 25.7729 - root_mean_squared_error: 5.0767\n",
      "Epoch 233/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 79.4431 - mean_squared_error: 79.4431 - root_mean_squared_error: 8.9131\n",
      "Epoch 234/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 20.8744 - mean_squared_error: 20.8744 - root_mean_squared_error: 4.5688\n",
      "Epoch 235/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 44.8878 - mean_squared_error: 44.8878 - root_mean_squared_error: 6.6998\n",
      "Epoch 236/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 20.2863 - mean_squared_error: 20.2863 - root_mean_squared_error: 4.5040\n",
      "Epoch 237/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 22.3369 - mean_squared_error: 22.3369 - root_mean_squared_error: 4.7262\n",
      "Epoch 238/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 16.3265 - mean_squared_error: 16.3265 - root_mean_squared_error: 4.0406\n",
      "Epoch 239/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 53.0338 - mean_squared_error: 53.0338 - root_mean_squared_error: 7.2824\n",
      "Epoch 240/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 84.2881 - mean_squared_error: 84.2881 - root_mean_squared_error: 9.1809\n",
      "Epoch 241/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 39.0177 - mean_squared_error: 39.0177 - root_mean_squared_error: 6.2464\n",
      "Epoch 242/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 20.6580 - mean_squared_error: 20.6580 - root_mean_squared_error: 4.5451\n",
      "Epoch 243/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 14.9257 - mean_squared_error: 14.9257 - root_mean_squared_error: 3.8634\n",
      "Epoch 244/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 11.5085 - mean_squared_error: 11.5085 - root_mean_squared_error: 3.3924\n",
      "Epoch 245/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 46.0926 - mean_squared_error: 46.0926 - root_mean_squared_error: 6.7892\n",
      "Epoch 246/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 64.7724 - mean_squared_error: 64.7724 - root_mean_squared_error: 8.0481\n",
      "Epoch 247/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 36.1861 - mean_squared_error: 36.1861 - root_mean_squared_error: 6.0155\n",
      "Epoch 248/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 49.3511 - mean_squared_error: 49.3511 - root_mean_squared_error: 7.0250\n",
      "Epoch 249/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 26.0564 - mean_squared_error: 26.0564 - root_mean_squared_error: 5.1045\n",
      "Epoch 250/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 13.7725 - mean_squared_error: 13.7725 - root_mean_squared_error: 3.7111\n",
      "Epoch 251/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 13.3619 - mean_squared_error: 13.3619 - root_mean_squared_error: 3.6554\n",
      "Epoch 252/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 13.6076 - mean_squared_error: 13.6076 - root_mean_squared_error: 3.6889\n",
      "Epoch 253/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 14.0355 - mean_squared_error: 14.0355 - root_mean_squared_error: 3.7464\n",
      "Epoch 254/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 23.9627 - mean_squared_error: 23.9627 - root_mean_squared_error: 4.8952\n",
      "Epoch 255/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 24.4951 - mean_squared_error: 24.4951 - root_mean_squared_error: 4.9493\n",
      "Epoch 256/500\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 50.0364 - mean_squared_error: 50.0364 - root_mean_squared_error: 7.0736\n",
      "Epoch 257/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 76.9851 - mean_squared_error: 76.9851 - root_mean_squared_error: 8.7741\n",
      "Epoch 258/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 46.9462 - mean_squared_error: 46.9462 - root_mean_squared_error: 6.8517\n",
      "Epoch 259/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 62.2820 - mean_squared_error: 62.2820 - root_mean_squared_error: 7.8919\n",
      "Epoch 260/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 20.1267 - mean_squared_error: 20.1267 - root_mean_squared_error: 4.4863\n",
      "Epoch 261/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.3338 - mean_squared_error: 19.3338 - root_mean_squared_error: 4.3970\n",
      "Epoch 262/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 15.4223 - mean_squared_error: 15.4223 - root_mean_squared_error: 3.9271\n",
      "Epoch 263/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.2989 - mean_squared_error: 16.2989 - root_mean_squared_error: 4.0372\n",
      "Epoch 264/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 9.2629 - mean_squared_error: 9.2629 - root_mean_squared_error: 3.0435\n",
      "Epoch 265/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 8.3575 - mean_squared_error: 8.3575 - root_mean_squared_error: 2.8909\n",
      "Epoch 266/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 9.5084 - mean_squared_error: 9.5084 - root_mean_squared_error: 3.0836\n",
      "Epoch 267/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 5.5183 - mean_squared_error: 5.5183 - root_mean_squared_error: 2.3491\n",
      "Epoch 268/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.6999 - mean_squared_error: 19.6999 - root_mean_squared_error: 4.4385\n",
      "Epoch 269/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.0408 - mean_squared_error: 17.0408 - root_mean_squared_error: 4.1280\n",
      "Epoch 270/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 67.9147 - mean_squared_error: 67.9147 - root_mean_squared_error: 8.2410\n",
      "Epoch 271/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 29.2886 - mean_squared_error: 29.2886 - root_mean_squared_error: 5.4119\n",
      "Epoch 272/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 12.2069 - mean_squared_error: 12.2069 - root_mean_squared_error: 3.4938\n",
      "Epoch 273/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 34.8624 - mean_squared_error: 34.8624 - root_mean_squared_error: 5.9044\n",
      "Epoch 274/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 18.2377 - mean_squared_error: 18.2377 - root_mean_squared_error: 4.2706\n",
      "Epoch 275/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 15.3914 - mean_squared_error: 15.3914 - root_mean_squared_error: 3.9232\n",
      "Epoch 276/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 164.5422 - mean_squared_error: 164.5422 - root_mean_squared_error: 12.8274\n",
      "Epoch 277/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 59.4433 - mean_squared_error: 59.4433 - root_mean_squared_error: 7.7099\n",
      "Epoch 278/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 22.8388 - mean_squared_error: 22.8388 - root_mean_squared_error: 4.7790\n",
      "Epoch 279/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 21.6996 - mean_squared_error: 21.6996 - root_mean_squared_error: 4.6583\n",
      "Epoch 280/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 21.9188 - mean_squared_error: 21.9188 - root_mean_squared_error: 4.6817\n",
      "Epoch 281/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 12.5436 - mean_squared_error: 12.5436 - root_mean_squared_error: 3.5417\n",
      "Epoch 282/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 12.9834 - mean_squared_error: 12.9834 - root_mean_squared_error: 3.6033\n",
      "Epoch 283/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 8.1168 - mean_squared_error: 8.1168 - root_mean_squared_error: 2.8490\n",
      "Epoch 284/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.7891 - mean_squared_error: 4.7891 - root_mean_squared_error: 2.1884\n",
      "Epoch 285/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 5.6479 - mean_squared_error: 5.6479 - root_mean_squared_error: 2.3765\n",
      "Epoch 286/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 28.1500 - mean_squared_error: 28.1500 - root_mean_squared_error: 5.3057\n",
      "Epoch 287/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 118.2489 - mean_squared_error: 118.2489 - root_mean_squared_error: 10.8742\n",
      "Epoch 288/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 45.3408 - mean_squared_error: 45.3408 - root_mean_squared_error: 6.7336\n",
      "Epoch 289/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 21.2760 - mean_squared_error: 21.2760 - root_mean_squared_error: 4.6126\n",
      "Epoch 290/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 11.2789 - mean_squared_error: 11.2789 - root_mean_squared_error: 3.3584\n",
      "Epoch 291/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 7.3057 - mean_squared_error: 7.3057 - root_mean_squared_error: 2.7029\n",
      "Epoch 292/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 5.2702 - mean_squared_error: 5.2702 - root_mean_squared_error: 2.2957\n",
      "Epoch 293/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 12.6387 - mean_squared_error: 12.6387 - root_mean_squared_error: 3.5551\n",
      "Epoch 294/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 95.8296 - mean_squared_error: 95.8296 - root_mean_squared_error: 9.7893\n",
      "Epoch 295/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 45.7438 - mean_squared_error: 45.7438 - root_mean_squared_error: 6.7634\n",
      "Epoch 296/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 41.7307 - mean_squared_error: 41.7307 - root_mean_squared_error: 6.4599\n",
      "Epoch 297/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.1843 - mean_squared_error: 18.1843 - root_mean_squared_error: 4.2643\n",
      "Epoch 298/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 22.4877 - mean_squared_error: 22.4877 - root_mean_squared_error: 4.7421\n",
      "Epoch 299/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 10.9525 - mean_squared_error: 10.9525 - root_mean_squared_error: 3.3095\n",
      "Epoch 300/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 7.1616 - mean_squared_error: 7.1616 - root_mean_squared_error: 2.6761\n",
      "Epoch 301/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 6.9788 - mean_squared_error: 6.9788 - root_mean_squared_error: 2.6417\n",
      "Epoch 302/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 6.7697 - mean_squared_error: 6.7697 - root_mean_squared_error: 2.6019\n",
      "Epoch 303/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 7.4976 - mean_squared_error: 7.4976 - root_mean_squared_error: 2.7382\n",
      "Epoch 304/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 8.3184 - mean_squared_error: 8.3184 - root_mean_squared_error: 2.8842\n",
      "Epoch 305/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 28.4250 - mean_squared_error: 28.4250 - root_mean_squared_error: 5.3315\n",
      "Epoch 306/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 28.1428 - mean_squared_error: 28.1428 - root_mean_squared_error: 5.3050\n",
      "Epoch 307/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 18.8865 - mean_squared_error: 18.8865 - root_mean_squared_error: 4.3459\n",
      "Epoch 308/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 25.3591 - mean_squared_error: 25.3591 - root_mean_squared_error: 5.0358\n",
      "Epoch 309/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 8.1622 - mean_squared_error: 8.1622 - root_mean_squared_error: 2.8570\n",
      "Epoch 310/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 27.7478 - mean_squared_error: 27.7478 - root_mean_squared_error: 5.2676\n",
      "Epoch 311/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 57.0149 - mean_squared_error: 57.0149 - root_mean_squared_error: 7.5508\n",
      "Epoch 312/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 43.2908 - mean_squared_error: 43.2908 - root_mean_squared_error: 6.5796\n",
      "Epoch 313/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 39.0187 - mean_squared_error: 39.0187 - root_mean_squared_error: 6.2465\n",
      "Epoch 314/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 41.8838 - mean_squared_error: 41.8838 - root_mean_squared_error: 6.4718\n",
      "Epoch 315/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 29.1686 - mean_squared_error: 29.1686 - root_mean_squared_error: 5.4008\n",
      "Epoch 316/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 23.7116 - mean_squared_error: 23.7116 - root_mean_squared_error: 4.8695\n",
      "Epoch 317/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 29.7445 - mean_squared_error: 29.7445 - root_mean_squared_error: 5.4539\n",
      "Epoch 318/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 26.3430 - mean_squared_error: 26.3430 - root_mean_squared_error: 5.1325\n",
      "Epoch 319/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 16.9858 - mean_squared_error: 16.9858 - root_mean_squared_error: 4.1214\n",
      "Epoch 320/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 32.1721 - mean_squared_error: 32.1721 - root_mean_squared_error: 5.6720\n",
      "Epoch 321/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 14.6620 - mean_squared_error: 14.6620 - root_mean_squared_error: 3.8291\n",
      "Epoch 322/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 32.9958 - mean_squared_error: 32.9958 - root_mean_squared_error: 5.7442\n",
      "Epoch 323/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 32.7930 - mean_squared_error: 32.7930 - root_mean_squared_error: 5.7265\n",
      "Epoch 324/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 11.2788 - mean_squared_error: 11.2788 - root_mean_squared_error: 3.3584\n",
      "Epoch 325/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 6.3916 - mean_squared_error: 6.3916 - root_mean_squared_error: 2.5282\n",
      "Epoch 326/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 24.3499 - mean_squared_error: 24.3499 - root_mean_squared_error: 4.9346\n",
      "Epoch 327/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 17.8705 - mean_squared_error: 17.8705 - root_mean_squared_error: 4.2273\n",
      "Epoch 328/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 59.6391 - mean_squared_error: 59.6391 - root_mean_squared_error: 7.7226\n",
      "Epoch 329/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 33.7016 - mean_squared_error: 33.7016 - root_mean_squared_error: 5.8053\n",
      "Epoch 330/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 22.0257 - mean_squared_error: 22.0256 - root_mean_squared_error: 4.6931\n",
      "Epoch 331/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 11.3069 - mean_squared_error: 11.3069 - root_mean_squared_error: 3.3626\n",
      "Epoch 332/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 12.5530 - mean_squared_error: 12.5530 - root_mean_squared_error: 3.5430\n",
      "Epoch 333/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 7.2283 - mean_squared_error: 7.2283 - root_mean_squared_error: 2.6885\n",
      "Epoch 334/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 36.8800 - mean_squared_error: 36.8800 - root_mean_squared_error: 6.0729\n",
      "Epoch 335/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 64.5805 - mean_squared_error: 64.5805 - root_mean_squared_error: 8.0362\n",
      "Epoch 336/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 19.9469 - mean_squared_error: 19.9469 - root_mean_squared_error: 4.4662\n",
      "Epoch 337/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 16.1186 - mean_squared_error: 16.1186 - root_mean_squared_error: 4.0148\n",
      "Epoch 338/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 52.0520 - mean_squared_error: 52.0520 - root_mean_squared_error: 7.2147\n",
      "Epoch 339/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 25.8770 - mean_squared_error: 25.8770 - root_mean_squared_error: 5.0869\n",
      "Epoch 340/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 10.9005 - mean_squared_error: 10.9005 - root_mean_squared_error: 3.3016\n",
      "Epoch 341/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.8920 - mean_squared_error: 18.8920 - root_mean_squared_error: 4.3465\n",
      "Epoch 342/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 8.6123 - mean_squared_error: 8.6123 - root_mean_squared_error: 2.9347\n",
      "Epoch 343/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 7.8115 - mean_squared_error: 7.8115 - root_mean_squared_error: 2.7949\n",
      "Epoch 344/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 8.9471 - mean_squared_error: 8.9471 - root_mean_squared_error: 2.9912\n",
      "Epoch 345/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 6.3665 - mean_squared_error: 6.3665 - root_mean_squared_error: 2.5232\n",
      "Epoch 346/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.5846 - mean_squared_error: 4.5846 - root_mean_squared_error: 2.1412\n",
      "Epoch 347/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 15.7904 - mean_squared_error: 15.7904 - root_mean_squared_error: 3.9737\n",
      "Epoch 348/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 45.6162 - mean_squared_error: 45.6162 - root_mean_squared_error: 6.7540\n",
      "Epoch 349/500\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 49.5532 - mean_squared_error: 49.5532 - root_mean_squared_error: 7.0394\n",
      "Epoch 350/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 63.4473 - mean_squared_error: 63.4473 - root_mean_squared_error: 7.9654\n",
      "Epoch 351/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 33.2225 - mean_squared_error: 33.2225 - root_mean_squared_error: 5.7639\n",
      "Epoch 352/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 14.6971 - mean_squared_error: 14.6971 - root_mean_squared_error: 3.8337\n",
      "Epoch 353/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 11.1535 - mean_squared_error: 11.1535 - root_mean_squared_error: 3.3397\n",
      "Epoch 354/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 11.1317 - mean_squared_error: 11.1317 - root_mean_squared_error: 3.3364\n",
      "Epoch 355/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.3307 - mean_squared_error: 19.3307 - root_mean_squared_error: 4.3967\n",
      "Epoch 356/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 72.3780 - mean_squared_error: 72.3780 - root_mean_squared_error: 8.5075\n",
      "Epoch 357/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 27.9368 - mean_squared_error: 27.9368 - root_mean_squared_error: 5.2855\n",
      "Epoch 358/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 25.5720 - mean_squared_error: 25.5720 - root_mean_squared_error: 5.0569\n",
      "Epoch 359/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 9.2056 - mean_squared_error: 9.2056 - root_mean_squared_error: 3.0341\n",
      "Epoch 360/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 8.2319 - mean_squared_error: 8.2319 - root_mean_squared_error: 2.8691\n",
      "Epoch 361/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.8560 - mean_squared_error: 16.8560 - root_mean_squared_error: 4.1056\n",
      "Epoch 362/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 8.2707 - mean_squared_error: 8.2707 - root_mean_squared_error: 2.8759\n",
      "Epoch 363/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 5.7630 - mean_squared_error: 5.7630 - root_mean_squared_error: 2.4006\n",
      "Epoch 364/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 32.5394 - mean_squared_error: 32.5394 - root_mean_squared_error: 5.7043\n",
      "Epoch 365/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 144.0462 - mean_squared_error: 144.0462 - root_mean_squared_error: 12.0019\n",
      "Epoch 366/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 33.8868 - mean_squared_error: 33.8868 - root_mean_squared_error: 5.8212\n",
      "Epoch 367/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 18.1006 - mean_squared_error: 18.1006 - root_mean_squared_error: 4.2545\n",
      "Epoch 368/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 14.1750 - mean_squared_error: 14.1750 - root_mean_squared_error: 3.7650\n",
      "Epoch 369/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 6.9806 - mean_squared_error: 6.9806 - root_mean_squared_error: 2.6421\n",
      "Epoch 370/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.4446 - mean_squared_error: 4.4446 - root_mean_squared_error: 2.1082\n",
      "Epoch 371/500\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 6.0696 - mean_squared_error: 6.0696 - root_mean_squared_error: 2.4636\n",
      "Epoch 372/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.6857 - mean_squared_error: 3.6857 - root_mean_squared_error: 1.9198\n",
      "Epoch 373/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 5.6080 - mean_squared_error: 5.6080 - root_mean_squared_error: 2.3681\n",
      "Epoch 374/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.1085 - mean_squared_error: 3.1085 - root_mean_squared_error: 1.7631\n",
      "Epoch 375/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 3.5401 - mean_squared_error: 3.5401 - root_mean_squared_error: 1.8815\n",
      "Epoch 376/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 5.7233 - mean_squared_error: 5.7233 - root_mean_squared_error: 2.3923\n",
      "Epoch 377/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 11.1958 - mean_squared_error: 11.1958 - root_mean_squared_error: 3.3460\n",
      "Epoch 378/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 43.2399 - mean_squared_error: 43.2399 - root_mean_squared_error: 6.5757\n",
      "Epoch 379/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 158.2240 - mean_squared_error: 158.2240 - root_mean_squared_error: 12.5787\n",
      "Epoch 380/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 52.0954 - mean_squared_error: 52.0954 - root_mean_squared_error: 7.2177\n",
      "Epoch 381/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 32.3416 - mean_squared_error: 32.3416 - root_mean_squared_error: 5.6870\n",
      "Epoch 382/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 31.1306 - mean_squared_error: 31.1306 - root_mean_squared_error: 5.5795\n",
      "Epoch 383/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 37.2526 - mean_squared_error: 37.2526 - root_mean_squared_error: 6.1035\n",
      "Epoch 384/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 20.1617 - mean_squared_error: 20.1617 - root_mean_squared_error: 4.4902\n",
      "Epoch 385/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 31.6901 - mean_squared_error: 31.6901 - root_mean_squared_error: 5.6294\n",
      "Epoch 386/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 29.3533 - mean_squared_error: 29.3533 - root_mean_squared_error: 5.4179\n",
      "Epoch 387/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 10.4614 - mean_squared_error: 10.4614 - root_mean_squared_error: 3.2344\n",
      "Epoch 388/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 12.3868 - mean_squared_error: 12.3868 - root_mean_squared_error: 3.5195\n",
      "Epoch 389/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 29.1624 - mean_squared_error: 29.1624 - root_mean_squared_error: 5.4002\n",
      "Epoch 390/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 10.6399 - mean_squared_error: 10.6399 - root_mean_squared_error: 3.2619\n",
      "Epoch 391/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 6.9885 - mean_squared_error: 6.9885 - root_mean_squared_error: 2.6436\n",
      "Epoch 392/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 5.0127 - mean_squared_error: 5.0127 - root_mean_squared_error: 2.2389\n",
      "Epoch 393/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 15.6851 - mean_squared_error: 15.6851 - root_mean_squared_error: 3.9604\n",
      "Epoch 394/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 23.5327 - mean_squared_error: 23.5327 - root_mean_squared_error: 4.8510\n",
      "Epoch 395/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 11.3561 - mean_squared_error: 11.3561 - root_mean_squared_error: 3.3699\n",
      "Epoch 396/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 27.0335 - mean_squared_error: 27.0335 - root_mean_squared_error: 5.1994\n",
      "Epoch 397/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 7.7505 - mean_squared_error: 7.7505 - root_mean_squared_error: 2.7840\n",
      "Epoch 398/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 11.2296 - mean_squared_error: 11.2296 - root_mean_squared_error: 3.3511\n",
      "Epoch 399/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.3133 - mean_squared_error: 4.3133 - root_mean_squared_error: 2.0768\n",
      "Epoch 400/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.2007 - mean_squared_error: 4.2007 - root_mean_squared_error: 2.0495\n",
      "Epoch 401/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 5.5738 - mean_squared_error: 5.5738 - root_mean_squared_error: 2.3609\n",
      "Epoch 402/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 13.3926 - mean_squared_error: 13.3926 - root_mean_squared_error: 3.6596\n",
      "Epoch 403/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 26.6717 - mean_squared_error: 26.6717 - root_mean_squared_error: 5.1645\n",
      "Epoch 404/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 169.4458 - mean_squared_error: 169.4458 - root_mean_squared_error: 13.0171\n",
      "Epoch 405/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 110.8576 - mean_squared_error: 110.8576 - root_mean_squared_error: 10.5289\n",
      "Epoch 406/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 56.9604 - mean_squared_error: 56.9604 - root_mean_squared_error: 7.5472\n",
      "Epoch 407/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.1406 - mean_squared_error: 19.1406 - root_mean_squared_error: 4.3750\n",
      "Epoch 408/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 10.0085 - mean_squared_error: 10.0085 - root_mean_squared_error: 3.1636\n",
      "Epoch 409/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 5.4627 - mean_squared_error: 5.4627 - root_mean_squared_error: 2.3373\n",
      "Epoch 410/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 3.8531 - mean_squared_error: 3.8531 - root_mean_squared_error: 1.9629\n",
      "Epoch 411/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 3.8328 - mean_squared_error: 3.8328 - root_mean_squared_error: 1.9578\n",
      "Epoch 412/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 10.8033 - mean_squared_error: 10.8033 - root_mean_squared_error: 3.2868\n",
      "Epoch 413/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 8.4042 - mean_squared_error: 8.4042 - root_mean_squared_error: 2.8990\n",
      "Epoch 414/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.0498 - mean_squared_error: 3.0498 - root_mean_squared_error: 1.7464\n",
      "Epoch 415/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.7547 - mean_squared_error: 1.7547 - root_mean_squared_error: 1.3246\n",
      "Epoch 416/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4471 - mean_squared_error: 1.4471 - root_mean_squared_error: 1.2030\n",
      "Epoch 417/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5201 - mean_squared_error: 1.5201 - root_mean_squared_error: 1.2329\n",
      "Epoch 418/500\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.3413 - mean_squared_error: 1.3413 - root_mean_squared_error: 1.1581\n",
      "Epoch 419/500\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.6761 - mean_squared_error: 1.6761 - root_mean_squared_error: 1.2946\n",
      "Epoch 420/500\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 4.1550 - mean_squared_error: 4.1550 - root_mean_squared_error: 2.0384\n",
      "Epoch 421/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.5389 - mean_squared_error: 2.5389 - root_mean_squared_error: 1.5934\n",
      "Epoch 422/500\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.6087 - mean_squared_error: 2.6087 - root_mean_squared_error: 1.6152\n",
      "Epoch 423/500\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.3831 - mean_squared_error: 1.3831 - root_mean_squared_error: 1.1761\n",
      "Epoch 424/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9347 - mean_squared_error: 1.9347 - root_mean_squared_error: 1.3909\n",
      "Epoch 425/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 40.6943 - mean_squared_error: 40.6943 - root_mean_squared_error: 6.3792\n",
      "Epoch 426/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 127.1859 - mean_squared_error: 127.1859 - root_mean_squared_error: 11.2777\n",
      "Epoch 427/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 45.1421 - mean_squared_error: 45.1421 - root_mean_squared_error: 6.7188\n",
      "Epoch 428/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 17.0277 - mean_squared_error: 17.0277 - root_mean_squared_error: 4.1265\n",
      "Epoch 429/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 10.2704 - mean_squared_error: 10.2704 - root_mean_squared_error: 3.2048\n",
      "Epoch 430/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 10.6477 - mean_squared_error: 10.6477 - root_mean_squared_error: 3.2631\n",
      "Epoch 431/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 15.3125 - mean_squared_error: 15.3125 - root_mean_squared_error: 3.9131\n",
      "Epoch 432/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.3229 - mean_squared_error: 18.3229 - root_mean_squared_error: 4.2805\n",
      "Epoch 433/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.5361 - mean_squared_error: 17.5361 - root_mean_squared_error: 4.1876\n",
      "Epoch 434/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 8.8350 - mean_squared_error: 8.8350 - root_mean_squared_error: 2.9724\n",
      "Epoch 435/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 6.0729 - mean_squared_error: 6.0729 - root_mean_squared_error: 2.4643\n",
      "Epoch 436/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 22.6078 - mean_squared_error: 22.6078 - root_mean_squared_error: 4.7548\n",
      "Epoch 437/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 12.7692 - mean_squared_error: 12.7692 - root_mean_squared_error: 3.5734\n",
      "Epoch 438/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 20.4303 - mean_squared_error: 20.4303 - root_mean_squared_error: 4.5200\n",
      "Epoch 439/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 5.7186 - mean_squared_error: 5.7186 - root_mean_squared_error: 2.3914\n",
      "Epoch 440/500\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 14.5322 - mean_squared_error: 14.5322 - root_mean_squared_error: 3.8121\n",
      "Epoch 441/500\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 57.8943 - mean_squared_error: 57.8943 - root_mean_squared_error: 7.6088\n",
      "Epoch 442/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 27.4031 - mean_squared_error: 27.4031 - root_mean_squared_error: 5.2348\n",
      "Epoch 443/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 31.9962 - mean_squared_error: 31.9962 - root_mean_squared_error: 5.6565\n",
      "Epoch 444/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 11.0470 - mean_squared_error: 11.0470 - root_mean_squared_error: 3.3237\n",
      "Epoch 445/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 14.4766 - mean_squared_error: 14.4766 - root_mean_squared_error: 3.8048\n",
      "Epoch 446/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 22.8527 - mean_squared_error: 22.8527 - root_mean_squared_error: 4.7804\n",
      "Epoch 447/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 34.4942 - mean_squared_error: 34.4942 - root_mean_squared_error: 5.8732\n",
      "Epoch 448/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 10.3848 - mean_squared_error: 10.3848 - root_mean_squared_error: 3.2225\n",
      "Epoch 449/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 8.1170 - mean_squared_error: 8.1170 - root_mean_squared_error: 2.8490\n",
      "Epoch 450/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 6.9954 - mean_squared_error: 6.9954 - root_mean_squared_error: 2.6449\n",
      "Epoch 451/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 8.3074 - mean_squared_error: 8.3074 - root_mean_squared_error: 2.8823\n",
      "Epoch 452/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 16.3892 - mean_squared_error: 16.3892 - root_mean_squared_error: 4.0484\n",
      "Epoch 453/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 14.6652 - mean_squared_error: 14.6652 - root_mean_squared_error: 3.8295\n",
      "Epoch 454/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 29.5625 - mean_squared_error: 29.5625 - root_mean_squared_error: 5.4371\n",
      "Epoch 455/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 110.7682 - mean_squared_error: 110.7682 - root_mean_squared_error: 10.5246\n",
      "Epoch 456/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 60.2110 - mean_squared_error: 60.2110 - root_mean_squared_error: 7.7596\n",
      "Epoch 457/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 133.5591 - mean_squared_error: 133.5591 - root_mean_squared_error: 11.5568\n",
      "Epoch 458/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 46.5647 - mean_squared_error: 46.5647 - root_mean_squared_error: 6.8238\n",
      "Epoch 459/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 26.7131 - mean_squared_error: 26.7131 - root_mean_squared_error: 5.1685\n",
      "Epoch 460/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 14.0219 - mean_squared_error: 14.0219 - root_mean_squared_error: 3.7446\n",
      "Epoch 461/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 9.7244 - mean_squared_error: 9.7244 - root_mean_squared_error: 3.1184\n",
      "Epoch 462/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 5.1912 - mean_squared_error: 5.1912 - root_mean_squared_error: 2.2784\n",
      "Epoch 463/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.8036 - mean_squared_error: 2.8036 - root_mean_squared_error: 1.6744\n",
      "Epoch 464/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.6642 - mean_squared_error: 2.6642 - root_mean_squared_error: 1.6322\n",
      "Epoch 465/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.7875 - mean_squared_error: 4.7875 - root_mean_squared_error: 2.1880\n",
      "Epoch 466/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.1138 - mean_squared_error: 2.1138 - root_mean_squared_error: 1.4539\n",
      "Epoch 467/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2689 - mean_squared_error: 1.2689 - root_mean_squared_error: 1.1265\n",
      "Epoch 468/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9500 - mean_squared_error: 0.9500 - root_mean_squared_error: 0.9747\n",
      "Epoch 469/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0525 - mean_squared_error: 1.0525 - root_mean_squared_error: 1.0259\n",
      "Epoch 470/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8574 - mean_squared_error: 0.8574 - root_mean_squared_error: 0.9260\n",
      "Epoch 471/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6273 - mean_squared_error: 0.6273 - root_mean_squared_error: 0.7921\n",
      "Epoch 472/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.0941 - mean_squared_error: 2.0941 - root_mean_squared_error: 1.4471\n",
      "Epoch 473/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0704 - mean_squared_error: 1.0704 - root_mean_squared_error: 1.0346\n",
      "Epoch 474/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.8903 - mean_squared_error: 2.8903 - root_mean_squared_error: 1.7001\n",
      "Epoch 475/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2484 - mean_squared_error: 2.2484 - root_mean_squared_error: 1.4995\n",
      "Epoch 476/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.0689 - mean_squared_error: 3.0689 - root_mean_squared_error: 1.7518\n",
      "Epoch 477/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 17.6757 - mean_squared_error: 17.6757 - root_mean_squared_error: 4.2042\n",
      "Epoch 478/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 75.0368 - mean_squared_error: 75.0368 - root_mean_squared_error: 8.6624\n",
      "Epoch 479/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 98.2498 - mean_squared_error: 98.2498 - root_mean_squared_error: 9.9121\n",
      "Epoch 480/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 46.5491 - mean_squared_error: 46.5491 - root_mean_squared_error: 6.8227\n",
      "Epoch 481/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 37.1749 - mean_squared_error: 37.1749 - root_mean_squared_error: 6.0971\n",
      "Epoch 482/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.4771 - mean_squared_error: 17.4771 - root_mean_squared_error: 4.1806\n",
      "Epoch 483/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 11.0216 - mean_squared_error: 11.0216 - root_mean_squared_error: 3.3199\n",
      "Epoch 484/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 10.5131 - mean_squared_error: 10.5131 - root_mean_squared_error: 3.2424\n",
      "Epoch 485/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 21.9727 - mean_squared_error: 21.9727 - root_mean_squared_error: 4.6875\n",
      "Epoch 486/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 7.1310 - mean_squared_error: 7.1310 - root_mean_squared_error: 2.6704\n",
      "Epoch 487/500\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 4.2300 - mean_squared_error: 4.2300 - root_mean_squared_error: 2.0567\n",
      "Epoch 488/500\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.5064 - mean_squared_error: 2.5064 - root_mean_squared_error: 1.5832\n",
      "Epoch 489/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.5960 - mean_squared_error: 4.5960 - root_mean_squared_error: 2.1438\n",
      "Epoch 490/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 3.6081 - mean_squared_error: 3.6081 - root_mean_squared_error: 1.8995\n",
      "Epoch 491/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 6.3787 - mean_squared_error: 6.3787 - root_mean_squared_error: 2.5256\n",
      "Epoch 492/500\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.9097 - mean_squared_error: 2.9097 - root_mean_squared_error: 1.7058\n",
      "Epoch 493/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 9.3131 - mean_squared_error: 9.3131 - root_mean_squared_error: 3.0517\n",
      "Epoch 494/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.2749 - mean_squared_error: 4.2749 - root_mean_squared_error: 2.0676\n",
      "Epoch 495/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 13.2481 - mean_squared_error: 13.2481 - root_mean_squared_error: 3.6398\n",
      "Epoch 496/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 21.4004 - mean_squared_error: 21.4004 - root_mean_squared_error: 4.6261\n",
      "Epoch 497/500\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 21.2490 - mean_squared_error: 21.2490 - root_mean_squared_error: 4.6097\n",
      "Epoch 498/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 36.0917 - mean_squared_error: 36.0917 - root_mean_squared_error: 6.0076\n",
      "Epoch 499/500\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 12.9606 - mean_squared_error: 12.9606 - root_mean_squared_error: 3.6001\n",
      "Epoch 500/500\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 7.1594 - mean_squared_error: 7.1594 - root_mean_squared_error: 2.6757\n",
      "23/23 [==============================] - 0s 773us/step - loss: 2192.0200 - mean_squared_error: 2192.0200 - root_mean_squared_error: 46.8190\n",
      "Test accuracy: [2192.02001953125, 2192.02001953125, 46.81901168823242]\n",
      "23/23 [==============================] - 0s 728us/step\n",
      "(714, 1) (714,)\n",
      "23/23 [==============================] - 0s 727us/step\n",
      "Test R2: 0.31029616321111486\n",
      "23/23 [==============================] - 0s 703us/step\n",
      "Test Bias (%): 16.412491182222848\n",
      "GEESEBAL accuracy (RMSE)= 57.51334780750559\n",
      "GEESEBAL R2 = -0.040771602283387054\n",
      "GEESEBAL MAPE = 21.832874662394723\n"
     ]
    }
   ],
   "source": [
    "## Vanilla neural network\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras.utils.layer_utils import count_params\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "class NN:\n",
    "    def __init__(self, input_shape):\n",
    "        self.model = self.create_model(input_shape)\n",
    "\n",
    "    def create_model(self, input_shape):\n",
    "        layers = list()\n",
    "        layers.append( tf.keras.layers.Flatten() )\n",
    "        units = [ 128,128,128,128,128,128,128,128,128,64]\n",
    "        for i in range( 10 ):\n",
    "            if i==0:\n",
    "                layers.append( tf.keras.layers.Dense( units[i] , activation='relu',input_shape=input_shape ) )\n",
    "            else:\n",
    "                layers.append( tf.keras.layers.Dense( units[i] , activation='relu') )\n",
    "        layers.append(tf.keras.layers.Dense(1))\n",
    "        model = keras.Sequential(layers)\n",
    "        # model.add\n",
    "        # model = Sequential([\n",
    "        #     Dense(64, activation='relu', input_shape=input_shape),\n",
    "        #     Dense(64, activation='relu'),\n",
    "        #     Dense(64,activation=\"relu\"),\n",
    "        #     Dense(1)\n",
    "        # ])\n",
    "        return model\n",
    "\n",
    "    def compile_model(self):\n",
    "        self.model.compile(optimizer='Nadam',\n",
    "                            loss='mean_squared_error',\n",
    "                            metrics=[keras.metrics.MeanSquaredError(),keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "    def train_model(self, x_train, y_train, epochs=500):\n",
    "        self.model.fit(x_train, y_train, epochs=epochs,batch_size=200)\n",
    "\n",
    "    def evaluate_model(self, x_test, y_test):\n",
    "        return self.model.evaluate(x_test, y_test)\n",
    "    def r2_calc(self,x_test,y_test):\n",
    "        print(self.model.predict(x_test).shape,y_test.shape)\n",
    "        return r2_score(y_test, self.model.predict(x_test))\n",
    "    def bias_calc(self,x_test,y_test):\n",
    "        return MAPE(y_test, self.model.predict(x_test).squeeze())\n",
    "# Example usage\n",
    "# Assuming x_train, y_train, x_test, y_test are your training and testing data\n",
    "input_shape = (None,)  # Input shape is flexible\n",
    "model = NN(input_shape=(X_train.shape[1],))\n",
    "model.compile_model()\n",
    "model.train_model(X_train, train_labels)\n",
    "accuracy = model.evaluate_model(X_test, test_labels)\n",
    "print(f\"Test accuracy: {accuracy}\")\n",
    "print(f\"Test R2: {model.r2_calc(X_test, test_labels)}\")\n",
    "print(f\"Test Bias (%): {model.bias_calc(X_test, test_labels)}\")\n",
    "print(\"GEESEBAL accuracy (RMSE)=\",rmse(test_labels,test_features_all[\"LE_Daily_model\"]) )\n",
    "print(\"GEESEBAL R2 =\",r2_score(test_labels,test_features_all[\"LE_Daily_model\"]) )\n",
    "print(\"GEESEBAL MAPE =\",MAPE(test_labels,test_features_all[\"LE_Daily_model\"]) )\n",
    "# def r2_score(y_test, y_pred):\n",
    "#     SS_res =  K.sum(K.square( y_test-y_pred )) \n",
    "#     SS_tot = K.sum(K.square( y_test - K.mean(y_test) ) ) \n",
    "#     return ( 1 - SS_res/(SS_tot) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Doing a torch version of this \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error as MAPE\n",
    "## Split data into features and labels\n",
    "features=['ALFA', 'Tao_sw_1', 'Rs_down',\n",
    "       'Rl_down', 'Rl_up', 'Rn','VV_norm', \n",
    "       'VH_norm', 'VV_red', 'VV_green', 'VV_blue', 'VV_NIR', 'VV_SWIR1',\n",
    "       'VV_SWIR2', 'VV_NDVI', 'VV_NDWI', 'VH_red', 'VH_green', 'VH_blue',\n",
    "       'VH_NIR', 'VH_SWIR1', 'VH_SWIR2', 'VH_NDVI', 'VH_NDWI', 'VH_LST',\n",
    "       'VV_LST', 'VH-VV',\"Ginst\",\"Hinst\",\"LEinst\",\"Gap\",\"LE_Daily_model\"]\n",
    "labels=[\"LE_closed\"]\n",
    "# ml_df=df[[\"B\",\"R\",\"GR\",\"NIR\",\"SWIR_1\",\"SWIR_2\",\"NDVI\",\"NDWI\",\"ALFA\",\"Rs_down\",\"Rl_down\",\"Rl_up\",\"Rn\",\"Ginst\",\"Hinst\",\"LEinst\",\"LE_closed\",\"LE_Daily_model\"]]\n",
    "# ml_df=ml_df.dropna()\n",
    "# print(ml_df.shape)\n",
    "# features=ml_df[[\"B\",\"R\",\"GR\",\"NIR\",\"SWIR_1\",\"SWIR_2\",\"NDVI\",\"NDWI\",\"ALFA\",\"Rs_down\",\"Rl_down\",\"Rl_up\",\"Rn\",\"Ginst\",\"Hinst\",\"LEinst\",\"LE_Daily_model\"]]\n",
    "# labels=ml_df[\"LE_closed\"]\n",
    "# print(features.shape)\n",
    "## Create a train set to include the geesebal output \n",
    "train_features_all=pd.concat(train)[features].dropna()\n",
    "test_features_all=pd.concat(val)[features].dropna()\n",
    "train_features=pd.concat(train)[features].dropna().drop(columns=\"LE_Daily_model\")\n",
    "train_features\n",
    "test_features=pd.concat(val)[features].dropna().drop(columns=\"LE_Daily_model\")\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_features)\n",
    "X_test = scaler.transform(test_features)\n",
    "# ## labels\n",
    "train_labels=np.array(pd.concat(train)[labels])\n",
    "test_labels=np.array(pd.concat(val)[labels]).squeeze()\n",
    "X_train=torch.tensor(X_train).to(torch.float32)\n",
    "X_test=torch.tensor(X_test).to(torch.float32)\n",
    "train_labels=torch.tensor(train_labels).to(torch.float32)\n",
    "test_labels=torch.tensor(test_labels).to(torch.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000, Loss: 12785.968424479166\n",
      "Epoch 2/3000, Loss: 12664.617350260416\n",
      "Epoch 3/3000, Loss: 12762.087239583334\n",
      "Epoch 4/3000, Loss: 12611.645345052084\n",
      "Epoch 5/3000, Loss: 12478.870442708334\n",
      "Epoch 6/3000, Loss: 11384.363932291666\n",
      "Epoch 7/3000, Loss: 9033.610595703125\n",
      "Epoch 8/3000, Loss: 6565.031005859375\n",
      "Epoch 9/3000, Loss: 5596.461018880208\n",
      "Epoch 10/3000, Loss: 4610.12548828125\n",
      "Epoch 11/3000, Loss: 4594.792643229167\n",
      "Epoch 12/3000, Loss: 4073.666707356771\n",
      "Epoch 13/3000, Loss: 4069.15087890625\n",
      "Epoch 14/3000, Loss: 4292.7804768880205\n",
      "Epoch 15/3000, Loss: 3796.9263509114585\n",
      "Epoch 16/3000, Loss: 3726.377482096354\n",
      "Epoch 17/3000, Loss: 3520.168416341146\n",
      "Epoch 18/3000, Loss: 3556.0406494140625\n",
      "Epoch 19/3000, Loss: 3222.1298014322915\n",
      "Epoch 20/3000, Loss: 3203.1713053385415\n",
      "Epoch 21/3000, Loss: 3512.0342610677085\n",
      "Epoch 22/3000, Loss: 3436.1710611979165\n",
      "Epoch 23/3000, Loss: 3106.040283203125\n",
      "Epoch 24/3000, Loss: 3219.96337890625\n",
      "Epoch 25/3000, Loss: 3314.3424072265625\n",
      "Epoch 26/3000, Loss: 2924.109659830729\n",
      "Epoch 27/3000, Loss: 3055.2606608072915\n",
      "Epoch 28/3000, Loss: 2963.4383951822915\n",
      "Epoch 29/3000, Loss: 3129.1351318359375\n",
      "Epoch 30/3000, Loss: 2816.699015299479\n",
      "Epoch 31/3000, Loss: 2883.6212158203125\n",
      "Epoch 32/3000, Loss: 2795.446980794271\n",
      "Epoch 33/3000, Loss: 2869.677042643229\n",
      "Epoch 34/3000, Loss: 2819.5028076171875\n",
      "Epoch 35/3000, Loss: 2721.2058919270835\n",
      "Epoch 36/3000, Loss: 2746.8162841796875\n",
      "Epoch 37/3000, Loss: 2581.4169921875\n",
      "Epoch 38/3000, Loss: 2698.774169921875\n",
      "Epoch 39/3000, Loss: 2535.1849772135415\n",
      "Epoch 40/3000, Loss: 2540.4591064453125\n",
      "Epoch 41/3000, Loss: 2632.806111653646\n",
      "Epoch 42/3000, Loss: 2596.0291544596353\n",
      "Epoch 43/3000, Loss: 2720.9210205078125\n",
      "Epoch 44/3000, Loss: 2540.6697998046875\n",
      "Epoch 45/3000, Loss: 2613.3370768229165\n",
      "Epoch 46/3000, Loss: 2601.2105305989585\n",
      "Epoch 47/3000, Loss: 2593.2166748046875\n",
      "Epoch 48/3000, Loss: 2498.5397135416665\n",
      "Epoch 49/3000, Loss: 2586.0730794270835\n",
      "Epoch 50/3000, Loss: 2449.1424967447915\n",
      "Epoch 51/3000, Loss: 2510.507080078125\n",
      "Epoch 52/3000, Loss: 2481.0465494791665\n",
      "Epoch 53/3000, Loss: 2404.005900065104\n",
      "Epoch 54/3000, Loss: 2381.8416341145835\n",
      "Epoch 55/3000, Loss: 2474.0667317708335\n",
      "Epoch 56/3000, Loss: 2441.022705078125\n",
      "Epoch 57/3000, Loss: 2438.9409993489585\n",
      "Epoch 58/3000, Loss: 2424.9545084635415\n",
      "Epoch 59/3000, Loss: 2500.6676432291665\n",
      "Epoch 60/3000, Loss: 2437.7764485677085\n",
      "Epoch 61/3000, Loss: 2462.4347330729165\n",
      "Epoch 62/3000, Loss: 2574.9618326822915\n",
      "Epoch 63/3000, Loss: 2275.99267578125\n",
      "Epoch 64/3000, Loss: 2344.8299560546875\n",
      "Epoch 65/3000, Loss: 2226.9215494791665\n",
      "Epoch 66/3000, Loss: 2419.0419921875\n",
      "Epoch 67/3000, Loss: 2400.2398478190103\n",
      "Epoch 68/3000, Loss: 2542.8242797851562\n",
      "Epoch 69/3000, Loss: 2368.120890299479\n",
      "Epoch 70/3000, Loss: 2503.464314778646\n",
      "Epoch 71/3000, Loss: 2463.4403279622397\n",
      "Epoch 72/3000, Loss: 2329.5253295898438\n",
      "Epoch 73/3000, Loss: 2292.8697916666665\n",
      "Epoch 74/3000, Loss: 2247.4713745117188\n",
      "Epoch 75/3000, Loss: 2266.1378580729165\n",
      "Epoch 76/3000, Loss: 2268.9583943684897\n",
      "Epoch 77/3000, Loss: 2281.042928059896\n",
      "Epoch 78/3000, Loss: 2391.0924479166665\n",
      "Epoch 79/3000, Loss: 2270.6522420247397\n",
      "Epoch 80/3000, Loss: 2289.5799560546875\n",
      "Epoch 81/3000, Loss: 2230.5036010742188\n",
      "Epoch 82/3000, Loss: 2237.6603393554688\n",
      "Epoch 83/3000, Loss: 2164.2301432291665\n",
      "Epoch 84/3000, Loss: 2241.0439453125\n",
      "Epoch 85/3000, Loss: 2284.3763427734375\n",
      "Epoch 86/3000, Loss: 2337.0997314453125\n",
      "Epoch 87/3000, Loss: 2183.700887044271\n",
      "Epoch 88/3000, Loss: 2258.7376708984375\n",
      "Epoch 89/3000, Loss: 2192.6026000976562\n",
      "Epoch 90/3000, Loss: 2307.909871419271\n",
      "Epoch 91/3000, Loss: 2251.9383341471353\n",
      "Epoch 92/3000, Loss: 2389.8357950846353\n",
      "Epoch 93/3000, Loss: 2226.5250244140625\n",
      "Epoch 94/3000, Loss: 2186.4462890625\n",
      "Epoch 95/3000, Loss: 2150.685506184896\n",
      "Epoch 96/3000, Loss: 2166.4667154947915\n",
      "Epoch 97/3000, Loss: 2226.679972330729\n",
      "Epoch 98/3000, Loss: 2243.6553344726562\n",
      "Epoch 99/3000, Loss: 2318.5217081705728\n",
      "Epoch 100/3000, Loss: 2213.7041015625\n",
      "Epoch 101/3000, Loss: 2136.5565185546875\n",
      "Epoch 102/3000, Loss: 2131.7879842122397\n",
      "Epoch 103/3000, Loss: 2073.975382486979\n",
      "Epoch 104/3000, Loss: 2182.2456258138022\n",
      "Epoch 105/3000, Loss: 2257.9012451171875\n",
      "Epoch 106/3000, Loss: 2089.3723958333335\n",
      "Epoch 107/3000, Loss: 2242.892374674479\n",
      "Epoch 108/3000, Loss: 2221.920125325521\n",
      "Epoch 109/3000, Loss: 2206.7225748697915\n",
      "Epoch 110/3000, Loss: 2034.2740681966145\n",
      "Epoch 111/3000, Loss: 2207.7439982096353\n",
      "Epoch 112/3000, Loss: 2245.3080240885415\n",
      "Epoch 113/3000, Loss: 2076.4640096028647\n",
      "Epoch 114/3000, Loss: 2056.1053670247397\n",
      "Epoch 115/3000, Loss: 2094.459187825521\n",
      "Epoch 116/3000, Loss: 2022.3987833658855\n",
      "Epoch 117/3000, Loss: 2152.44873046875\n",
      "Epoch 118/3000, Loss: 2051.7686767578125\n",
      "Epoch 119/3000, Loss: 2035.2767944335938\n",
      "Epoch 120/3000, Loss: 2020.4131876627605\n",
      "Epoch 121/3000, Loss: 2074.477254231771\n",
      "Epoch 122/3000, Loss: 1978.0389607747395\n",
      "Epoch 123/3000, Loss: 2083.537150065104\n",
      "Epoch 124/3000, Loss: 2150.293701171875\n",
      "Epoch 125/3000, Loss: 2041.2189127604167\n",
      "Epoch 126/3000, Loss: 2096.998779296875\n",
      "Epoch 127/3000, Loss: 2007.0332641601562\n",
      "Epoch 128/3000, Loss: 2058.6329142252603\n",
      "Epoch 129/3000, Loss: 1982.1189778645833\n",
      "Epoch 130/3000, Loss: 2166.2500813802085\n",
      "Epoch 131/3000, Loss: 2042.1864217122395\n",
      "Epoch 132/3000, Loss: 1977.4541625976562\n",
      "Epoch 133/3000, Loss: 2020.9825032552083\n",
      "Epoch 134/3000, Loss: 2094.578653971354\n",
      "Epoch 135/3000, Loss: 2053.316162109375\n",
      "Epoch 136/3000, Loss: 1987.3016764322917\n",
      "Epoch 137/3000, Loss: 2066.5452473958335\n",
      "Epoch 138/3000, Loss: 2072.5961100260415\n",
      "Epoch 139/3000, Loss: 2095.4952189127603\n",
      "Epoch 140/3000, Loss: 1950.8420817057292\n",
      "Epoch 141/3000, Loss: 2098.5184733072915\n",
      "Epoch 142/3000, Loss: 2056.2085774739585\n",
      "Epoch 143/3000, Loss: 2166.7942708333335\n",
      "Epoch 144/3000, Loss: 2062.7357381184897\n",
      "Epoch 145/3000, Loss: 2163.1456502278647\n",
      "Epoch 146/3000, Loss: 2036.9613037109375\n",
      "Epoch 147/3000, Loss: 1934.8699137369792\n",
      "Epoch 148/3000, Loss: 2111.6913248697915\n",
      "Epoch 149/3000, Loss: 1904.1405232747395\n",
      "Epoch 150/3000, Loss: 2100.597941080729\n",
      "Epoch 151/3000, Loss: 1985.1169840494792\n",
      "Epoch 152/3000, Loss: 1965.8933715820312\n",
      "Epoch 153/3000, Loss: 1919.236083984375\n",
      "Epoch 154/3000, Loss: 2037.1832682291667\n",
      "Epoch 155/3000, Loss: 1932.262674967448\n",
      "Epoch 156/3000, Loss: 1878.2562866210938\n",
      "Epoch 157/3000, Loss: 1972.105977376302\n",
      "Epoch 158/3000, Loss: 2034.4652303059895\n",
      "Epoch 159/3000, Loss: 1938.9299723307292\n",
      "Epoch 160/3000, Loss: 2011.1336466471355\n",
      "Epoch 161/3000, Loss: 1925.5970865885417\n",
      "Epoch 162/3000, Loss: 1928.3626098632812\n",
      "Epoch 163/3000, Loss: 1830.3043212890625\n",
      "Epoch 164/3000, Loss: 1931.6534016927083\n",
      "Epoch 165/3000, Loss: 1899.8802490234375\n",
      "Epoch 166/3000, Loss: 1827.261454264323\n",
      "Epoch 167/3000, Loss: 2014.401123046875\n",
      "Epoch 168/3000, Loss: 1843.2933349609375\n",
      "Epoch 169/3000, Loss: 1941.8033854166667\n",
      "Epoch 170/3000, Loss: 2036.990498860677\n",
      "Epoch 171/3000, Loss: 2001.905049641927\n",
      "Epoch 172/3000, Loss: 1961.8848063151042\n",
      "Epoch 173/3000, Loss: 2089.026570638021\n",
      "Epoch 174/3000, Loss: 1938.8174235026042\n",
      "Epoch 175/3000, Loss: 2123.842488606771\n",
      "Epoch 176/3000, Loss: 1880.2813313802083\n",
      "Epoch 177/3000, Loss: 1854.0645141601562\n",
      "Epoch 178/3000, Loss: 1866.5092366536458\n",
      "Epoch 179/3000, Loss: 2059.7854817708335\n",
      "Epoch 180/3000, Loss: 1920.4456583658855\n",
      "Epoch 181/3000, Loss: 1883.9303181966145\n",
      "Epoch 182/3000, Loss: 1990.3877970377605\n",
      "Epoch 183/3000, Loss: 1950.1926676432292\n",
      "Epoch 184/3000, Loss: 1933.6022542317708\n",
      "Epoch 185/3000, Loss: 1925.3983357747395\n",
      "Epoch 186/3000, Loss: 1894.3113606770833\n",
      "Epoch 187/3000, Loss: 2033.49267578125\n",
      "Epoch 188/3000, Loss: 2048.6255493164062\n",
      "Epoch 189/3000, Loss: 2139.1668701171875\n",
      "Epoch 190/3000, Loss: 2060.1749471028647\n",
      "Epoch 191/3000, Loss: 1840.4112955729167\n",
      "Epoch 192/3000, Loss: 1880.3787027994792\n",
      "Epoch 193/3000, Loss: 1866.2266845703125\n",
      "Epoch 194/3000, Loss: 1943.6043090820312\n",
      "Epoch 195/3000, Loss: 1909.0481974283855\n",
      "Epoch 196/3000, Loss: 1865.5939331054688\n",
      "Epoch 197/3000, Loss: 2024.6072591145833\n",
      "Epoch 198/3000, Loss: 1996.682108561198\n",
      "Epoch 199/3000, Loss: 1902.3162434895833\n",
      "Epoch 200/3000, Loss: 1959.5723876953125\n",
      "Epoch 201/3000, Loss: 1807.158426920573\n",
      "Epoch 202/3000, Loss: 1995.950215657552\n",
      "Epoch 203/3000, Loss: 1961.1676228841145\n",
      "Epoch 204/3000, Loss: 1804.1807250976562\n",
      "Epoch 205/3000, Loss: 1809.8050537109375\n",
      "Epoch 206/3000, Loss: 1867.64794921875\n",
      "Epoch 207/3000, Loss: 1975.560038248698\n",
      "Epoch 208/3000, Loss: 1762.2107950846355\n",
      "Epoch 209/3000, Loss: 1954.6929931640625\n",
      "Epoch 210/3000, Loss: 1890.705546061198\n",
      "Epoch 211/3000, Loss: 1859.6839599609375\n",
      "Epoch 212/3000, Loss: 1820.0005900065105\n",
      "Epoch 213/3000, Loss: 1925.024393717448\n",
      "Epoch 214/3000, Loss: 1822.0912068684895\n",
      "Epoch 215/3000, Loss: 1896.2753295898438\n",
      "Epoch 216/3000, Loss: 1913.7593383789062\n",
      "Epoch 217/3000, Loss: 1830.903584798177\n",
      "Epoch 218/3000, Loss: 1805.107401529948\n",
      "Epoch 219/3000, Loss: 1852.3890380859375\n",
      "Epoch 220/3000, Loss: 1962.7172444661458\n",
      "Epoch 221/3000, Loss: 1779.2719319661458\n",
      "Epoch 222/3000, Loss: 1856.2399291992188\n",
      "Epoch 223/3000, Loss: 1860.6632080078125\n",
      "Epoch 224/3000, Loss: 1836.804178873698\n",
      "Epoch 225/3000, Loss: 1839.8053385416667\n",
      "Epoch 226/3000, Loss: 1919.3943684895833\n",
      "Epoch 227/3000, Loss: 1790.8765869140625\n",
      "Epoch 228/3000, Loss: 1749.3209025065105\n",
      "Epoch 229/3000, Loss: 1853.471211751302\n",
      "Epoch 230/3000, Loss: 1695.7671508789062\n",
      "Epoch 231/3000, Loss: 1899.3497111002605\n",
      "Epoch 232/3000, Loss: 1760.886454264323\n",
      "Epoch 233/3000, Loss: 1867.1334635416667\n",
      "Epoch 234/3000, Loss: 1804.002197265625\n",
      "Epoch 235/3000, Loss: 1826.5737915039062\n",
      "Epoch 236/3000, Loss: 1743.932352701823\n",
      "Epoch 237/3000, Loss: 1881.1146240234375\n",
      "Epoch 238/3000, Loss: 1750.1993001302083\n",
      "Epoch 239/3000, Loss: 1794.3951212565105\n",
      "Epoch 240/3000, Loss: 1740.884033203125\n",
      "Epoch 241/3000, Loss: 1745.502421061198\n",
      "Epoch 242/3000, Loss: 1772.8157348632812\n",
      "Epoch 243/3000, Loss: 1891.2144775390625\n",
      "Epoch 244/3000, Loss: 1803.5476481119792\n",
      "Epoch 245/3000, Loss: 1721.3226928710938\n",
      "Epoch 246/3000, Loss: 1839.6654459635417\n",
      "Epoch 247/3000, Loss: 1720.9179077148438\n",
      "Epoch 248/3000, Loss: 1775.8469848632812\n",
      "Epoch 249/3000, Loss: 1799.2107340494792\n",
      "Epoch 250/3000, Loss: 1730.7310994466145\n",
      "Epoch 251/3000, Loss: 1796.1621907552083\n",
      "Epoch 252/3000, Loss: 1890.54541015625\n",
      "Epoch 253/3000, Loss: 1877.4219970703125\n",
      "Epoch 254/3000, Loss: 1810.898457845052\n",
      "Epoch 255/3000, Loss: 1830.947489420573\n",
      "Epoch 256/3000, Loss: 1754.4087524414062\n",
      "Epoch 257/3000, Loss: 1760.790771484375\n",
      "Epoch 258/3000, Loss: 1782.982889811198\n",
      "Epoch 259/3000, Loss: 1803.6051025390625\n",
      "Epoch 260/3000, Loss: 1707.8192545572917\n",
      "Epoch 261/3000, Loss: 1873.5552571614583\n",
      "Epoch 262/3000, Loss: 1737.823954264323\n",
      "Epoch 263/3000, Loss: 1739.7543334960938\n",
      "Epoch 264/3000, Loss: 1730.0200805664062\n",
      "Epoch 265/3000, Loss: 1848.0613403320312\n",
      "Epoch 266/3000, Loss: 1758.635986328125\n",
      "Epoch 267/3000, Loss: 1763.5029296875\n",
      "Epoch 268/3000, Loss: 1696.047587076823\n",
      "Epoch 269/3000, Loss: 1774.1156209309895\n",
      "Epoch 270/3000, Loss: 1835.0941162109375\n",
      "Epoch 271/3000, Loss: 1820.4479370117188\n",
      "Epoch 272/3000, Loss: 1762.7347615559895\n",
      "Epoch 273/3000, Loss: 1764.1075032552083\n",
      "Epoch 274/3000, Loss: 1690.954610188802\n",
      "Epoch 275/3000, Loss: 1768.8087158203125\n",
      "Epoch 276/3000, Loss: 1754.6089680989583\n",
      "Epoch 277/3000, Loss: 1677.7842814127605\n",
      "Epoch 278/3000, Loss: 1776.53271484375\n",
      "Epoch 279/3000, Loss: 1793.0604248046875\n",
      "Epoch 280/3000, Loss: 1697.4415283203125\n",
      "Epoch 281/3000, Loss: 1715.1651204427083\n",
      "Epoch 282/3000, Loss: 1899.8201904296875\n",
      "Epoch 283/3000, Loss: 1895.1607462565105\n",
      "Epoch 284/3000, Loss: 1711.3668212890625\n",
      "Epoch 285/3000, Loss: 1694.1110432942708\n",
      "Epoch 286/3000, Loss: 1692.0188395182292\n",
      "Epoch 287/3000, Loss: 1779.7967122395833\n",
      "Epoch 288/3000, Loss: 1768.669921875\n",
      "Epoch 289/3000, Loss: 1693.37451171875\n",
      "Epoch 290/3000, Loss: 1702.7935384114583\n",
      "Epoch 291/3000, Loss: 1709.8771565755208\n",
      "Epoch 292/3000, Loss: 1628.7540893554688\n",
      "Epoch 293/3000, Loss: 1792.0289916992188\n",
      "Epoch 294/3000, Loss: 1624.739766438802\n",
      "Epoch 295/3000, Loss: 1733.8369750976562\n",
      "Epoch 296/3000, Loss: 1815.3583374023438\n",
      "Epoch 297/3000, Loss: 1659.8956705729167\n",
      "Epoch 298/3000, Loss: 1749.1524251302083\n",
      "Epoch 299/3000, Loss: 1720.7931722005208\n",
      "Epoch 300/3000, Loss: 1772.248270670573\n",
      "Epoch 301/3000, Loss: 1814.2432047526042\n",
      "Epoch 302/3000, Loss: 1775.8585815429688\n",
      "Epoch 303/3000, Loss: 1703.6072184244792\n",
      "Epoch 304/3000, Loss: 1794.7156575520833\n",
      "Epoch 305/3000, Loss: 1712.3573608398438\n",
      "Epoch 306/3000, Loss: 1679.9454142252605\n",
      "Epoch 307/3000, Loss: 1786.697977701823\n",
      "Epoch 308/3000, Loss: 1754.9791870117188\n",
      "Epoch 309/3000, Loss: 1684.2012125651042\n",
      "Epoch 310/3000, Loss: 1617.650126139323\n",
      "Epoch 311/3000, Loss: 1721.5171305338542\n",
      "Epoch 312/3000, Loss: 1664.9303181966145\n",
      "Epoch 313/3000, Loss: 1759.138651529948\n",
      "Epoch 314/3000, Loss: 1669.4051717122395\n",
      "Epoch 315/3000, Loss: 1783.10107421875\n",
      "Epoch 316/3000, Loss: 1671.7306722005208\n",
      "Epoch 317/3000, Loss: 1710.9633178710938\n",
      "Epoch 318/3000, Loss: 1778.7498575846355\n",
      "Epoch 319/3000, Loss: 1661.9825236002605\n",
      "Epoch 320/3000, Loss: 1580.245096842448\n",
      "Epoch 321/3000, Loss: 1699.7416585286458\n",
      "Epoch 322/3000, Loss: 1699.4464111328125\n",
      "Epoch 323/3000, Loss: 1682.3492431640625\n",
      "Epoch 324/3000, Loss: 1687.0456949869792\n",
      "Epoch 325/3000, Loss: 1772.883544921875\n",
      "Epoch 326/3000, Loss: 1614.6351318359375\n",
      "Epoch 327/3000, Loss: 1757.650410970052\n",
      "Epoch 328/3000, Loss: 1718.3499959309895\n",
      "Epoch 329/3000, Loss: 1655.608154296875\n",
      "Epoch 330/3000, Loss: 1727.6580403645833\n",
      "Epoch 331/3000, Loss: 1606.9723307291667\n",
      "Epoch 332/3000, Loss: 1810.2075602213542\n",
      "Epoch 333/3000, Loss: 1593.1356811523438\n",
      "Epoch 334/3000, Loss: 1721.9785970052083\n",
      "Epoch 335/3000, Loss: 1703.5879923502605\n",
      "Epoch 336/3000, Loss: 1686.40234375\n",
      "Epoch 337/3000, Loss: 1781.661641438802\n",
      "Epoch 338/3000, Loss: 1703.4413045247395\n",
      "Epoch 339/3000, Loss: 1609.3260498046875\n",
      "Epoch 340/3000, Loss: 1666.0570678710938\n",
      "Epoch 341/3000, Loss: 1699.9967244466145\n",
      "Epoch 342/3000, Loss: 1727.224833170573\n",
      "Epoch 343/3000, Loss: 1623.2543538411458\n",
      "Epoch 344/3000, Loss: 1672.4586588541667\n",
      "Epoch 345/3000, Loss: 1704.5166422526042\n",
      "Epoch 346/3000, Loss: 1599.8602701822917\n",
      "Epoch 347/3000, Loss: 1637.775899251302\n",
      "Epoch 348/3000, Loss: 1558.8231201171875\n",
      "Epoch 349/3000, Loss: 1617.1357828776042\n",
      "Epoch 350/3000, Loss: 1777.8582560221355\n",
      "Epoch 351/3000, Loss: 1625.7152913411458\n",
      "Epoch 352/3000, Loss: 1680.3283081054688\n",
      "Epoch 353/3000, Loss: 1710.317403157552\n",
      "Epoch 354/3000, Loss: 1745.5968831380208\n",
      "Epoch 355/3000, Loss: 1522.963399251302\n",
      "Epoch 356/3000, Loss: 1701.1299845377605\n",
      "Epoch 357/3000, Loss: 1711.0408325195312\n",
      "Epoch 358/3000, Loss: 1611.4578450520833\n",
      "Epoch 359/3000, Loss: 1635.5799763997395\n",
      "Epoch 360/3000, Loss: 1663.03173828125\n",
      "Epoch 361/3000, Loss: 1640.3215942382812\n",
      "Epoch 362/3000, Loss: 1538.202392578125\n",
      "Epoch 363/3000, Loss: 1673.0677897135417\n",
      "Epoch 364/3000, Loss: 1622.7547200520833\n",
      "Epoch 365/3000, Loss: 1701.076883951823\n",
      "Epoch 366/3000, Loss: 1595.0424397786458\n",
      "Epoch 367/3000, Loss: 1667.6417032877605\n",
      "Epoch 368/3000, Loss: 1693.552754720052\n",
      "Epoch 369/3000, Loss: 1647.3555094401042\n",
      "Epoch 370/3000, Loss: 1632.3528645833333\n",
      "Epoch 371/3000, Loss: 1664.1724243164062\n",
      "Epoch 372/3000, Loss: 1683.1466064453125\n",
      "Epoch 373/3000, Loss: 1782.0769856770833\n",
      "Epoch 374/3000, Loss: 1631.9671427408855\n",
      "Epoch 375/3000, Loss: 1638.4765014648438\n",
      "Epoch 376/3000, Loss: 1647.4672037760417\n",
      "Epoch 377/3000, Loss: 1657.080830891927\n",
      "Epoch 378/3000, Loss: 1572.778076171875\n",
      "Epoch 379/3000, Loss: 1593.586690266927\n",
      "Epoch 380/3000, Loss: 1530.9896443684895\n",
      "Epoch 381/3000, Loss: 1706.0155029296875\n",
      "Epoch 382/3000, Loss: 1776.3233846028645\n",
      "Epoch 383/3000, Loss: 1623.2341715494792\n",
      "Epoch 384/3000, Loss: 1743.5519205729167\n",
      "Epoch 385/3000, Loss: 1704.680928548177\n",
      "Epoch 386/3000, Loss: 1751.8298746744792\n",
      "Epoch 387/3000, Loss: 1597.368408203125\n",
      "Epoch 388/3000, Loss: 1702.0939331054688\n",
      "Epoch 389/3000, Loss: 1657.3695882161458\n",
      "Epoch 390/3000, Loss: 1604.7040608723958\n",
      "Epoch 391/3000, Loss: 1598.5675862630208\n",
      "Epoch 392/3000, Loss: 1619.4086100260417\n",
      "Epoch 393/3000, Loss: 1777.2852783203125\n",
      "Epoch 394/3000, Loss: 1649.715352376302\n",
      "Epoch 395/3000, Loss: 1749.5126953125\n",
      "Epoch 396/3000, Loss: 1631.5531616210938\n",
      "Epoch 397/3000, Loss: 1531.7844848632812\n",
      "Epoch 398/3000, Loss: 1589.352518717448\n",
      "Epoch 399/3000, Loss: 1595.3056233723958\n",
      "Epoch 400/3000, Loss: 1677.4331868489583\n",
      "Epoch 401/3000, Loss: 1573.125\n",
      "Epoch 402/3000, Loss: 1504.1398518880208\n",
      "Epoch 403/3000, Loss: 1592.369160970052\n",
      "Epoch 404/3000, Loss: 1562.436991373698\n",
      "Epoch 405/3000, Loss: 1602.075703938802\n",
      "Epoch 406/3000, Loss: 1673.6426595052083\n",
      "Epoch 407/3000, Loss: 1718.7711995442708\n",
      "Epoch 408/3000, Loss: 1620.154805501302\n",
      "Epoch 409/3000, Loss: 1590.5208943684895\n",
      "Epoch 410/3000, Loss: 1647.5629069010417\n",
      "Epoch 411/3000, Loss: 1471.194071451823\n",
      "Epoch 412/3000, Loss: 1708.918212890625\n",
      "Epoch 413/3000, Loss: 1759.2525024414062\n",
      "Epoch 414/3000, Loss: 1623.1898600260417\n",
      "Epoch 415/3000, Loss: 1658.0335489908855\n",
      "Epoch 416/3000, Loss: 1660.5925903320312\n",
      "Epoch 417/3000, Loss: 1684.4751383463542\n",
      "Epoch 418/3000, Loss: 1529.89013671875\n",
      "Epoch 419/3000, Loss: 1578.692626953125\n",
      "Epoch 420/3000, Loss: 1625.3725179036458\n",
      "Epoch 421/3000, Loss: 1647.418212890625\n",
      "Epoch 422/3000, Loss: 1550.8551432291667\n",
      "Epoch 423/3000, Loss: 1573.9379069010417\n",
      "Epoch 424/3000, Loss: 1579.487040201823\n",
      "Epoch 425/3000, Loss: 1654.3041178385417\n",
      "Epoch 426/3000, Loss: 1574.5797322591145\n",
      "Epoch 427/3000, Loss: 1611.3527221679688\n",
      "Epoch 428/3000, Loss: 1633.850321451823\n",
      "Epoch 429/3000, Loss: 1604.3429361979167\n",
      "Epoch 430/3000, Loss: 1508.987040201823\n",
      "Epoch 431/3000, Loss: 1570.5040283203125\n",
      "Epoch 432/3000, Loss: 1623.5021362304688\n",
      "Epoch 433/3000, Loss: 1573.2525227864583\n",
      "Epoch 434/3000, Loss: 1622.2313435872395\n",
      "Epoch 435/3000, Loss: 1604.9923299153645\n",
      "Epoch 436/3000, Loss: 1577.5755411783855\n",
      "Epoch 437/3000, Loss: 1555.205586751302\n",
      "Epoch 438/3000, Loss: 1585.7413126627605\n",
      "Epoch 439/3000, Loss: 1558.4551798502605\n",
      "Epoch 440/3000, Loss: 1549.0588785807292\n",
      "Epoch 441/3000, Loss: 1640.1517130533855\n",
      "Epoch 442/3000, Loss: 1612.924540201823\n",
      "Epoch 443/3000, Loss: 1548.7841796875\n",
      "Epoch 444/3000, Loss: 1609.5834147135417\n",
      "Epoch 445/3000, Loss: 1555.735616048177\n",
      "Epoch 446/3000, Loss: 1619.5389607747395\n",
      "Epoch 447/3000, Loss: 1622.7472737630208\n",
      "Epoch 448/3000, Loss: 1491.9727783203125\n",
      "Epoch 449/3000, Loss: 1583.759297688802\n",
      "Epoch 450/3000, Loss: 1626.5323893229167\n",
      "Epoch 451/3000, Loss: 1628.2939453125\n",
      "Epoch 452/3000, Loss: 1658.7443440755208\n",
      "Epoch 453/3000, Loss: 1592.039815266927\n",
      "Epoch 454/3000, Loss: 1726.061299641927\n",
      "Epoch 455/3000, Loss: 1563.1178792317708\n",
      "Epoch 456/3000, Loss: 1634.0396321614583\n",
      "Epoch 457/3000, Loss: 1532.2446695963542\n",
      "Epoch 458/3000, Loss: 1570.1727294921875\n",
      "Epoch 459/3000, Loss: 1581.2943522135417\n",
      "Epoch 460/3000, Loss: 1617.2522583007812\n",
      "Epoch 461/3000, Loss: 1611.7108968098958\n",
      "Epoch 462/3000, Loss: 1553.6343994140625\n",
      "Epoch 463/3000, Loss: 1548.3602701822917\n",
      "Epoch 464/3000, Loss: 1630.4857381184895\n",
      "Epoch 465/3000, Loss: 1503.4704182942708\n",
      "Epoch 466/3000, Loss: 1696.0259399414062\n",
      "Epoch 467/3000, Loss: 1558.5682983398438\n",
      "Epoch 468/3000, Loss: 1584.961669921875\n",
      "Epoch 469/3000, Loss: 1524.3493245442708\n",
      "Epoch 470/3000, Loss: 1596.5343017578125\n",
      "Epoch 471/3000, Loss: 1517.2093302408855\n",
      "Epoch 472/3000, Loss: 1550.831787109375\n",
      "Epoch 473/3000, Loss: 1697.936991373698\n",
      "Epoch 474/3000, Loss: 1540.6919962565105\n",
      "Epoch 475/3000, Loss: 1520.1524861653645\n",
      "Epoch 476/3000, Loss: 1550.479756673177\n",
      "Epoch 477/3000, Loss: 1659.6654866536458\n",
      "Epoch 478/3000, Loss: 1646.3553670247395\n",
      "Epoch 479/3000, Loss: 1656.2152913411458\n",
      "Epoch 480/3000, Loss: 1622.353291829427\n",
      "Epoch 481/3000, Loss: 1652.1209106445312\n",
      "Epoch 482/3000, Loss: 1509.6201782226562\n",
      "Epoch 483/3000, Loss: 1652.3259887695312\n",
      "Epoch 484/3000, Loss: 1551.1450602213542\n",
      "Epoch 485/3000, Loss: 1529.35888671875\n",
      "Epoch 486/3000, Loss: 1711.9508666992188\n",
      "Epoch 487/3000, Loss: 1623.652119954427\n",
      "Epoch 488/3000, Loss: 1554.4180297851562\n",
      "Epoch 489/3000, Loss: 1630.39453125\n",
      "Epoch 490/3000, Loss: 1637.1839803059895\n",
      "Epoch 491/3000, Loss: 1561.3292846679688\n",
      "Epoch 492/3000, Loss: 1556.791239420573\n",
      "Epoch 493/3000, Loss: 1519.7037963867188\n",
      "Epoch 494/3000, Loss: 1545.2081705729167\n",
      "Epoch 495/3000, Loss: 1565.6407470703125\n",
      "Epoch 496/3000, Loss: 1558.3530883789062\n",
      "Epoch 497/3000, Loss: 1662.1660766601562\n",
      "Epoch 498/3000, Loss: 1608.2842203776042\n",
      "Epoch 499/3000, Loss: 1528.783426920573\n",
      "Epoch 500/3000, Loss: 1589.9322306315105\n",
      "Epoch 501/3000, Loss: 1525.3287760416667\n",
      "Epoch 502/3000, Loss: 1519.0271809895833\n",
      "Epoch 503/3000, Loss: 1603.8883260091145\n",
      "Epoch 504/3000, Loss: 1596.890869140625\n",
      "Epoch 505/3000, Loss: 1473.9812418619792\n",
      "Epoch 506/3000, Loss: 1516.443827311198\n",
      "Epoch 507/3000, Loss: 1506.1794026692708\n",
      "Epoch 508/3000, Loss: 1506.7900797526042\n",
      "Epoch 509/3000, Loss: 1538.4707438151042\n",
      "Epoch 510/3000, Loss: 1641.650370279948\n",
      "Epoch 511/3000, Loss: 1396.959004720052\n",
      "Epoch 512/3000, Loss: 1621.2501831054688\n",
      "Epoch 513/3000, Loss: 1590.763916015625\n",
      "Epoch 514/3000, Loss: 1593.39404296875\n",
      "Epoch 515/3000, Loss: 1511.093729654948\n",
      "Epoch 516/3000, Loss: 1641.3639526367188\n",
      "Epoch 517/3000, Loss: 1577.2833455403645\n",
      "Epoch 518/3000, Loss: 1556.7467244466145\n",
      "Epoch 519/3000, Loss: 1563.6693929036458\n",
      "Epoch 520/3000, Loss: 1514.811055501302\n",
      "Epoch 521/3000, Loss: 1525.6271362304688\n",
      "Epoch 522/3000, Loss: 1491.926737467448\n",
      "Epoch 523/3000, Loss: 1608.4529622395833\n",
      "Epoch 524/3000, Loss: 1620.2564697265625\n",
      "Epoch 525/3000, Loss: 1562.0770874023438\n",
      "Epoch 526/3000, Loss: 1600.2168782552083\n",
      "Epoch 527/3000, Loss: 1487.7515869140625\n",
      "Epoch 528/3000, Loss: 1527.7593383789062\n",
      "Epoch 529/3000, Loss: 1565.8927408854167\n",
      "Epoch 530/3000, Loss: 1588.9892171223958\n",
      "Epoch 531/3000, Loss: 1518.4519653320312\n",
      "Epoch 532/3000, Loss: 1556.080810546875\n",
      "Epoch 533/3000, Loss: 1496.0524495442708\n",
      "Epoch 534/3000, Loss: 1548.828633626302\n",
      "Epoch 535/3000, Loss: 1437.6862182617188\n",
      "Epoch 536/3000, Loss: 1538.525126139323\n",
      "Epoch 537/3000, Loss: 1527.8091227213542\n",
      "Epoch 538/3000, Loss: 1493.4457194010417\n",
      "Epoch 539/3000, Loss: 1507.408182779948\n",
      "Epoch 540/3000, Loss: 1580.6378987630208\n",
      "Epoch 541/3000, Loss: 1434.3504028320312\n",
      "Epoch 542/3000, Loss: 1560.2291463216145\n",
      "Epoch 543/3000, Loss: 1513.3693237304688\n",
      "Epoch 544/3000, Loss: 1525.3427734375\n",
      "Epoch 545/3000, Loss: 1576.4691162109375\n",
      "Epoch 546/3000, Loss: 1530.9232177734375\n",
      "Epoch 547/3000, Loss: 1512.3494669596355\n",
      "Epoch 548/3000, Loss: 1444.7252604166667\n",
      "Epoch 549/3000, Loss: 1529.5250854492188\n",
      "Epoch 550/3000, Loss: 1511.0865071614583\n",
      "Epoch 551/3000, Loss: 1533.5723470052083\n",
      "Epoch 552/3000, Loss: 1518.7008666992188\n",
      "Epoch 553/3000, Loss: 1602.1097615559895\n",
      "Epoch 554/3000, Loss: 1500.91455078125\n",
      "Epoch 555/3000, Loss: 1571.8153076171875\n",
      "Epoch 556/3000, Loss: 1509.7471923828125\n",
      "Epoch 557/3000, Loss: 1503.328592936198\n",
      "Epoch 558/3000, Loss: 1532.2369181315105\n",
      "Epoch 559/3000, Loss: 1579.3568522135417\n",
      "Epoch 560/3000, Loss: 1521.3864339192708\n",
      "Epoch 561/3000, Loss: 1533.957275390625\n",
      "Epoch 562/3000, Loss: 1515.2504069010417\n",
      "Epoch 563/3000, Loss: 1513.7527465820312\n",
      "Epoch 564/3000, Loss: 1535.1088053385417\n",
      "Epoch 565/3000, Loss: 1541.1299438476562\n",
      "Epoch 566/3000, Loss: 1525.8046875\n",
      "Epoch 567/3000, Loss: 1569.3003946940105\n",
      "Epoch 568/3000, Loss: 1480.5098470052083\n",
      "Epoch 569/3000, Loss: 1578.3994954427083\n",
      "Epoch 570/3000, Loss: 1507.5902099609375\n",
      "Epoch 571/3000, Loss: 1441.7123006184895\n",
      "Epoch 572/3000, Loss: 1481.3722737630208\n",
      "Epoch 573/3000, Loss: 1538.1798502604167\n",
      "Epoch 574/3000, Loss: 1618.5098876953125\n",
      "Epoch 575/3000, Loss: 1540.2686360677083\n",
      "Epoch 576/3000, Loss: 1481.7444254557292\n",
      "Epoch 577/3000, Loss: 1642.488749186198\n",
      "Epoch 578/3000, Loss: 1450.37451171875\n",
      "Epoch 579/3000, Loss: 1523.5044555664062\n",
      "Epoch 580/3000, Loss: 1567.5044555664062\n",
      "Epoch 581/3000, Loss: 1589.1212972005208\n",
      "Epoch 582/3000, Loss: 1477.311747233073\n",
      "Epoch 583/3000, Loss: 1498.7147013346355\n",
      "Epoch 584/3000, Loss: 1540.1681111653645\n",
      "Epoch 585/3000, Loss: 1570.5103352864583\n",
      "Epoch 586/3000, Loss: 1473.7413533528645\n",
      "Epoch 587/3000, Loss: 1447.0899454752605\n",
      "Epoch 588/3000, Loss: 1489.6000569661458\n",
      "Epoch 589/3000, Loss: 1496.454610188802\n",
      "Epoch 590/3000, Loss: 1427.6783040364583\n",
      "Epoch 591/3000, Loss: 1572.152567545573\n",
      "Epoch 592/3000, Loss: 1623.6265462239583\n",
      "Epoch 593/3000, Loss: 1516.082499186198\n",
      "Epoch 594/3000, Loss: 1469.0304768880208\n",
      "Epoch 595/3000, Loss: 1429.3099975585938\n",
      "Epoch 596/3000, Loss: 1430.840352376302\n",
      "Epoch 597/3000, Loss: 1436.1976521809895\n",
      "Epoch 598/3000, Loss: 1481.8236083984375\n",
      "Epoch 599/3000, Loss: 1522.1093139648438\n",
      "Epoch 600/3000, Loss: 1530.845479329427\n",
      "Epoch 601/3000, Loss: 1562.531982421875\n",
      "Epoch 602/3000, Loss: 1432.7993570963542\n",
      "Epoch 603/3000, Loss: 1372.1299845377605\n",
      "Epoch 604/3000, Loss: 1428.078633626302\n",
      "Epoch 605/3000, Loss: 1454.2386067708333\n",
      "Epoch 606/3000, Loss: 1501.0772094726562\n",
      "Epoch 607/3000, Loss: 1536.7572428385417\n",
      "Epoch 608/3000, Loss: 1437.6932779947917\n",
      "Epoch 609/3000, Loss: 1555.4528401692708\n",
      "Epoch 610/3000, Loss: 1466.036397298177\n",
      "Epoch 611/3000, Loss: 1563.4207560221355\n",
      "Epoch 612/3000, Loss: 1491.5491943359375\n",
      "Epoch 613/3000, Loss: 1456.9411214192708\n",
      "Epoch 614/3000, Loss: 1521.6973673502605\n",
      "Epoch 615/3000, Loss: 1499.3754069010417\n",
      "Epoch 616/3000, Loss: 1549.5729370117188\n",
      "Epoch 617/3000, Loss: 1515.9181722005208\n",
      "Epoch 618/3000, Loss: 1495.7925211588542\n",
      "Epoch 619/3000, Loss: 1435.585957845052\n",
      "Epoch 620/3000, Loss: 1476.1527506510417\n",
      "Epoch 621/3000, Loss: 1486.2850952148438\n",
      "Epoch 622/3000, Loss: 1444.0470377604167\n",
      "Epoch 623/3000, Loss: 1471.9156901041667\n",
      "Epoch 624/3000, Loss: 1511.037129720052\n",
      "Epoch 625/3000, Loss: 1468.715799967448\n",
      "Epoch 626/3000, Loss: 1559.0133463541667\n",
      "Epoch 627/3000, Loss: 1418.2062377929688\n",
      "Epoch 628/3000, Loss: 1547.467529296875\n",
      "Epoch 629/3000, Loss: 1546.376444498698\n",
      "Epoch 630/3000, Loss: 1572.6850992838542\n",
      "Epoch 631/3000, Loss: 1511.7918701171875\n",
      "Epoch 632/3000, Loss: 1497.7417805989583\n",
      "Epoch 633/3000, Loss: 1456.0958048502605\n",
      "Epoch 634/3000, Loss: 1421.8489583333333\n",
      "Epoch 635/3000, Loss: 1496.3651326497395\n",
      "Epoch 636/3000, Loss: 1431.7354532877605\n",
      "Epoch 637/3000, Loss: 1452.4441731770833\n",
      "Epoch 638/3000, Loss: 1459.9462280273438\n",
      "Epoch 639/3000, Loss: 1438.174092610677\n",
      "Epoch 640/3000, Loss: 1602.4332275390625\n",
      "Epoch 641/3000, Loss: 1523.7185872395833\n",
      "Epoch 642/3000, Loss: 1471.5130615234375\n",
      "Epoch 643/3000, Loss: 1479.9351806640625\n",
      "Epoch 644/3000, Loss: 1436.971903483073\n",
      "Epoch 645/3000, Loss: 1420.990987141927\n",
      "Epoch 646/3000, Loss: 1358.1554158528645\n",
      "Epoch 647/3000, Loss: 1494.2767333984375\n",
      "Epoch 648/3000, Loss: 1419.01025390625\n",
      "Epoch 649/3000, Loss: 1558.1702067057292\n",
      "Epoch 650/3000, Loss: 1445.9735310872395\n",
      "Epoch 651/3000, Loss: 1452.4958902994792\n",
      "Epoch 652/3000, Loss: 1513.7188924153645\n",
      "Epoch 653/3000, Loss: 1478.2952067057292\n",
      "Epoch 654/3000, Loss: 1490.0911865234375\n",
      "Epoch 655/3000, Loss: 1494.1619059244792\n",
      "Epoch 656/3000, Loss: 1424.7880045572917\n",
      "Epoch 657/3000, Loss: 1505.8270060221355\n",
      "Epoch 658/3000, Loss: 1411.5706990559895\n",
      "Epoch 659/3000, Loss: 1501.1545003255208\n",
      "Epoch 660/3000, Loss: 1527.875467936198\n",
      "Epoch 661/3000, Loss: 1462.0979614257812\n",
      "Epoch 662/3000, Loss: 1463.7698160807292\n",
      "Epoch 663/3000, Loss: 1476.947998046875\n",
      "Epoch 664/3000, Loss: 1500.049540201823\n",
      "Epoch 665/3000, Loss: 1503.2234497070312\n",
      "Epoch 666/3000, Loss: 1455.0499471028645\n",
      "Epoch 667/3000, Loss: 1467.9781697591145\n",
      "Epoch 668/3000, Loss: 1413.3622639973958\n",
      "Epoch 669/3000, Loss: 1484.0306396484375\n",
      "Epoch 670/3000, Loss: 1567.5375162760417\n",
      "Epoch 671/3000, Loss: 1472.3951822916667\n",
      "Epoch 672/3000, Loss: 1450.351786295573\n",
      "Epoch 673/3000, Loss: 1506.0896402994792\n",
      "Epoch 674/3000, Loss: 1524.3011881510417\n",
      "Epoch 675/3000, Loss: 1493.946797688802\n",
      "Epoch 676/3000, Loss: 1481.4602864583333\n",
      "Epoch 677/3000, Loss: 1552.1522827148438\n",
      "Epoch 678/3000, Loss: 1494.5852457682292\n",
      "Epoch 679/3000, Loss: 1494.4979654947917\n",
      "Epoch 680/3000, Loss: 1453.2445068359375\n",
      "Epoch 681/3000, Loss: 1498.220723470052\n",
      "Epoch 682/3000, Loss: 1480.64208984375\n",
      "Epoch 683/3000, Loss: 1505.161865234375\n",
      "Epoch 684/3000, Loss: 1492.082784016927\n",
      "Epoch 685/3000, Loss: 1418.9226481119792\n",
      "Epoch 686/3000, Loss: 1530.7833658854167\n",
      "Epoch 687/3000, Loss: 1488.8047485351562\n",
      "Epoch 688/3000, Loss: 1430.8226318359375\n",
      "Epoch 689/3000, Loss: 1524.4054768880208\n",
      "Epoch 690/3000, Loss: 1432.213887532552\n",
      "Epoch 691/3000, Loss: 1356.5389607747395\n",
      "Epoch 692/3000, Loss: 1480.157470703125\n",
      "Epoch 693/3000, Loss: 1479.8722737630208\n",
      "Epoch 694/3000, Loss: 1444.24658203125\n",
      "Epoch 695/3000, Loss: 1423.90771484375\n",
      "Epoch 696/3000, Loss: 1427.5735270182292\n",
      "Epoch 697/3000, Loss: 1435.185791015625\n",
      "Epoch 698/3000, Loss: 1357.8559977213542\n",
      "Epoch 699/3000, Loss: 1482.3062133789062\n",
      "Epoch 700/3000, Loss: 1507.0906982421875\n",
      "Epoch 701/3000, Loss: 1503.5903727213542\n",
      "Epoch 702/3000, Loss: 1477.9878946940105\n",
      "Epoch 703/3000, Loss: 1404.4414672851562\n",
      "Epoch 704/3000, Loss: 1478.1939493815105\n",
      "Epoch 705/3000, Loss: 1393.4494018554688\n",
      "Epoch 706/3000, Loss: 1334.4686686197917\n",
      "Epoch 707/3000, Loss: 1454.1558634440105\n",
      "Epoch 708/3000, Loss: 1397.4532063802083\n",
      "Epoch 709/3000, Loss: 1496.1721801757812\n",
      "Epoch 710/3000, Loss: 1454.7106119791667\n",
      "Epoch 711/3000, Loss: 1472.8515218098958\n",
      "Epoch 712/3000, Loss: 1432.305928548177\n",
      "Epoch 713/3000, Loss: 1457.8685506184895\n",
      "Epoch 714/3000, Loss: 1368.1186930338542\n",
      "Epoch 715/3000, Loss: 1422.4552205403645\n",
      "Epoch 716/3000, Loss: 1488.4542846679688\n",
      "Epoch 717/3000, Loss: 1414.997314453125\n",
      "Epoch 718/3000, Loss: 1419.4557291666667\n",
      "Epoch 719/3000, Loss: 1461.0135498046875\n",
      "Epoch 720/3000, Loss: 1514.831766764323\n",
      "Epoch 721/3000, Loss: 1578.4276326497395\n",
      "Epoch 722/3000, Loss: 1461.699462890625\n",
      "Epoch 723/3000, Loss: 1383.0047607421875\n",
      "Epoch 724/3000, Loss: 1429.555887858073\n",
      "Epoch 725/3000, Loss: 1398.8517659505208\n",
      "Epoch 726/3000, Loss: 1406.4523518880208\n",
      "Epoch 727/3000, Loss: 1453.4199829101562\n",
      "Epoch 728/3000, Loss: 1404.726582845052\n",
      "Epoch 729/3000, Loss: 1467.2569783528645\n",
      "Epoch 730/3000, Loss: 1368.149149576823\n",
      "Epoch 731/3000, Loss: 1343.7275390625\n",
      "Epoch 732/3000, Loss: 1513.7908935546875\n",
      "Epoch 733/3000, Loss: 1432.6798095703125\n",
      "Epoch 734/3000, Loss: 1515.5550944010417\n",
      "Epoch 735/3000, Loss: 1520.5387776692708\n",
      "Epoch 736/3000, Loss: 1446.0059000651042\n",
      "Epoch 737/3000, Loss: 1583.223876953125\n",
      "Epoch 738/3000, Loss: 1439.6048380533855\n",
      "Epoch 739/3000, Loss: 1443.4440511067708\n",
      "Epoch 740/3000, Loss: 1532.7986857096355\n",
      "Epoch 741/3000, Loss: 1463.3256022135417\n",
      "Epoch 742/3000, Loss: 1518.9429728190105\n",
      "Epoch 743/3000, Loss: 1435.778055826823\n",
      "Epoch 744/3000, Loss: 1548.0273030598958\n",
      "Epoch 745/3000, Loss: 1435.1778971354167\n",
      "Epoch 746/3000, Loss: 1504.1823120117188\n",
      "Epoch 747/3000, Loss: 1463.9016927083333\n",
      "Epoch 748/3000, Loss: 1441.028096516927\n",
      "Epoch 749/3000, Loss: 1543.230448404948\n",
      "Epoch 750/3000, Loss: 1344.4200032552083\n",
      "Epoch 751/3000, Loss: 1438.9722696940105\n",
      "Epoch 752/3000, Loss: 1400.3891194661458\n",
      "Epoch 753/3000, Loss: 1461.3880818684895\n",
      "Epoch 754/3000, Loss: 1474.1172688802083\n",
      "Epoch 755/3000, Loss: 1430.8009236653645\n",
      "Epoch 756/3000, Loss: 1370.3012084960938\n",
      "Epoch 757/3000, Loss: 1415.8895467122395\n",
      "Epoch 758/3000, Loss: 1556.3680419921875\n",
      "Epoch 759/3000, Loss: 1501.053202311198\n",
      "Epoch 760/3000, Loss: 1497.4769083658855\n",
      "Epoch 761/3000, Loss: 1501.6613362630208\n",
      "Epoch 762/3000, Loss: 1405.1233317057292\n",
      "Epoch 763/3000, Loss: 1452.2738850911458\n",
      "Epoch 764/3000, Loss: 1468.2183227539062\n",
      "Epoch 765/3000, Loss: 1446.4270833333333\n",
      "Epoch 766/3000, Loss: 1421.5804646809895\n",
      "Epoch 767/3000, Loss: 1489.5706380208333\n",
      "Epoch 768/3000, Loss: 1410.9971720377605\n",
      "Epoch 769/3000, Loss: 1527.023905436198\n",
      "Epoch 770/3000, Loss: 1400.6521402994792\n",
      "Epoch 771/3000, Loss: 1490.270284016927\n",
      "Epoch 772/3000, Loss: 1425.4251505533855\n",
      "Epoch 773/3000, Loss: 1382.3402709960938\n",
      "Epoch 774/3000, Loss: 1401.888163248698\n",
      "Epoch 775/3000, Loss: 1422.8889567057292\n",
      "Epoch 776/3000, Loss: 1480.324198404948\n",
      "Epoch 777/3000, Loss: 1477.6444498697917\n",
      "Epoch 778/3000, Loss: 1496.9720052083333\n",
      "Epoch 779/3000, Loss: 1410.1546630859375\n",
      "Epoch 780/3000, Loss: 1494.3306884765625\n",
      "Epoch 781/3000, Loss: 1412.2603352864583\n",
      "Epoch 782/3000, Loss: 1428.8734537760417\n",
      "Epoch 783/3000, Loss: 1397.9578857421875\n",
      "Epoch 784/3000, Loss: 1457.9859619140625\n",
      "Epoch 785/3000, Loss: 1296.0582478841145\n",
      "Epoch 786/3000, Loss: 1405.7935384114583\n",
      "Epoch 787/3000, Loss: 1425.3678588867188\n",
      "Epoch 788/3000, Loss: 1451.9090576171875\n",
      "Epoch 789/3000, Loss: 1380.7832641601562\n",
      "Epoch 790/3000, Loss: 1448.6536254882812\n",
      "Epoch 791/3000, Loss: 1458.9940795898438\n",
      "Epoch 792/3000, Loss: 1393.0650634765625\n",
      "Epoch 793/3000, Loss: 1454.0758666992188\n",
      "Epoch 794/3000, Loss: 1414.66650390625\n",
      "Epoch 795/3000, Loss: 1444.2700602213542\n",
      "Epoch 796/3000, Loss: 1393.5164998372395\n",
      "Epoch 797/3000, Loss: 1556.4934895833333\n",
      "Epoch 798/3000, Loss: 1410.4066162109375\n",
      "Epoch 799/3000, Loss: 1453.7510172526042\n",
      "Epoch 800/3000, Loss: 1404.558349609375\n",
      "Epoch 801/3000, Loss: 1481.4298299153645\n",
      "Epoch 802/3000, Loss: 1310.4480590820312\n",
      "Epoch 803/3000, Loss: 1460.5127563476562\n",
      "Epoch 804/3000, Loss: 1456.8992106119792\n",
      "Epoch 805/3000, Loss: 1476.8641357421875\n",
      "Epoch 806/3000, Loss: 1490.0174153645833\n",
      "Epoch 807/3000, Loss: 1373.1178181966145\n",
      "Epoch 808/3000, Loss: 1365.374979654948\n",
      "Epoch 809/3000, Loss: 1389.8461303710938\n",
      "Epoch 810/3000, Loss: 1452.368916829427\n",
      "Epoch 811/3000, Loss: 1428.9969278971355\n",
      "Epoch 812/3000, Loss: 1427.5155843098958\n",
      "Epoch 813/3000, Loss: 1361.5819905598958\n",
      "Epoch 814/3000, Loss: 1356.7378743489583\n",
      "Epoch 815/3000, Loss: 1347.6357014973958\n",
      "Epoch 816/3000, Loss: 1387.29248046875\n",
      "Epoch 817/3000, Loss: 1415.2826131184895\n",
      "Epoch 818/3000, Loss: 1347.8911031087239\n",
      "Epoch 819/3000, Loss: 1444.5233357747395\n",
      "Epoch 820/3000, Loss: 1406.102294921875\n",
      "Epoch 821/3000, Loss: 1349.8789876302083\n",
      "Epoch 822/3000, Loss: 1380.022928873698\n",
      "Epoch 823/3000, Loss: 1395.7910563151042\n",
      "Epoch 824/3000, Loss: 1315.9212036132812\n",
      "Epoch 825/3000, Loss: 1330.986328125\n",
      "Epoch 826/3000, Loss: 1321.818623860677\n",
      "Epoch 827/3000, Loss: 1414.3274332682292\n",
      "Epoch 828/3000, Loss: 1367.033203125\n",
      "Epoch 829/3000, Loss: 1427.2665201822917\n",
      "Epoch 830/3000, Loss: 1389.193623860677\n",
      "Epoch 831/3000, Loss: 1462.183369954427\n",
      "Epoch 832/3000, Loss: 1357.9523111979167\n",
      "Epoch 833/3000, Loss: 1395.3454182942708\n",
      "Epoch 834/3000, Loss: 1376.6629638671875\n",
      "Epoch 835/3000, Loss: 1454.0753987630208\n",
      "Epoch 836/3000, Loss: 1340.4664306640625\n",
      "Epoch 837/3000, Loss: 1447.8845825195312\n",
      "Epoch 838/3000, Loss: 1370.8046875\n",
      "Epoch 839/3000, Loss: 1468.2838541666667\n",
      "Epoch 840/3000, Loss: 1395.075927734375\n",
      "Epoch 841/3000, Loss: 1414.0866088867188\n",
      "Epoch 842/3000, Loss: 1502.2759399414062\n",
      "Epoch 843/3000, Loss: 1422.9770914713542\n",
      "Epoch 844/3000, Loss: 1465.5562337239583\n",
      "Epoch 845/3000, Loss: 1400.8644612630208\n",
      "Epoch 846/3000, Loss: 1396.0233968098958\n",
      "Epoch 847/3000, Loss: 1364.5315958658855\n",
      "Epoch 848/3000, Loss: 1300.557840983073\n",
      "Epoch 849/3000, Loss: 1547.40380859375\n",
      "Epoch 850/3000, Loss: 1460.5293579101562\n",
      "Epoch 851/3000, Loss: 1387.624247233073\n",
      "Epoch 852/3000, Loss: 1438.155782063802\n",
      "Epoch 853/3000, Loss: 1395.6154378255208\n",
      "Epoch 854/3000, Loss: 1397.5708414713542\n",
      "Epoch 855/3000, Loss: 1425.5401204427083\n",
      "Epoch 856/3000, Loss: 1373.9676920572917\n",
      "Epoch 857/3000, Loss: 1401.1651000976562\n",
      "Epoch 858/3000, Loss: 1557.4335327148438\n",
      "Epoch 859/3000, Loss: 1358.4343668619792\n",
      "Epoch 860/3000, Loss: 1465.6573893229167\n",
      "Epoch 861/3000, Loss: 1373.5710042317708\n",
      "Epoch 862/3000, Loss: 1249.1399841308594\n",
      "Epoch 863/3000, Loss: 1513.4822387695312\n",
      "Epoch 864/3000, Loss: 1397.1988932291667\n",
      "Epoch 865/3000, Loss: 1369.7769368489583\n",
      "Epoch 866/3000, Loss: 1363.5136108398438\n",
      "Epoch 867/3000, Loss: 1362.9608357747395\n",
      "Epoch 868/3000, Loss: 1465.448506673177\n",
      "Epoch 869/3000, Loss: 1355.1859944661458\n",
      "Epoch 870/3000, Loss: 1391.422139485677\n",
      "Epoch 871/3000, Loss: 1388.9264729817708\n",
      "Epoch 872/3000, Loss: 1264.2122192382812\n",
      "Epoch 873/3000, Loss: 1329.7832845052083\n",
      "Epoch 874/3000, Loss: 1393.389668782552\n",
      "Epoch 875/3000, Loss: 1286.4989217122395\n",
      "Epoch 876/3000, Loss: 1403.7559000651042\n",
      "Epoch 877/3000, Loss: 1443.7999877929688\n",
      "Epoch 878/3000, Loss: 1447.8531494140625\n",
      "Epoch 879/3000, Loss: 1453.3146158854167\n",
      "Epoch 880/3000, Loss: 1414.494364420573\n",
      "Epoch 881/3000, Loss: 1387.523701985677\n",
      "Epoch 882/3000, Loss: 1314.8175862630208\n",
      "Epoch 883/3000, Loss: 1384.2072143554688\n",
      "Epoch 884/3000, Loss: 1390.8451131184895\n",
      "Epoch 885/3000, Loss: 1439.4590250651042\n",
      "Epoch 886/3000, Loss: 1457.3731892903645\n",
      "Epoch 887/3000, Loss: 1377.6023559570312\n",
      "Epoch 888/3000, Loss: 1403.720723470052\n",
      "Epoch 889/3000, Loss: 1437.8983357747395\n",
      "Epoch 890/3000, Loss: 1454.8187052408855\n",
      "Epoch 891/3000, Loss: 1316.218994140625\n",
      "Epoch 892/3000, Loss: 1290.1699829101562\n",
      "Epoch 893/3000, Loss: 1440.5036010742188\n",
      "Epoch 894/3000, Loss: 1433.825174967448\n",
      "Epoch 895/3000, Loss: 1361.9808146158855\n",
      "Epoch 896/3000, Loss: 1420.3859252929688\n",
      "Epoch 897/3000, Loss: 1349.3784586588542\n",
      "Epoch 898/3000, Loss: 1358.2320353190105\n",
      "Epoch 899/3000, Loss: 1480.1509806315105\n",
      "Epoch 900/3000, Loss: 1382.1678466796875\n",
      "Epoch 901/3000, Loss: 1441.3807576497395\n",
      "Epoch 902/3000, Loss: 1426.548604329427\n",
      "Epoch 903/3000, Loss: 1343.9607747395833\n",
      "Epoch 904/3000, Loss: 1429.0482381184895\n",
      "Epoch 905/3000, Loss: 1447.6367594401042\n",
      "Epoch 906/3000, Loss: 1378.5333658854167\n",
      "Epoch 907/3000, Loss: 1462.2542317708333\n",
      "Epoch 908/3000, Loss: 1408.878641764323\n",
      "Epoch 909/3000, Loss: 1318.1640218098958\n",
      "Epoch 910/3000, Loss: 1377.9841715494792\n",
      "Epoch 911/3000, Loss: 1357.557149251302\n",
      "Epoch 912/3000, Loss: 1347.95068359375\n",
      "Epoch 913/3000, Loss: 1401.8407796223958\n",
      "Epoch 914/3000, Loss: 1459.2548217773438\n",
      "Epoch 915/3000, Loss: 1428.9590250651042\n",
      "Epoch 916/3000, Loss: 1366.1142985026042\n",
      "Epoch 917/3000, Loss: 1334.4634806315105\n",
      "Epoch 918/3000, Loss: 1447.875\n",
      "Epoch 919/3000, Loss: 1324.7547200520833\n",
      "Epoch 920/3000, Loss: 1453.4347330729167\n",
      "Epoch 921/3000, Loss: 1403.6269124348958\n",
      "Epoch 922/3000, Loss: 1501.446268717448\n",
      "Epoch 923/3000, Loss: 1396.67724609375\n",
      "Epoch 924/3000, Loss: 1359.651631673177\n",
      "Epoch 925/3000, Loss: 1372.7009887695312\n",
      "Epoch 926/3000, Loss: 1446.0350748697917\n",
      "Epoch 927/3000, Loss: 1377.2320963541667\n",
      "Epoch 928/3000, Loss: 1410.2870483398438\n",
      "Epoch 929/3000, Loss: 1312.9209594726562\n",
      "Epoch 930/3000, Loss: 1411.8402506510417\n",
      "Epoch 931/3000, Loss: 1321.6925862630208\n",
      "Epoch 932/3000, Loss: 1384.3619588216145\n",
      "Epoch 933/3000, Loss: 1385.2166137695312\n",
      "Epoch 934/3000, Loss: 1236.1784973144531\n",
      "Epoch 935/3000, Loss: 1335.9626871744792\n",
      "Epoch 936/3000, Loss: 1378.9140014648438\n",
      "Epoch 937/3000, Loss: 1346.2576497395833\n",
      "Epoch 938/3000, Loss: 1402.036112467448\n",
      "Epoch 939/3000, Loss: 1370.3548380533855\n",
      "Epoch 940/3000, Loss: 1400.2814331054688\n",
      "Epoch 941/3000, Loss: 1332.7952473958333\n",
      "Epoch 942/3000, Loss: 1489.055643717448\n",
      "Epoch 943/3000, Loss: 1321.5123087565105\n",
      "Epoch 944/3000, Loss: 1405.232421875\n",
      "Epoch 945/3000, Loss: 1404.7301839192708\n",
      "Epoch 946/3000, Loss: 1360.363749186198\n",
      "Epoch 947/3000, Loss: 1448.2400309244792\n",
      "Epoch 948/3000, Loss: 1326.0093587239583\n",
      "Epoch 949/3000, Loss: 1403.968994140625\n",
      "Epoch 950/3000, Loss: 1423.558614095052\n",
      "Epoch 951/3000, Loss: 1311.757344563802\n",
      "Epoch 952/3000, Loss: 1390.9403483072917\n",
      "Epoch 953/3000, Loss: 1442.639139811198\n",
      "Epoch 954/3000, Loss: 1443.4732259114583\n",
      "Epoch 955/3000, Loss: 1452.6631469726562\n",
      "Epoch 956/3000, Loss: 1394.3500569661458\n",
      "Epoch 957/3000, Loss: 1348.7501424153645\n",
      "Epoch 958/3000, Loss: 1358.7093505859375\n",
      "Epoch 959/3000, Loss: 1377.8905639648438\n",
      "Epoch 960/3000, Loss: 1311.4645589192708\n",
      "Epoch 961/3000, Loss: 1373.783182779948\n",
      "Epoch 962/3000, Loss: 1268.5946044921875\n",
      "Epoch 963/3000, Loss: 1303.4738159179688\n",
      "Epoch 964/3000, Loss: 1319.2592163085938\n",
      "Epoch 965/3000, Loss: 1373.5764973958333\n",
      "Epoch 966/3000, Loss: 1241.6349487304688\n",
      "Epoch 967/3000, Loss: 1459.261454264323\n",
      "Epoch 968/3000, Loss: 1400.7539876302083\n",
      "Epoch 969/3000, Loss: 1329.408467610677\n",
      "Epoch 970/3000, Loss: 1289.5528971354167\n",
      "Epoch 971/3000, Loss: 1421.1271158854167\n",
      "Epoch 972/3000, Loss: 1275.627217610677\n",
      "Epoch 973/3000, Loss: 1356.9942626953125\n",
      "Epoch 974/3000, Loss: 1310.1678059895833\n",
      "Epoch 975/3000, Loss: 1390.8822631835938\n",
      "Epoch 976/3000, Loss: 1392.970194498698\n",
      "Epoch 977/3000, Loss: 1323.2120157877605\n",
      "Epoch 978/3000, Loss: 1399.7283121744792\n",
      "Epoch 979/3000, Loss: 1355.3922932942708\n",
      "Epoch 980/3000, Loss: 1372.3126017252605\n",
      "Epoch 981/3000, Loss: 1372.8153686523438\n",
      "Epoch 982/3000, Loss: 1405.2992146809895\n",
      "Epoch 983/3000, Loss: 1306.5590006510417\n",
      "Epoch 984/3000, Loss: 1473.5343627929688\n",
      "Epoch 985/3000, Loss: 1310.7355143229167\n",
      "Epoch 986/3000, Loss: 1370.349385579427\n",
      "Epoch 987/3000, Loss: 1426.7424926757812\n",
      "Epoch 988/3000, Loss: 1414.2254435221355\n",
      "Epoch 989/3000, Loss: 1427.4610188802083\n",
      "Epoch 990/3000, Loss: 1402.5059407552083\n",
      "Epoch 991/3000, Loss: 1306.55810546875\n",
      "Epoch 992/3000, Loss: 1387.1734415690105\n",
      "Epoch 993/3000, Loss: 1411.2963053385417\n",
      "Epoch 994/3000, Loss: 1390.5792846679688\n",
      "Epoch 995/3000, Loss: 1335.2860717773438\n",
      "Epoch 996/3000, Loss: 1344.3677978515625\n",
      "Epoch 997/3000, Loss: 1329.8963623046875\n",
      "Epoch 998/3000, Loss: 1376.583516438802\n",
      "Epoch 999/3000, Loss: 1340.5337931315105\n",
      "Epoch 1000/3000, Loss: 1309.3653361002605\n",
      "Epoch 1001/3000, Loss: 1433.2638753255208\n",
      "Epoch 1002/3000, Loss: 1323.1177164713542\n",
      "Epoch 1003/3000, Loss: 1336.941426595052\n",
      "Epoch 1004/3000, Loss: 1406.7206624348958\n",
      "Epoch 1005/3000, Loss: 1319.869120279948\n",
      "Epoch 1006/3000, Loss: 1278.9483337402344\n",
      "Epoch 1007/3000, Loss: 1368.9014892578125\n",
      "Epoch 1008/3000, Loss: 1240.6681823730469\n",
      "Epoch 1009/3000, Loss: 1242.8035278320312\n",
      "Epoch 1010/3000, Loss: 1434.2535603841145\n",
      "Epoch 1011/3000, Loss: 1356.545918782552\n",
      "Epoch 1012/3000, Loss: 1368.429443359375\n",
      "Epoch 1013/3000, Loss: 1349.2003580729167\n",
      "Epoch 1014/3000, Loss: 1322.080057779948\n",
      "Epoch 1015/3000, Loss: 1273.1677347819011\n",
      "Epoch 1016/3000, Loss: 1365.486348470052\n",
      "Epoch 1017/3000, Loss: 1354.940165201823\n",
      "Epoch 1018/3000, Loss: 1415.2245279947917\n",
      "Epoch 1019/3000, Loss: 1397.1540934244792\n",
      "Epoch 1020/3000, Loss: 1395.859375\n",
      "Epoch 1021/3000, Loss: 1344.607930501302\n",
      "Epoch 1022/3000, Loss: 1337.2379557291667\n",
      "Epoch 1023/3000, Loss: 1238.0983072916667\n",
      "Epoch 1024/3000, Loss: 1277.9551188151042\n",
      "Epoch 1025/3000, Loss: 1381.6427205403645\n",
      "Epoch 1026/3000, Loss: 1523.1182047526042\n",
      "Epoch 1027/3000, Loss: 1385.4046630859375\n",
      "Epoch 1028/3000, Loss: 1320.0182698567708\n",
      "Epoch 1029/3000, Loss: 1376.3597208658855\n",
      "Epoch 1030/3000, Loss: 1301.2878011067708\n",
      "Epoch 1031/3000, Loss: 1338.2890014648438\n",
      "Epoch 1032/3000, Loss: 1253.6013997395833\n",
      "Epoch 1033/3000, Loss: 1434.0634358723958\n",
      "Epoch 1034/3000, Loss: 1333.766092936198\n",
      "Epoch 1035/3000, Loss: 1371.800069173177\n",
      "Epoch 1036/3000, Loss: 1415.0838826497395\n",
      "Epoch 1037/3000, Loss: 1213.4414164225261\n",
      "Epoch 1038/3000, Loss: 1348.3907470703125\n",
      "Epoch 1039/3000, Loss: 1396.0476481119792\n",
      "Epoch 1040/3000, Loss: 1318.1600341796875\n",
      "Epoch 1041/3000, Loss: 1317.0391438802083\n",
      "Epoch 1042/3000, Loss: 1306.5467732747395\n",
      "Epoch 1043/3000, Loss: 1355.6471354166667\n",
      "Epoch 1044/3000, Loss: 1329.9565022786458\n",
      "Epoch 1045/3000, Loss: 1382.3521931966145\n",
      "Epoch 1046/3000, Loss: 1418.6682942708333\n",
      "Epoch 1047/3000, Loss: 1341.7572631835938\n",
      "Epoch 1048/3000, Loss: 1374.5199788411458\n",
      "Epoch 1049/3000, Loss: 1279.9722900390625\n",
      "Epoch 1050/3000, Loss: 1378.0982666015625\n",
      "Epoch 1051/3000, Loss: 1350.1036987304688\n",
      "Epoch 1052/3000, Loss: 1323.5020548502605\n",
      "Epoch 1053/3000, Loss: 1328.994649251302\n",
      "Epoch 1054/3000, Loss: 1417.7491455078125\n",
      "Epoch 1055/3000, Loss: 1402.510986328125\n",
      "Epoch 1056/3000, Loss: 1283.1210530598958\n",
      "Epoch 1057/3000, Loss: 1314.1283365885417\n",
      "Epoch 1058/3000, Loss: 1401.8567911783855\n",
      "Epoch 1059/3000, Loss: 1357.612284342448\n",
      "Epoch 1060/3000, Loss: 1366.8811645507812\n",
      "Epoch 1061/3000, Loss: 1301.6001993815105\n",
      "Epoch 1062/3000, Loss: 1334.601094563802\n",
      "Epoch 1063/3000, Loss: 1305.2000122070312\n",
      "Epoch 1064/3000, Loss: 1295.554219563802\n",
      "Epoch 1065/3000, Loss: 1399.5088500976562\n",
      "Epoch 1066/3000, Loss: 1314.9158935546875\n",
      "Epoch 1067/3000, Loss: 1448.0391642252605\n",
      "Epoch 1068/3000, Loss: 1368.902364095052\n",
      "Epoch 1069/3000, Loss: 1348.5364990234375\n",
      "Epoch 1070/3000, Loss: 1428.8939412434895\n",
      "Epoch 1071/3000, Loss: 1315.4043782552083\n",
      "Epoch 1072/3000, Loss: 1383.006815592448\n",
      "Epoch 1073/3000, Loss: 1311.6212972005208\n",
      "Epoch 1074/3000, Loss: 1326.8411458333333\n",
      "Epoch 1075/3000, Loss: 1307.487060546875\n",
      "Epoch 1076/3000, Loss: 1317.95849609375\n",
      "Epoch 1077/3000, Loss: 1348.5458577473958\n",
      "Epoch 1078/3000, Loss: 1370.5010579427083\n",
      "Epoch 1079/3000, Loss: 1359.5313110351562\n",
      "Epoch 1080/3000, Loss: 1319.2600504557292\n",
      "Epoch 1081/3000, Loss: 1237.5061848958333\n",
      "Epoch 1082/3000, Loss: 1289.5803833007812\n",
      "Epoch 1083/3000, Loss: 1287.3880411783855\n",
      "Epoch 1084/3000, Loss: 1328.010009765625\n",
      "Epoch 1085/3000, Loss: 1332.626688639323\n",
      "Epoch 1086/3000, Loss: 1286.8173319498699\n",
      "Epoch 1087/3000, Loss: 1348.5685424804688\n",
      "Epoch 1088/3000, Loss: 1350.108662923177\n",
      "Epoch 1089/3000, Loss: 1367.8919881184895\n",
      "Epoch 1090/3000, Loss: 1384.8074747721355\n",
      "Epoch 1091/3000, Loss: 1462.825419108073\n",
      "Epoch 1092/3000, Loss: 1340.204366048177\n",
      "Epoch 1093/3000, Loss: 1372.2233072916667\n",
      "Epoch 1094/3000, Loss: 1374.1175537109375\n",
      "Epoch 1095/3000, Loss: 1280.5407307942708\n",
      "Epoch 1096/3000, Loss: 1359.7052815755208\n",
      "Epoch 1097/3000, Loss: 1293.3248087565105\n",
      "Epoch 1098/3000, Loss: 1280.1680704752605\n",
      "Epoch 1099/3000, Loss: 1319.975606282552\n",
      "Epoch 1100/3000, Loss: 1450.6826578776042\n",
      "Epoch 1101/3000, Loss: 1297.8471272786458\n",
      "Epoch 1102/3000, Loss: 1267.3121744791667\n",
      "Epoch 1103/3000, Loss: 1343.162618001302\n",
      "Epoch 1104/3000, Loss: 1453.573465983073\n",
      "Epoch 1105/3000, Loss: 1257.0872599283855\n",
      "Epoch 1106/3000, Loss: 1315.9289143880208\n",
      "Epoch 1107/3000, Loss: 1297.5331624348958\n",
      "Epoch 1108/3000, Loss: 1285.6717732747395\n",
      "Epoch 1109/3000, Loss: 1389.1743774414062\n",
      "Epoch 1110/3000, Loss: 1273.4607747395833\n",
      "Epoch 1111/3000, Loss: 1311.727274576823\n",
      "Epoch 1112/3000, Loss: 1329.0079549153645\n",
      "Epoch 1113/3000, Loss: 1385.328592936198\n",
      "Epoch 1114/3000, Loss: 1288.809061686198\n",
      "Epoch 1115/3000, Loss: 1330.4546915690105\n",
      "Epoch 1116/3000, Loss: 1372.3638916015625\n",
      "Epoch 1117/3000, Loss: 1343.9604899088542\n",
      "Epoch 1118/3000, Loss: 1389.7967732747395\n",
      "Epoch 1119/3000, Loss: 1425.7725830078125\n",
      "Epoch 1120/3000, Loss: 1353.1365356445312\n",
      "Epoch 1121/3000, Loss: 1315.8853556315105\n",
      "Epoch 1122/3000, Loss: 1409.901123046875\n",
      "Epoch 1123/3000, Loss: 1333.1746622721355\n",
      "Epoch 1124/3000, Loss: 1301.334493001302\n",
      "Epoch 1125/3000, Loss: 1416.2717692057292\n",
      "Epoch 1126/3000, Loss: 1354.1193440755208\n",
      "Epoch 1127/3000, Loss: 1305.942891438802\n",
      "Epoch 1128/3000, Loss: 1292.0120035807292\n",
      "Epoch 1129/3000, Loss: 1344.4160563151042\n",
      "Epoch 1130/3000, Loss: 1261.5987345377605\n",
      "Epoch 1131/3000, Loss: 1316.4624837239583\n",
      "Epoch 1132/3000, Loss: 1348.9962361653645\n",
      "Epoch 1133/3000, Loss: 1433.6881306966145\n",
      "Epoch 1134/3000, Loss: 1424.5472208658855\n",
      "Epoch 1135/3000, Loss: 1368.3893432617188\n",
      "Epoch 1136/3000, Loss: 1351.7981567382812\n",
      "Epoch 1137/3000, Loss: 1239.1341349283855\n",
      "Epoch 1138/3000, Loss: 1362.0223592122395\n",
      "Epoch 1139/3000, Loss: 1377.765401204427\n",
      "Epoch 1140/3000, Loss: 1382.1435139973958\n",
      "Epoch 1141/3000, Loss: 1351.1714274088542\n",
      "Epoch 1142/3000, Loss: 1363.1453043619792\n",
      "Epoch 1143/3000, Loss: 1353.713602701823\n",
      "Epoch 1144/3000, Loss: 1316.5410868326824\n",
      "Epoch 1145/3000, Loss: 1291.697001139323\n",
      "Epoch 1146/3000, Loss: 1333.0050048828125\n",
      "Epoch 1147/3000, Loss: 1307.3185017903645\n",
      "Epoch 1148/3000, Loss: 1302.5441080729167\n",
      "Epoch 1149/3000, Loss: 1373.4823201497395\n",
      "Epoch 1150/3000, Loss: 1408.5867106119792\n",
      "Epoch 1151/3000, Loss: 1270.080790201823\n",
      "Epoch 1152/3000, Loss: 1306.764424641927\n",
      "Epoch 1153/3000, Loss: 1373.6456298828125\n",
      "Epoch 1154/3000, Loss: 1325.1307779947917\n",
      "Epoch 1155/3000, Loss: 1325.0279337565105\n",
      "Epoch 1156/3000, Loss: 1271.2893778483074\n",
      "Epoch 1157/3000, Loss: 1339.1898803710938\n",
      "Epoch 1158/3000, Loss: 1298.1918538411458\n",
      "Epoch 1159/3000, Loss: 1405.58056640625\n",
      "Epoch 1160/3000, Loss: 1322.2981770833333\n",
      "Epoch 1161/3000, Loss: 1330.5644124348958\n",
      "Epoch 1162/3000, Loss: 1345.8942260742188\n",
      "Epoch 1163/3000, Loss: 1342.8729654947917\n",
      "Epoch 1164/3000, Loss: 1304.7598876953125\n",
      "Epoch 1165/3000, Loss: 1280.6792399088542\n",
      "Epoch 1166/3000, Loss: 1310.5625813802083\n",
      "Epoch 1167/3000, Loss: 1320.5716552734375\n",
      "Epoch 1168/3000, Loss: 1305.1170043945312\n",
      "Epoch 1169/3000, Loss: 1329.2732340494792\n",
      "Epoch 1170/3000, Loss: 1390.3361206054688\n",
      "Epoch 1171/3000, Loss: 1427.8548380533855\n",
      "Epoch 1172/3000, Loss: 1357.7289021809895\n",
      "Epoch 1173/3000, Loss: 1348.8543701171875\n",
      "Epoch 1174/3000, Loss: 1462.2393391927083\n",
      "Epoch 1175/3000, Loss: 1288.1980590820312\n",
      "Epoch 1176/3000, Loss: 1339.5256958007812\n",
      "Epoch 1177/3000, Loss: 1325.0178833007812\n",
      "Epoch 1178/3000, Loss: 1322.5216267903645\n",
      "Epoch 1179/3000, Loss: 1316.112772623698\n",
      "Epoch 1180/3000, Loss: 1295.317118326823\n",
      "Epoch 1181/3000, Loss: 1318.7848103841145\n",
      "Epoch 1182/3000, Loss: 1371.214579264323\n",
      "Epoch 1183/3000, Loss: 1311.4100952148438\n",
      "Epoch 1184/3000, Loss: 1304.1282958984375\n",
      "Epoch 1185/3000, Loss: 1236.3999735514324\n",
      "Epoch 1186/3000, Loss: 1330.7008666992188\n",
      "Epoch 1187/3000, Loss: 1321.7009887695312\n",
      "Epoch 1188/3000, Loss: 1347.180399576823\n",
      "Epoch 1189/3000, Loss: 1251.8995768229167\n",
      "Epoch 1190/3000, Loss: 1375.0787150065105\n",
      "Epoch 1191/3000, Loss: 1342.9761149088542\n",
      "Epoch 1192/3000, Loss: 1339.3761189778645\n",
      "Epoch 1193/3000, Loss: 1332.7549641927083\n",
      "Epoch 1194/3000, Loss: 1417.060282389323\n",
      "Epoch 1195/3000, Loss: 1293.9000040690105\n",
      "Epoch 1196/3000, Loss: 1445.7265218098958\n",
      "Epoch 1197/3000, Loss: 1363.341796875\n",
      "Epoch 1198/3000, Loss: 1398.4679158528645\n",
      "Epoch 1199/3000, Loss: 1342.3338826497395\n",
      "Epoch 1200/3000, Loss: 1354.8312581380208\n",
      "Epoch 1201/3000, Loss: 1341.5452067057292\n",
      "Epoch 1202/3000, Loss: 1329.084737141927\n",
      "Epoch 1203/3000, Loss: 1332.155741373698\n",
      "Epoch 1204/3000, Loss: 1468.2832438151042\n",
      "Epoch 1205/3000, Loss: 1342.419942220052\n",
      "Epoch 1206/3000, Loss: 1352.8643188476562\n",
      "Epoch 1207/3000, Loss: 1337.4838460286458\n",
      "Epoch 1208/3000, Loss: 1331.781514485677\n",
      "Epoch 1209/3000, Loss: 1357.4118245442708\n",
      "Epoch 1210/3000, Loss: 1319.1202596028645\n",
      "Epoch 1211/3000, Loss: 1324.2831217447917\n",
      "Epoch 1212/3000, Loss: 1298.7702026367188\n",
      "Epoch 1213/3000, Loss: 1269.1508687337239\n",
      "Epoch 1214/3000, Loss: 1374.7389729817708\n",
      "Epoch 1215/3000, Loss: 1302.1190795898438\n",
      "Epoch 1216/3000, Loss: 1344.341776529948\n",
      "Epoch 1217/3000, Loss: 1333.3128051757812\n",
      "Epoch 1218/3000, Loss: 1303.3974609375\n",
      "Epoch 1219/3000, Loss: 1362.892333984375\n",
      "Epoch 1220/3000, Loss: 1295.2951253255208\n",
      "Epoch 1221/3000, Loss: 1314.0226033528645\n",
      "Epoch 1222/3000, Loss: 1358.3845621744792\n",
      "Epoch 1223/3000, Loss: 1344.3535970052083\n",
      "Epoch 1224/3000, Loss: 1299.077616373698\n",
      "Epoch 1225/3000, Loss: 1291.7516479492188\n",
      "Epoch 1226/3000, Loss: 1220.2197672526042\n",
      "Epoch 1227/3000, Loss: 1254.1051635742188\n",
      "Epoch 1228/3000, Loss: 1329.5492146809895\n",
      "Epoch 1229/3000, Loss: 1247.1106872558594\n",
      "Epoch 1230/3000, Loss: 1277.42724609375\n",
      "Epoch 1231/3000, Loss: 1277.3844197591145\n",
      "Epoch 1232/3000, Loss: 1314.5838419596355\n",
      "Epoch 1233/3000, Loss: 1327.3302001953125\n",
      "Epoch 1234/3000, Loss: 1492.4703369140625\n",
      "Epoch 1235/3000, Loss: 1329.5394083658855\n",
      "Epoch 1236/3000, Loss: 1347.4418538411458\n",
      "Epoch 1237/3000, Loss: 1316.8296915690105\n",
      "Epoch 1238/3000, Loss: 1300.2150065104167\n",
      "Epoch 1239/3000, Loss: 1257.96533203125\n",
      "Epoch 1240/3000, Loss: 1358.1304524739583\n",
      "Epoch 1241/3000, Loss: 1429.8912556966145\n",
      "Epoch 1242/3000, Loss: 1279.4265543619792\n",
      "Epoch 1243/3000, Loss: 1475.9422200520833\n",
      "Epoch 1244/3000, Loss: 1338.4622599283855\n",
      "Epoch 1245/3000, Loss: 1299.8412984212239\n",
      "Epoch 1246/3000, Loss: 1257.1996053059895\n",
      "Epoch 1247/3000, Loss: 1308.6686197916667\n",
      "Epoch 1248/3000, Loss: 1300.2070821126301\n",
      "Epoch 1249/3000, Loss: 1214.8213500976562\n",
      "Epoch 1250/3000, Loss: 1312.6326293945312\n",
      "Epoch 1251/3000, Loss: 1334.6543986002605\n",
      "Epoch 1252/3000, Loss: 1419.7601521809895\n",
      "Epoch 1253/3000, Loss: 1312.785400390625\n",
      "Epoch 1254/3000, Loss: 1425.780293782552\n",
      "Epoch 1255/3000, Loss: 1292.601298014323\n",
      "Epoch 1256/3000, Loss: 1301.7315877278645\n",
      "Epoch 1257/3000, Loss: 1201.79833984375\n",
      "Epoch 1258/3000, Loss: 1256.2337849934895\n",
      "Epoch 1259/3000, Loss: 1279.783203125\n",
      "Epoch 1260/3000, Loss: 1342.197489420573\n",
      "Epoch 1261/3000, Loss: 1238.3424682617188\n",
      "Epoch 1262/3000, Loss: 1259.4645182291667\n",
      "Epoch 1263/3000, Loss: 1319.5567626953125\n",
      "Epoch 1264/3000, Loss: 1253.747090657552\n",
      "Epoch 1265/3000, Loss: 1323.9419352213542\n",
      "Epoch 1266/3000, Loss: 1328.939188639323\n",
      "Epoch 1267/3000, Loss: 1246.448465983073\n",
      "Epoch 1268/3000, Loss: 1384.814432779948\n",
      "Epoch 1269/3000, Loss: 1280.3465169270833\n",
      "Epoch 1270/3000, Loss: 1269.796875\n",
      "Epoch 1271/3000, Loss: 1319.5744018554688\n",
      "Epoch 1272/3000, Loss: 1276.2011108398438\n",
      "Epoch 1273/3000, Loss: 1344.8341674804688\n",
      "Epoch 1274/3000, Loss: 1238.1839904785156\n",
      "Epoch 1275/3000, Loss: 1269.3522338867188\n",
      "Epoch 1276/3000, Loss: 1275.107686360677\n",
      "Epoch 1277/3000, Loss: 1320.3276774088542\n",
      "Epoch 1278/3000, Loss: 1277.973388671875\n",
      "Epoch 1279/3000, Loss: 1300.6817016601562\n",
      "Epoch 1280/3000, Loss: 1210.6284790039062\n",
      "Epoch 1281/3000, Loss: 1295.1518147786458\n",
      "Epoch 1282/3000, Loss: 1302.0738932291667\n",
      "Epoch 1283/3000, Loss: 1294.9777018229167\n",
      "Epoch 1284/3000, Loss: 1280.3673502604167\n",
      "Epoch 1285/3000, Loss: 1280.1113688151042\n",
      "Epoch 1286/3000, Loss: 1252.6807149251301\n",
      "Epoch 1287/3000, Loss: 1242.3613688151042\n",
      "Epoch 1288/3000, Loss: 1197.1080729166667\n",
      "Epoch 1289/3000, Loss: 1305.4965209960938\n",
      "Epoch 1290/3000, Loss: 1346.4931233723958\n",
      "Epoch 1291/3000, Loss: 1217.5871175130208\n",
      "Epoch 1292/3000, Loss: 1289.4455668131511\n",
      "Epoch 1293/3000, Loss: 1306.3094278971355\n",
      "Epoch 1294/3000, Loss: 1318.103780110677\n",
      "Epoch 1295/3000, Loss: 1255.5328776041667\n",
      "Epoch 1296/3000, Loss: 1345.379374186198\n",
      "Epoch 1297/3000, Loss: 1293.5713500976562\n",
      "Epoch 1298/3000, Loss: 1241.9466349283855\n",
      "Epoch 1299/3000, Loss: 1332.9429219563801\n",
      "Epoch 1300/3000, Loss: 1289.1420084635417\n",
      "Epoch 1301/3000, Loss: 1262.1942545572917\n",
      "Epoch 1302/3000, Loss: 1234.938944498698\n",
      "Epoch 1303/3000, Loss: 1349.491455078125\n",
      "Epoch 1304/3000, Loss: 1287.8031209309895\n",
      "Epoch 1305/3000, Loss: 1391.5813395182292\n",
      "Epoch 1306/3000, Loss: 1280.6319783528645\n",
      "Epoch 1307/3000, Loss: 1296.52685546875\n",
      "Epoch 1308/3000, Loss: 1248.2724609375\n",
      "Epoch 1309/3000, Loss: 1264.2660522460938\n",
      "Epoch 1310/3000, Loss: 1284.3455200195312\n",
      "Epoch 1311/3000, Loss: 1368.5465698242188\n",
      "Epoch 1312/3000, Loss: 1351.932352701823\n",
      "Epoch 1313/3000, Loss: 1275.8018798828125\n",
      "Epoch 1314/3000, Loss: 1369.8212280273438\n",
      "Epoch 1315/3000, Loss: 1328.2180989583333\n",
      "Epoch 1316/3000, Loss: 1216.6057942708333\n",
      "Epoch 1317/3000, Loss: 1225.0698852539062\n",
      "Epoch 1318/3000, Loss: 1205.9239095052083\n",
      "Epoch 1319/3000, Loss: 1235.32177734375\n",
      "Epoch 1320/3000, Loss: 1347.9862874348958\n",
      "Epoch 1321/3000, Loss: 1245.432840983073\n",
      "Epoch 1322/3000, Loss: 1359.5908813476562\n",
      "Epoch 1323/3000, Loss: 1279.5372111002605\n",
      "Epoch 1324/3000, Loss: 1263.0997314453125\n",
      "Epoch 1325/3000, Loss: 1324.845194498698\n",
      "Epoch 1326/3000, Loss: 1269.5415445963542\n",
      "Epoch 1327/3000, Loss: 1346.2904459635417\n",
      "Epoch 1328/3000, Loss: 1274.3213704427083\n",
      "Epoch 1329/3000, Loss: 1216.8873087565105\n",
      "Epoch 1330/3000, Loss: 1278.2162068684895\n",
      "Epoch 1331/3000, Loss: 1296.8288981119792\n",
      "Epoch 1332/3000, Loss: 1276.8775634765625\n",
      "Epoch 1333/3000, Loss: 1203.7526448567708\n",
      "Epoch 1334/3000, Loss: 1294.2632649739583\n",
      "Epoch 1335/3000, Loss: 1220.182373046875\n",
      "Epoch 1336/3000, Loss: 1193.5101725260417\n",
      "Epoch 1337/3000, Loss: 1363.6575927734375\n",
      "Epoch 1338/3000, Loss: 1367.0037231445312\n",
      "Epoch 1339/3000, Loss: 1370.8290405273438\n",
      "Epoch 1340/3000, Loss: 1308.9562174479167\n",
      "Epoch 1341/3000, Loss: 1325.1163126627605\n",
      "Epoch 1342/3000, Loss: 1219.915303548177\n",
      "Epoch 1343/3000, Loss: 1253.6978556315105\n",
      "Epoch 1344/3000, Loss: 1313.776346842448\n",
      "Epoch 1345/3000, Loss: 1284.830301920573\n",
      "Epoch 1346/3000, Loss: 1272.7374877929688\n",
      "Epoch 1347/3000, Loss: 1266.5139770507812\n",
      "Epoch 1348/3000, Loss: 1285.5407307942708\n",
      "Epoch 1349/3000, Loss: 1219.4036661783855\n",
      "Epoch 1350/3000, Loss: 1275.4496459960938\n",
      "Epoch 1351/3000, Loss: 1261.106424967448\n",
      "Epoch 1352/3000, Loss: 1326.0873209635417\n",
      "Epoch 1353/3000, Loss: 1316.5791015625\n",
      "Epoch 1354/3000, Loss: 1319.9342041015625\n",
      "Epoch 1355/3000, Loss: 1275.7247111002605\n",
      "Epoch 1356/3000, Loss: 1233.052754720052\n",
      "Epoch 1357/3000, Loss: 1399.080322265625\n",
      "Epoch 1358/3000, Loss: 1295.422383626302\n",
      "Epoch 1359/3000, Loss: 1286.3431396484375\n",
      "Epoch 1360/3000, Loss: 1213.6849466959636\n",
      "Epoch 1361/3000, Loss: 1335.8744710286458\n",
      "Epoch 1362/3000, Loss: 1308.9603678385417\n",
      "Epoch 1363/3000, Loss: 1295.7149251302083\n",
      "Epoch 1364/3000, Loss: 1365.0472615559895\n",
      "Epoch 1365/3000, Loss: 1308.7855428059895\n",
      "Epoch 1366/3000, Loss: 1320.5248006184895\n",
      "Epoch 1367/3000, Loss: 1321.214864095052\n",
      "Epoch 1368/3000, Loss: 1269.4954020182292\n",
      "Epoch 1369/3000, Loss: 1321.8112386067708\n",
      "Epoch 1370/3000, Loss: 1355.629659016927\n",
      "Epoch 1371/3000, Loss: 1312.0851440429688\n",
      "Epoch 1372/3000, Loss: 1241.7250061035156\n",
      "Epoch 1373/3000, Loss: 1234.7535909016926\n",
      "Epoch 1374/3000, Loss: 1243.6265055338542\n",
      "Epoch 1375/3000, Loss: 1251.8591715494792\n",
      "Epoch 1376/3000, Loss: 1336.9009399414062\n",
      "Epoch 1377/3000, Loss: 1189.1058959960938\n",
      "Epoch 1378/3000, Loss: 1285.8093668619792\n",
      "Epoch 1379/3000, Loss: 1335.5747477213542\n",
      "Epoch 1380/3000, Loss: 1212.9266153971355\n",
      "Epoch 1381/3000, Loss: 1282.9410400390625\n",
      "Epoch 1382/3000, Loss: 1252.0605061848958\n",
      "Epoch 1383/3000, Loss: 1273.6192830403645\n",
      "Epoch 1384/3000, Loss: 1325.722900390625\n",
      "Epoch 1385/3000, Loss: 1297.8023071289062\n",
      "Epoch 1386/3000, Loss: 1231.1242065429688\n",
      "Epoch 1387/3000, Loss: 1283.823994954427\n",
      "Epoch 1388/3000, Loss: 1271.588887532552\n",
      "Epoch 1389/3000, Loss: 1316.4155883789062\n",
      "Epoch 1390/3000, Loss: 1163.7540893554688\n",
      "Epoch 1391/3000, Loss: 1287.9802652994792\n",
      "Epoch 1392/3000, Loss: 1231.6580301920574\n",
      "Epoch 1393/3000, Loss: 1227.9552815755208\n",
      "Epoch 1394/3000, Loss: 1278.026387532552\n",
      "Epoch 1395/3000, Loss: 1281.0446879069011\n",
      "Epoch 1396/3000, Loss: 1292.5882059733074\n",
      "Epoch 1397/3000, Loss: 1209.2780659993489\n",
      "Epoch 1398/3000, Loss: 1235.3158162434895\n",
      "Epoch 1399/3000, Loss: 1284.2770385742188\n",
      "Epoch 1400/3000, Loss: 1285.9679565429688\n",
      "Epoch 1401/3000, Loss: 1376.6182250976562\n",
      "Epoch 1402/3000, Loss: 1239.9176228841145\n",
      "Epoch 1403/3000, Loss: 1172.5738016764324\n",
      "Epoch 1404/3000, Loss: 1319.8407796223958\n",
      "Epoch 1405/3000, Loss: 1213.0016072591145\n",
      "Epoch 1406/3000, Loss: 1172.2036743164062\n",
      "Epoch 1407/3000, Loss: 1260.6734212239583\n",
      "Epoch 1408/3000, Loss: 1405.1253865559895\n",
      "Epoch 1409/3000, Loss: 1258.840576171875\n",
      "Epoch 1410/3000, Loss: 1265.9004923502605\n",
      "Epoch 1411/3000, Loss: 1305.3837076822917\n",
      "Epoch 1412/3000, Loss: 1285.5902099609375\n",
      "Epoch 1413/3000, Loss: 1263.8172403971355\n",
      "Epoch 1414/3000, Loss: 1331.3697713216145\n",
      "Epoch 1415/3000, Loss: 1287.4480590820312\n",
      "Epoch 1416/3000, Loss: 1234.3080037434895\n",
      "Epoch 1417/3000, Loss: 1284.0398763020833\n",
      "Epoch 1418/3000, Loss: 1298.7967936197917\n",
      "Epoch 1419/3000, Loss: 1248.4034830729167\n",
      "Epoch 1420/3000, Loss: 1277.695536295573\n",
      "Epoch 1421/3000, Loss: 1342.4921264648438\n",
      "Epoch 1422/3000, Loss: 1260.6226603190105\n",
      "Epoch 1423/3000, Loss: 1300.5857950846355\n",
      "Epoch 1424/3000, Loss: 1278.0105997721355\n",
      "Epoch 1425/3000, Loss: 1304.386942545573\n",
      "Epoch 1426/3000, Loss: 1260.8419189453125\n",
      "Epoch 1427/3000, Loss: 1263.4672037760417\n",
      "Epoch 1428/3000, Loss: 1273.3221638997395\n",
      "Epoch 1429/3000, Loss: 1201.3820292154949\n",
      "Epoch 1430/3000, Loss: 1322.7356974283855\n",
      "Epoch 1431/3000, Loss: 1336.7863159179688\n",
      "Epoch 1432/3000, Loss: 1271.8702596028645\n",
      "Epoch 1433/3000, Loss: 1286.2347819010417\n",
      "Epoch 1434/3000, Loss: 1279.9529418945312\n",
      "Epoch 1435/3000, Loss: 1257.632832845052\n",
      "Epoch 1436/3000, Loss: 1206.1490275065105\n",
      "Epoch 1437/3000, Loss: 1244.1685180664062\n",
      "Epoch 1438/3000, Loss: 1257.7093302408855\n",
      "Epoch 1439/3000, Loss: 1211.4569193522136\n",
      "Epoch 1440/3000, Loss: 1267.900858561198\n",
      "Epoch 1441/3000, Loss: 1218.198506673177\n",
      "Epoch 1442/3000, Loss: 1255.6068420410156\n",
      "Epoch 1443/3000, Loss: 1206.4874267578125\n",
      "Epoch 1444/3000, Loss: 1258.953348795573\n",
      "Epoch 1445/3000, Loss: 1252.2486572265625\n",
      "Epoch 1446/3000, Loss: 1270.7670084635417\n",
      "Epoch 1447/3000, Loss: 1291.6531575520833\n",
      "Epoch 1448/3000, Loss: 1254.0076090494792\n",
      "Epoch 1449/3000, Loss: 1288.7624918619792\n",
      "Epoch 1450/3000, Loss: 1257.6099243164062\n",
      "Epoch 1451/3000, Loss: 1309.874532063802\n",
      "Epoch 1452/3000, Loss: 1277.3866780598958\n",
      "Epoch 1453/3000, Loss: 1367.0434977213542\n",
      "Epoch 1454/3000, Loss: 1280.1907552083333\n",
      "Epoch 1455/3000, Loss: 1208.9065958658855\n",
      "Epoch 1456/3000, Loss: 1299.6571044921875\n",
      "Epoch 1457/3000, Loss: 1345.7551676432292\n",
      "Epoch 1458/3000, Loss: 1358.3727416992188\n",
      "Epoch 1459/3000, Loss: 1215.4288940429688\n",
      "Epoch 1460/3000, Loss: 1221.8932393391926\n",
      "Epoch 1461/3000, Loss: 1276.3551228841145\n",
      "Epoch 1462/3000, Loss: 1190.5579833984375\n",
      "Epoch 1463/3000, Loss: 1252.1829223632812\n",
      "Epoch 1464/3000, Loss: 1245.031982421875\n",
      "Epoch 1465/3000, Loss: 1306.0003051757812\n",
      "Epoch 1466/3000, Loss: 1323.4271036783855\n",
      "Epoch 1467/3000, Loss: 1330.5679524739583\n",
      "Epoch 1468/3000, Loss: 1374.7051798502605\n",
      "Epoch 1469/3000, Loss: 1337.5231323242188\n",
      "Epoch 1470/3000, Loss: 1230.2913411458333\n",
      "Epoch 1471/3000, Loss: 1206.0125122070312\n",
      "Epoch 1472/3000, Loss: 1340.4002685546875\n",
      "Epoch 1473/3000, Loss: 1274.6983439127605\n",
      "Epoch 1474/3000, Loss: 1285.4393920898438\n",
      "Epoch 1475/3000, Loss: 1250.0367431640625\n",
      "Epoch 1476/3000, Loss: 1177.2621256510417\n",
      "Epoch 1477/3000, Loss: 1305.3819580078125\n",
      "Epoch 1478/3000, Loss: 1287.7827555338542\n",
      "Epoch 1479/3000, Loss: 1198.6028849283855\n",
      "Epoch 1480/3000, Loss: 1241.359354654948\n",
      "Epoch 1481/3000, Loss: 1343.9981892903645\n",
      "Epoch 1482/3000, Loss: 1301.724853515625\n",
      "Epoch 1483/3000, Loss: 1224.7209065755208\n",
      "Epoch 1484/3000, Loss: 1222.8461303710938\n",
      "Epoch 1485/3000, Loss: 1222.6505126953125\n",
      "Epoch 1486/3000, Loss: 1216.3796081542969\n",
      "Epoch 1487/3000, Loss: 1292.9223225911458\n",
      "Epoch 1488/3000, Loss: 1253.31298828125\n",
      "Epoch 1489/3000, Loss: 1212.0736490885417\n",
      "Epoch 1490/3000, Loss: 1274.7886555989583\n",
      "Epoch 1491/3000, Loss: 1268.945068359375\n",
      "Epoch 1492/3000, Loss: 1181.219217936198\n",
      "Epoch 1493/3000, Loss: 1264.1833902994792\n",
      "Epoch 1494/3000, Loss: 1316.170145670573\n",
      "Epoch 1495/3000, Loss: 1216.784647623698\n",
      "Epoch 1496/3000, Loss: 1237.1062622070312\n",
      "Epoch 1497/3000, Loss: 1355.2281290690105\n",
      "Epoch 1498/3000, Loss: 1151.7711385091145\n",
      "Epoch 1499/3000, Loss: 1279.2611490885417\n",
      "Epoch 1500/3000, Loss: 1340.9692789713542\n",
      "Epoch 1501/3000, Loss: 1224.2174682617188\n",
      "Epoch 1502/3000, Loss: 1213.6529744466145\n",
      "Epoch 1503/3000, Loss: 1331.243408203125\n",
      "Epoch 1504/3000, Loss: 1202.8260803222656\n",
      "Epoch 1505/3000, Loss: 1316.9921875\n",
      "Epoch 1506/3000, Loss: 1325.26904296875\n",
      "Epoch 1507/3000, Loss: 1254.5579223632812\n",
      "Epoch 1508/3000, Loss: 1304.2765299479167\n",
      "Epoch 1509/3000, Loss: 1317.4068603515625\n",
      "Epoch 1510/3000, Loss: 1264.5903727213542\n",
      "Epoch 1511/3000, Loss: 1311.7462361653645\n",
      "Epoch 1512/3000, Loss: 1317.6876627604167\n",
      "Epoch 1513/3000, Loss: 1282.898213704427\n",
      "Epoch 1514/3000, Loss: 1280.0252482096355\n",
      "Epoch 1515/3000, Loss: 1303.9597574869792\n",
      "Epoch 1516/3000, Loss: 1244.9619954427083\n",
      "Epoch 1517/3000, Loss: 1283.0977376302083\n",
      "Epoch 1518/3000, Loss: 1185.8961995442708\n",
      "Epoch 1519/3000, Loss: 1180.3651936848958\n",
      "Epoch 1520/3000, Loss: 1238.3534545898438\n",
      "Epoch 1521/3000, Loss: 1253.443623860677\n",
      "Epoch 1522/3000, Loss: 1257.4617513020833\n",
      "Epoch 1523/3000, Loss: 1249.3079833984375\n",
      "Epoch 1524/3000, Loss: 1279.355936686198\n",
      "Epoch 1525/3000, Loss: 1243.2101033528645\n",
      "Epoch 1526/3000, Loss: 1339.3091634114583\n",
      "Epoch 1527/3000, Loss: 1226.9578450520833\n",
      "Epoch 1528/3000, Loss: 1318.531270345052\n",
      "Epoch 1529/3000, Loss: 1253.4474690755208\n",
      "Epoch 1530/3000, Loss: 1227.9014485677083\n",
      "Epoch 1531/3000, Loss: 1312.9774576822917\n",
      "Epoch 1532/3000, Loss: 1315.0663045247395\n",
      "Epoch 1533/3000, Loss: 1311.182861328125\n",
      "Epoch 1534/3000, Loss: 1256.2931111653645\n",
      "Epoch 1535/3000, Loss: 1213.654032389323\n",
      "Epoch 1536/3000, Loss: 1289.9603474934895\n",
      "Epoch 1537/3000, Loss: 1168.4462992350261\n",
      "Epoch 1538/3000, Loss: 1259.6464029947917\n",
      "Epoch 1539/3000, Loss: 1344.0455729166667\n",
      "Epoch 1540/3000, Loss: 1227.0003255208333\n",
      "Epoch 1541/3000, Loss: 1201.15869140625\n",
      "Epoch 1542/3000, Loss: 1212.5833129882812\n",
      "Epoch 1543/3000, Loss: 1234.134012858073\n",
      "Epoch 1544/3000, Loss: 1314.2880452473958\n",
      "Epoch 1545/3000, Loss: 1222.0738932291667\n",
      "Epoch 1546/3000, Loss: 1273.0826212565105\n",
      "Epoch 1547/3000, Loss: 1212.3826497395833\n",
      "Epoch 1548/3000, Loss: 1179.6948547363281\n",
      "Epoch 1549/3000, Loss: 1202.9878438313801\n",
      "Epoch 1550/3000, Loss: 1268.741963704427\n",
      "Epoch 1551/3000, Loss: 1305.2684733072917\n",
      "Epoch 1552/3000, Loss: 1274.9520161946614\n",
      "Epoch 1553/3000, Loss: 1250.3939412434895\n",
      "Epoch 1554/3000, Loss: 1258.0796712239583\n",
      "Epoch 1555/3000, Loss: 1230.3790893554688\n",
      "Epoch 1556/3000, Loss: 1180.9786987304688\n",
      "Epoch 1557/3000, Loss: 1240.1653645833333\n",
      "Epoch 1558/3000, Loss: 1203.6051127115886\n",
      "Epoch 1559/3000, Loss: 1297.9419555664062\n",
      "Epoch 1560/3000, Loss: 1253.0020955403645\n",
      "Epoch 1561/3000, Loss: 1239.5254313151042\n",
      "Epoch 1562/3000, Loss: 1219.2730611165364\n",
      "Epoch 1563/3000, Loss: 1182.1689860026042\n",
      "Epoch 1564/3000, Loss: 1224.4447224934895\n",
      "Epoch 1565/3000, Loss: 1249.2012736002605\n",
      "Epoch 1566/3000, Loss: 1293.891133626302\n",
      "Epoch 1567/3000, Loss: 1346.2039794921875\n",
      "Epoch 1568/3000, Loss: 1257.7372233072917\n",
      "Epoch 1569/3000, Loss: 1309.7374877929688\n",
      "Epoch 1570/3000, Loss: 1285.0098470052083\n",
      "Epoch 1571/3000, Loss: 1278.9847819010417\n",
      "Epoch 1572/3000, Loss: 1238.9710083007812\n",
      "Epoch 1573/3000, Loss: 1261.06884765625\n",
      "Epoch 1574/3000, Loss: 1268.0133056640625\n",
      "Epoch 1575/3000, Loss: 1358.3638712565105\n",
      "Epoch 1576/3000, Loss: 1284.6153767903645\n",
      "Epoch 1577/3000, Loss: 1193.2760620117188\n",
      "Epoch 1578/3000, Loss: 1254.8905639648438\n",
      "Epoch 1579/3000, Loss: 1207.7005411783855\n",
      "Epoch 1580/3000, Loss: 1294.6046142578125\n",
      "Epoch 1581/3000, Loss: 1218.7015686035156\n",
      "Epoch 1582/3000, Loss: 1292.890604654948\n",
      "Epoch 1583/3000, Loss: 1309.839111328125\n",
      "Epoch 1584/3000, Loss: 1271.305684407552\n",
      "Epoch 1585/3000, Loss: 1249.510009765625\n",
      "Epoch 1586/3000, Loss: 1313.7419840494792\n",
      "Epoch 1587/3000, Loss: 1288.9055582682292\n",
      "Epoch 1588/3000, Loss: 1229.4281412760417\n",
      "Epoch 1589/3000, Loss: 1258.3848876953125\n",
      "Epoch 1590/3000, Loss: 1169.2013041178386\n",
      "Epoch 1591/3000, Loss: 1222.5493977864583\n",
      "Epoch 1592/3000, Loss: 1232.8789469401042\n",
      "Epoch 1593/3000, Loss: 1304.5724080403645\n",
      "Epoch 1594/3000, Loss: 1293.2174072265625\n",
      "Epoch 1595/3000, Loss: 1332.4310302734375\n",
      "Epoch 1596/3000, Loss: 1168.9881591796875\n",
      "Epoch 1597/3000, Loss: 1221.2445068359375\n",
      "Epoch 1598/3000, Loss: 1284.0950520833333\n",
      "Epoch 1599/3000, Loss: 1361.890625\n",
      "Epoch 1600/3000, Loss: 1210.8914591471355\n",
      "Epoch 1601/3000, Loss: 1287.5838419596355\n",
      "Epoch 1602/3000, Loss: 1300.02685546875\n",
      "Epoch 1603/3000, Loss: 1175.5465087890625\n",
      "Epoch 1604/3000, Loss: 1268.4227905273438\n",
      "Epoch 1605/3000, Loss: 1286.8517354329426\n",
      "Epoch 1606/3000, Loss: 1170.252217610677\n",
      "Epoch 1607/3000, Loss: 1218.1541544596355\n",
      "Epoch 1608/3000, Loss: 1283.5909627278645\n",
      "Epoch 1609/3000, Loss: 1237.6398518880208\n",
      "Epoch 1610/3000, Loss: 1196.993916829427\n",
      "Epoch 1611/3000, Loss: 1234.4859619140625\n",
      "Epoch 1612/3000, Loss: 1226.3397318522136\n",
      "Epoch 1613/3000, Loss: 1255.9805704752605\n",
      "Epoch 1614/3000, Loss: 1202.8348388671875\n",
      "Epoch 1615/3000, Loss: 1130.7831217447917\n",
      "Epoch 1616/3000, Loss: 1365.2570190429688\n",
      "Epoch 1617/3000, Loss: 1276.9961547851562\n",
      "Epoch 1618/3000, Loss: 1245.4414672851562\n",
      "Epoch 1619/3000, Loss: 1232.6163533528645\n",
      "Epoch 1620/3000, Loss: 1214.6427510579426\n",
      "Epoch 1621/3000, Loss: 1271.220703125\n",
      "Epoch 1622/3000, Loss: 1230.029032389323\n",
      "Epoch 1623/3000, Loss: 1265.8332926432292\n",
      "Epoch 1624/3000, Loss: 1247.4319458007812\n",
      "Epoch 1625/3000, Loss: 1304.799784342448\n",
      "Epoch 1626/3000, Loss: 1316.5557657877605\n",
      "Epoch 1627/3000, Loss: 1190.3914794921875\n",
      "Epoch 1628/3000, Loss: 1294.230448404948\n",
      "Epoch 1629/3000, Loss: 1302.542236328125\n",
      "Epoch 1630/3000, Loss: 1279.3630777994792\n",
      "Epoch 1631/3000, Loss: 1309.355000813802\n",
      "Epoch 1632/3000, Loss: 1266.6358032226562\n",
      "Epoch 1633/3000, Loss: 1253.469706217448\n",
      "Epoch 1634/3000, Loss: 1260.1490987141926\n",
      "Epoch 1635/3000, Loss: 1266.8094685872395\n",
      "Epoch 1636/3000, Loss: 1271.7716674804688\n",
      "Epoch 1637/3000, Loss: 1302.4241536458333\n",
      "Epoch 1638/3000, Loss: 1223.132344563802\n",
      "Epoch 1639/3000, Loss: 1292.7215169270833\n",
      "Epoch 1640/3000, Loss: 1269.960713704427\n",
      "Epoch 1641/3000, Loss: 1215.0539957682292\n",
      "Epoch 1642/3000, Loss: 1233.6703186035156\n",
      "Epoch 1643/3000, Loss: 1256.4639282226562\n",
      "Epoch 1644/3000, Loss: 1218.7271321614583\n",
      "Epoch 1645/3000, Loss: 1263.9247639973958\n",
      "Epoch 1646/3000, Loss: 1221.5823567708333\n",
      "Epoch 1647/3000, Loss: 1256.2948201497395\n",
      "Epoch 1648/3000, Loss: 1168.422098795573\n",
      "Epoch 1649/3000, Loss: 1213.573486328125\n",
      "Epoch 1650/3000, Loss: 1209.8763020833333\n",
      "Epoch 1651/3000, Loss: 1169.238301595052\n",
      "Epoch 1652/3000, Loss: 1178.4725748697917\n",
      "Epoch 1653/3000, Loss: 1163.426534016927\n",
      "Epoch 1654/3000, Loss: 1276.0350545247395\n",
      "Epoch 1655/3000, Loss: 1189.334737141927\n",
      "Epoch 1656/3000, Loss: 1173.9873657226562\n",
      "Epoch 1657/3000, Loss: 1281.979024251302\n",
      "Epoch 1658/3000, Loss: 1205.8551025390625\n",
      "Epoch 1659/3000, Loss: 1251.9112141927083\n",
      "Epoch 1660/3000, Loss: 1299.71044921875\n",
      "Epoch 1661/3000, Loss: 1185.2550760904949\n",
      "Epoch 1662/3000, Loss: 1295.455098470052\n",
      "Epoch 1663/3000, Loss: 1163.603291829427\n",
      "Epoch 1664/3000, Loss: 1330.3284098307292\n",
      "Epoch 1665/3000, Loss: 1191.6803487141926\n",
      "Epoch 1666/3000, Loss: 1191.5512288411458\n",
      "Epoch 1667/3000, Loss: 1230.507832845052\n",
      "Epoch 1668/3000, Loss: 1235.7151896158855\n",
      "Epoch 1669/3000, Loss: 1281.8178100585938\n",
      "Epoch 1670/3000, Loss: 1217.1400349934895\n",
      "Epoch 1671/3000, Loss: 1230.5602518717449\n",
      "Epoch 1672/3000, Loss: 1234.2096761067708\n",
      "Epoch 1673/3000, Loss: 1253.354512532552\n",
      "Epoch 1674/3000, Loss: 1273.6841430664062\n",
      "Epoch 1675/3000, Loss: 1263.8443400065105\n",
      "Epoch 1676/3000, Loss: 1212.074239095052\n",
      "Epoch 1677/3000, Loss: 1174.8184916178386\n",
      "Epoch 1678/3000, Loss: 1196.2949625651042\n",
      "Epoch 1679/3000, Loss: 1227.2173563639324\n",
      "Epoch 1680/3000, Loss: 1154.0945332845051\n",
      "Epoch 1681/3000, Loss: 1190.059326171875\n",
      "Epoch 1682/3000, Loss: 1263.748514811198\n",
      "Epoch 1683/3000, Loss: 1263.2039184570312\n",
      "Epoch 1684/3000, Loss: 1140.0078531901042\n",
      "Epoch 1685/3000, Loss: 1151.188741048177\n",
      "Epoch 1686/3000, Loss: 1182.92919921875\n",
      "Epoch 1687/3000, Loss: 1154.4836018880208\n",
      "Epoch 1688/3000, Loss: 1230.5632731119792\n",
      "Epoch 1689/3000, Loss: 1244.9761454264324\n",
      "Epoch 1690/3000, Loss: 1266.5488891601562\n",
      "Epoch 1691/3000, Loss: 1278.9241943359375\n",
      "Epoch 1692/3000, Loss: 1235.0660603841145\n",
      "Epoch 1693/3000, Loss: 1181.5848185221355\n",
      "Epoch 1694/3000, Loss: 1207.8644002278645\n",
      "Epoch 1695/3000, Loss: 1285.4299723307292\n",
      "Epoch 1696/3000, Loss: 1268.3058675130208\n",
      "Epoch 1697/3000, Loss: 1207.8638916015625\n",
      "Epoch 1698/3000, Loss: 1176.7149047851562\n",
      "Epoch 1699/3000, Loss: 1190.6231689453125\n",
      "Epoch 1700/3000, Loss: 1156.7568461100261\n",
      "Epoch 1701/3000, Loss: 1256.8484497070312\n",
      "Epoch 1702/3000, Loss: 1211.6772054036458\n",
      "Epoch 1703/3000, Loss: 1160.4282124837239\n",
      "Epoch 1704/3000, Loss: 1273.7283732096355\n",
      "Epoch 1705/3000, Loss: 1227.2085571289062\n",
      "Epoch 1706/3000, Loss: 1230.4933471679688\n",
      "Epoch 1707/3000, Loss: 1117.7327880859375\n",
      "Epoch 1708/3000, Loss: 1179.8334655761719\n",
      "Epoch 1709/3000, Loss: 1193.28955078125\n",
      "Epoch 1710/3000, Loss: 1230.443603515625\n",
      "Epoch 1711/3000, Loss: 1208.3068237304688\n",
      "Epoch 1712/3000, Loss: 1107.1657816569011\n",
      "Epoch 1713/3000, Loss: 1218.2088012695312\n",
      "Epoch 1714/3000, Loss: 1325.6072387695312\n",
      "Epoch 1715/3000, Loss: 1242.554219563802\n",
      "Epoch 1716/3000, Loss: 1189.6662190755208\n",
      "Epoch 1717/3000, Loss: 1262.9845377604167\n",
      "Epoch 1718/3000, Loss: 1233.3502197265625\n",
      "Epoch 1719/3000, Loss: 1176.7981974283855\n",
      "Epoch 1720/3000, Loss: 1207.8353373209636\n",
      "Epoch 1721/3000, Loss: 1199.7762451171875\n",
      "Epoch 1722/3000, Loss: 1265.9288940429688\n",
      "Epoch 1723/3000, Loss: 1157.2719319661458\n",
      "Epoch 1724/3000, Loss: 1265.2892456054688\n",
      "Epoch 1725/3000, Loss: 1324.5126342773438\n",
      "Epoch 1726/3000, Loss: 1181.3391418457031\n",
      "Epoch 1727/3000, Loss: 1368.211669921875\n",
      "Epoch 1728/3000, Loss: 1239.1181233723958\n",
      "Epoch 1729/3000, Loss: 1466.4267578125\n",
      "Epoch 1730/3000, Loss: 1272.2838134765625\n",
      "Epoch 1731/3000, Loss: 1275.8318277994792\n",
      "Epoch 1732/3000, Loss: 1204.0621134440105\n",
      "Epoch 1733/3000, Loss: 1204.7340087890625\n",
      "Epoch 1734/3000, Loss: 1214.920654296875\n",
      "Epoch 1735/3000, Loss: 1176.5472412109375\n",
      "Epoch 1736/3000, Loss: 1313.6875\n",
      "Epoch 1737/3000, Loss: 1220.79931640625\n",
      "Epoch 1738/3000, Loss: 1265.3993733723958\n",
      "Epoch 1739/3000, Loss: 1161.3602498372395\n",
      "Epoch 1740/3000, Loss: 1227.212137858073\n",
      "Epoch 1741/3000, Loss: 1150.7068786621094\n",
      "Epoch 1742/3000, Loss: 1238.524658203125\n",
      "Epoch 1743/3000, Loss: 1217.4413045247395\n",
      "Epoch 1744/3000, Loss: 1178.8319091796875\n",
      "Epoch 1745/3000, Loss: 1210.2531331380208\n",
      "Epoch 1746/3000, Loss: 1192.2539672851562\n",
      "Epoch 1747/3000, Loss: 1237.9861450195312\n",
      "Epoch 1748/3000, Loss: 1214.1656901041667\n",
      "Epoch 1749/3000, Loss: 1197.0629069010417\n",
      "Epoch 1750/3000, Loss: 1202.6036173502605\n",
      "Epoch 1751/3000, Loss: 1182.0918986002605\n",
      "Epoch 1752/3000, Loss: 1371.937764485677\n",
      "Epoch 1753/3000, Loss: 1210.3942260742188\n",
      "Epoch 1754/3000, Loss: 1247.0756429036458\n",
      "Epoch 1755/3000, Loss: 1186.5787048339844\n",
      "Epoch 1756/3000, Loss: 1177.2605895996094\n",
      "Epoch 1757/3000, Loss: 1261.566385904948\n",
      "Epoch 1758/3000, Loss: 1063.1389770507812\n",
      "Epoch 1759/3000, Loss: 1126.9896952311199\n",
      "Epoch 1760/3000, Loss: 1165.1714680989583\n",
      "Epoch 1761/3000, Loss: 1230.3679504394531\n",
      "Epoch 1762/3000, Loss: 1231.9644571940105\n",
      "Epoch 1763/3000, Loss: 1214.5988159179688\n",
      "Epoch 1764/3000, Loss: 1223.6985270182292\n",
      "Epoch 1765/3000, Loss: 1218.2946065266926\n",
      "Epoch 1766/3000, Loss: 1170.5350240071614\n",
      "Epoch 1767/3000, Loss: 1260.350362141927\n",
      "Epoch 1768/3000, Loss: 1218.6419067382812\n",
      "Epoch 1769/3000, Loss: 1170.9814453125\n",
      "Epoch 1770/3000, Loss: 1279.285868326823\n",
      "Epoch 1771/3000, Loss: 1252.7490234375\n",
      "Epoch 1772/3000, Loss: 1262.2760620117188\n",
      "Epoch 1773/3000, Loss: 1236.115966796875\n",
      "Epoch 1774/3000, Loss: 1220.337666829427\n",
      "Epoch 1775/3000, Loss: 1276.5465698242188\n",
      "Epoch 1776/3000, Loss: 1187.7379048665364\n",
      "Epoch 1777/3000, Loss: 1195.051534016927\n",
      "Epoch 1778/3000, Loss: 1353.557393391927\n",
      "Epoch 1779/3000, Loss: 1246.3424072265625\n",
      "Epoch 1780/3000, Loss: 1225.55224609375\n",
      "Epoch 1781/3000, Loss: 1137.5102030436199\n",
      "Epoch 1782/3000, Loss: 1258.5293782552083\n",
      "Epoch 1783/3000, Loss: 1289.367411295573\n",
      "Epoch 1784/3000, Loss: 1295.4765625\n",
      "Epoch 1785/3000, Loss: 1239.9905395507812\n",
      "Epoch 1786/3000, Loss: 1187.8839823404949\n",
      "Epoch 1787/3000, Loss: 1244.611592610677\n",
      "Epoch 1788/3000, Loss: 1156.6393025716145\n",
      "Epoch 1789/3000, Loss: 1277.3178100585938\n",
      "Epoch 1790/3000, Loss: 1271.9535319010417\n",
      "Epoch 1791/3000, Loss: 1235.4260660807292\n",
      "Epoch 1792/3000, Loss: 1270.604268391927\n",
      "Epoch 1793/3000, Loss: 1290.0466715494792\n",
      "Epoch 1794/3000, Loss: 1130.1592915852864\n",
      "Epoch 1795/3000, Loss: 1286.0827026367188\n",
      "Epoch 1796/3000, Loss: 1280.950215657552\n",
      "Epoch 1797/3000, Loss: 1284.8070068359375\n",
      "Epoch 1798/3000, Loss: 1234.3863016764324\n",
      "Epoch 1799/3000, Loss: 1217.1868489583333\n",
      "Epoch 1800/3000, Loss: 1224.4407145182292\n",
      "Epoch 1801/3000, Loss: 1160.9940592447917\n",
      "Epoch 1802/3000, Loss: 1171.701416015625\n",
      "Epoch 1803/3000, Loss: 1183.0274047851562\n",
      "Epoch 1804/3000, Loss: 1133.2896118164062\n",
      "Epoch 1805/3000, Loss: 1187.9002278645833\n",
      "Epoch 1806/3000, Loss: 1162.8218282063801\n",
      "Epoch 1807/3000, Loss: 1170.583719889323\n",
      "Epoch 1808/3000, Loss: 1135.613301595052\n",
      "Epoch 1809/3000, Loss: 1187.7120564778645\n",
      "Epoch 1810/3000, Loss: 1172.3091532389324\n",
      "Epoch 1811/3000, Loss: 1167.0733337402344\n",
      "Epoch 1812/3000, Loss: 1293.8036295572917\n",
      "Epoch 1813/3000, Loss: 1272.655517578125\n",
      "Epoch 1814/3000, Loss: 1171.8755391438801\n",
      "Epoch 1815/3000, Loss: 1176.1749267578125\n",
      "Epoch 1816/3000, Loss: 1259.2631022135417\n",
      "Epoch 1817/3000, Loss: 1206.1141357421875\n",
      "Epoch 1818/3000, Loss: 1150.7668863932292\n",
      "Epoch 1819/3000, Loss: 1217.6971537272136\n",
      "Epoch 1820/3000, Loss: 1250.3841552734375\n",
      "Epoch 1821/3000, Loss: 1282.0410766601562\n",
      "Epoch 1822/3000, Loss: 1159.7227274576824\n",
      "Epoch 1823/3000, Loss: 1242.0283610026042\n",
      "Epoch 1824/3000, Loss: 1219.7674153645833\n",
      "Epoch 1825/3000, Loss: 1192.754903157552\n",
      "Epoch 1826/3000, Loss: 1294.9761962890625\n",
      "Epoch 1827/3000, Loss: 1263.6695149739583\n",
      "Epoch 1828/3000, Loss: 1223.41015625\n",
      "Epoch 1829/3000, Loss: 1156.3645629882812\n",
      "Epoch 1830/3000, Loss: 1183.2650146484375\n",
      "Epoch 1831/3000, Loss: 1081.3479614257812\n",
      "Epoch 1832/3000, Loss: 1288.319112141927\n",
      "Epoch 1833/3000, Loss: 1159.9460856119792\n",
      "Epoch 1834/3000, Loss: 1203.5725708007812\n",
      "Epoch 1835/3000, Loss: 1207.4174194335938\n",
      "Epoch 1836/3000, Loss: 1235.9730834960938\n",
      "Epoch 1837/3000, Loss: 1199.529317220052\n",
      "Epoch 1838/3000, Loss: 1187.4110209147136\n",
      "Epoch 1839/3000, Loss: 1198.5851236979167\n",
      "Epoch 1840/3000, Loss: 1092.143819173177\n",
      "Epoch 1841/3000, Loss: 1146.7452901204426\n",
      "Epoch 1842/3000, Loss: 1353.845479329427\n",
      "Epoch 1843/3000, Loss: 1144.0767110188801\n",
      "Epoch 1844/3000, Loss: 1201.5429585774739\n",
      "Epoch 1845/3000, Loss: 1159.2662353515625\n",
      "Epoch 1846/3000, Loss: 1180.1873982747395\n",
      "Epoch 1847/3000, Loss: 1291.5384521484375\n",
      "Epoch 1848/3000, Loss: 1119.0243225097656\n",
      "Epoch 1849/3000, Loss: 1222.5610961914062\n",
      "Epoch 1850/3000, Loss: 1160.8664957682292\n",
      "Epoch 1851/3000, Loss: 1161.2008565266926\n",
      "Epoch 1852/3000, Loss: 1302.8958536783855\n",
      "Epoch 1853/3000, Loss: 1235.1979166666667\n",
      "Epoch 1854/3000, Loss: 1158.3998209635417\n",
      "Epoch 1855/3000, Loss: 1266.9632975260417\n",
      "Epoch 1856/3000, Loss: 1312.79833984375\n",
      "Epoch 1857/3000, Loss: 1218.7474772135417\n",
      "Epoch 1858/3000, Loss: 1144.6390787760417\n",
      "Epoch 1859/3000, Loss: 1206.6278686523438\n",
      "Epoch 1860/3000, Loss: 1150.8800659179688\n",
      "Epoch 1861/3000, Loss: 1131.3546651204426\n",
      "Epoch 1862/3000, Loss: 1159.625244140625\n",
      "Epoch 1863/3000, Loss: 1187.2796834309895\n",
      "Epoch 1864/3000, Loss: 1328.498311360677\n",
      "Epoch 1865/3000, Loss: 1194.2504272460938\n",
      "Epoch 1866/3000, Loss: 1271.6082153320312\n",
      "Epoch 1867/3000, Loss: 1201.4099527994792\n",
      "Epoch 1868/3000, Loss: 1155.1219991048176\n",
      "Epoch 1869/3000, Loss: 1147.4927673339844\n",
      "Epoch 1870/3000, Loss: 1172.4647318522136\n",
      "Epoch 1871/3000, Loss: 1223.0418497721355\n",
      "Epoch 1872/3000, Loss: 1201.313212076823\n",
      "Epoch 1873/3000, Loss: 1307.652587890625\n",
      "Epoch 1874/3000, Loss: 1172.820068359375\n",
      "Epoch 1875/3000, Loss: 1222.1199340820312\n",
      "Epoch 1876/3000, Loss: 1238.347147623698\n",
      "Epoch 1877/3000, Loss: 1239.1737162272136\n",
      "Epoch 1878/3000, Loss: 1191.5869547526042\n",
      "Epoch 1879/3000, Loss: 1331.6409912109375\n",
      "Epoch 1880/3000, Loss: 1232.7147420247395\n",
      "Epoch 1881/3000, Loss: 1227.262715657552\n",
      "Epoch 1882/3000, Loss: 1209.8239034016926\n",
      "Epoch 1883/3000, Loss: 1207.254903157552\n",
      "Epoch 1884/3000, Loss: 1187.1183675130208\n",
      "Epoch 1885/3000, Loss: 1187.517313639323\n",
      "Epoch 1886/3000, Loss: 1234.4776611328125\n",
      "Epoch 1887/3000, Loss: 1132.4869486490886\n",
      "Epoch 1888/3000, Loss: 1198.7931213378906\n",
      "Epoch 1889/3000, Loss: 1242.3511149088542\n",
      "Epoch 1890/3000, Loss: 1157.1466267903645\n",
      "Epoch 1891/3000, Loss: 1257.1199137369792\n",
      "Epoch 1892/3000, Loss: 1194.3145751953125\n",
      "Epoch 1893/3000, Loss: 1251.822021484375\n",
      "Epoch 1894/3000, Loss: 1226.8948771158855\n",
      "Epoch 1895/3000, Loss: 1135.823018391927\n",
      "Epoch 1896/3000, Loss: 1165.3845113118489\n",
      "Epoch 1897/3000, Loss: 1187.5856018066406\n",
      "Epoch 1898/3000, Loss: 1293.2636108398438\n",
      "Epoch 1899/3000, Loss: 1186.7123514811199\n",
      "Epoch 1900/3000, Loss: 1150.206787109375\n",
      "Epoch 1901/3000, Loss: 1231.531473795573\n",
      "Epoch 1902/3000, Loss: 1218.8814290364583\n",
      "Epoch 1903/3000, Loss: 1180.7200113932292\n",
      "Epoch 1904/3000, Loss: 1168.0890502929688\n",
      "Epoch 1905/3000, Loss: 1135.8529663085938\n",
      "Epoch 1906/3000, Loss: 1202.154276529948\n",
      "Epoch 1907/3000, Loss: 1144.1751098632812\n",
      "Epoch 1908/3000, Loss: 1225.6049194335938\n",
      "Epoch 1909/3000, Loss: 1230.9127197265625\n",
      "Epoch 1910/3000, Loss: 1301.2623087565105\n",
      "Epoch 1911/3000, Loss: 1168.5394694010417\n",
      "Epoch 1912/3000, Loss: 1213.6514485677083\n",
      "Epoch 1913/3000, Loss: 1323.5708618164062\n",
      "Epoch 1914/3000, Loss: 1175.6858622233074\n",
      "Epoch 1915/3000, Loss: 1221.3268432617188\n",
      "Epoch 1916/3000, Loss: 1149.8437093098958\n",
      "Epoch 1917/3000, Loss: 1158.3748575846355\n",
      "Epoch 1918/3000, Loss: 1192.386494954427\n",
      "Epoch 1919/3000, Loss: 1153.9027709960938\n",
      "Epoch 1920/3000, Loss: 1218.7083536783855\n",
      "Epoch 1921/3000, Loss: 1253.1448364257812\n",
      "Epoch 1922/3000, Loss: 1204.4865112304688\n",
      "Epoch 1923/3000, Loss: 1242.7490132649739\n",
      "Epoch 1924/3000, Loss: 1126.8267517089844\n",
      "Epoch 1925/3000, Loss: 1123.2038269042969\n",
      "Epoch 1926/3000, Loss: 1157.2969665527344\n",
      "Epoch 1927/3000, Loss: 1233.1737670898438\n",
      "Epoch 1928/3000, Loss: 1114.6702779134114\n",
      "Epoch 1929/3000, Loss: 1197.2725423177083\n",
      "Epoch 1930/3000, Loss: 1235.3623046875\n",
      "Epoch 1931/3000, Loss: 1169.0046997070312\n",
      "Epoch 1932/3000, Loss: 1111.3407287597656\n",
      "Epoch 1933/3000, Loss: 1186.7300618489583\n",
      "Epoch 1934/3000, Loss: 1142.4607340494792\n",
      "Epoch 1935/3000, Loss: 1175.5911254882812\n",
      "Epoch 1936/3000, Loss: 1234.4353434244792\n",
      "Epoch 1937/3000, Loss: 1191.5846761067708\n",
      "Epoch 1938/3000, Loss: 1260.2338460286458\n",
      "Epoch 1939/3000, Loss: 1254.0662638346355\n",
      "Epoch 1940/3000, Loss: 1286.36767578125\n",
      "Epoch 1941/3000, Loss: 1135.7995910644531\n",
      "Epoch 1942/3000, Loss: 1241.8988952636719\n",
      "Epoch 1943/3000, Loss: 1232.1175944010417\n",
      "Epoch 1944/3000, Loss: 1170.678690592448\n",
      "Epoch 1945/3000, Loss: 1134.5486450195312\n",
      "Epoch 1946/3000, Loss: 1144.0850219726562\n",
      "Epoch 1947/3000, Loss: 1131.437744140625\n",
      "Epoch 1948/3000, Loss: 1223.6543579101562\n",
      "Epoch 1949/3000, Loss: 1165.3736165364583\n",
      "Epoch 1950/3000, Loss: 1182.9938557942708\n",
      "Epoch 1951/3000, Loss: 1122.0094604492188\n",
      "Epoch 1952/3000, Loss: 1168.4949340820312\n",
      "Epoch 1953/3000, Loss: 1187.6077168782551\n",
      "Epoch 1954/3000, Loss: 1188.7910563151042\n",
      "Epoch 1955/3000, Loss: 1268.1453653971355\n",
      "Epoch 1956/3000, Loss: 1129.906494140625\n",
      "Epoch 1957/3000, Loss: 1148.9116516113281\n",
      "Epoch 1958/3000, Loss: 1173.3605346679688\n",
      "Epoch 1959/3000, Loss: 1227.8044026692708\n",
      "Epoch 1960/3000, Loss: 1174.1165364583333\n",
      "Epoch 1961/3000, Loss: 1170.8907572428386\n",
      "Epoch 1962/3000, Loss: 1173.0057373046875\n",
      "Epoch 1963/3000, Loss: 1140.6543070475261\n",
      "Epoch 1964/3000, Loss: 1115.6972554524739\n",
      "Epoch 1965/3000, Loss: 1219.6146240234375\n",
      "Epoch 1966/3000, Loss: 1219.825419108073\n",
      "Epoch 1967/3000, Loss: 1247.1729329427083\n",
      "Epoch 1968/3000, Loss: 1229.0054931640625\n",
      "Epoch 1969/3000, Loss: 1196.8073120117188\n",
      "Epoch 1970/3000, Loss: 1127.108174641927\n",
      "Epoch 1971/3000, Loss: 1227.6575520833333\n",
      "Epoch 1972/3000, Loss: 1126.7466125488281\n",
      "Epoch 1973/3000, Loss: 1193.9422098795574\n",
      "Epoch 1974/3000, Loss: 1200.7222595214844\n",
      "Epoch 1975/3000, Loss: 1188.5584106445312\n",
      "Epoch 1976/3000, Loss: 1220.7971801757812\n",
      "Epoch 1977/3000, Loss: 1178.4769897460938\n",
      "Epoch 1978/3000, Loss: 1176.8744506835938\n",
      "Epoch 1979/3000, Loss: 1192.8953653971355\n",
      "Epoch 1980/3000, Loss: 1151.853251139323\n",
      "Epoch 1981/3000, Loss: 1184.1233011881511\n",
      "Epoch 1982/3000, Loss: 1216.3500569661458\n",
      "Epoch 1983/3000, Loss: 1171.6227722167969\n",
      "Epoch 1984/3000, Loss: 1216.4108072916667\n",
      "Epoch 1985/3000, Loss: 1170.082051595052\n",
      "Epoch 1986/3000, Loss: 1200.7704060872395\n",
      "Epoch 1987/3000, Loss: 1303.0934448242188\n",
      "Epoch 1988/3000, Loss: 1214.536844889323\n",
      "Epoch 1989/3000, Loss: 1236.6807454427083\n",
      "Epoch 1990/3000, Loss: 1204.8303120930989\n",
      "Epoch 1991/3000, Loss: 1261.983866373698\n",
      "Epoch 1992/3000, Loss: 1166.8209228515625\n",
      "Epoch 1993/3000, Loss: 1185.3043619791667\n",
      "Epoch 1994/3000, Loss: 1212.5515543619792\n",
      "Epoch 1995/3000, Loss: 1219.96044921875\n",
      "Epoch 1996/3000, Loss: 1156.805155436198\n",
      "Epoch 1997/3000, Loss: 1095.5960998535156\n",
      "Epoch 1998/3000, Loss: 1206.7120157877605\n",
      "Epoch 1999/3000, Loss: 1176.8977864583333\n",
      "Epoch 2000/3000, Loss: 1152.7747294108074\n",
      "Epoch 2001/3000, Loss: 1221.8761596679688\n",
      "Epoch 2002/3000, Loss: 1188.5640157063801\n",
      "Epoch 2003/3000, Loss: 1160.3848673502605\n",
      "Epoch 2004/3000, Loss: 1211.6130676269531\n",
      "Epoch 2005/3000, Loss: 1204.139892578125\n",
      "Epoch 2006/3000, Loss: 1181.3855387369792\n",
      "Epoch 2007/3000, Loss: 1193.7197774251301\n",
      "Epoch 2008/3000, Loss: 1083.5455220540364\n",
      "Epoch 2009/3000, Loss: 1094.5334167480469\n",
      "Epoch 2010/3000, Loss: 1182.9000651041667\n",
      "Epoch 2011/3000, Loss: 1093.1509399414062\n",
      "Epoch 2012/3000, Loss: 1161.843037923177\n",
      "Epoch 2013/3000, Loss: 1153.8477783203125\n",
      "Epoch 2014/3000, Loss: 1174.3839619954426\n",
      "Epoch 2015/3000, Loss: 1166.1226704915364\n",
      "Epoch 2016/3000, Loss: 1178.036376953125\n",
      "Epoch 2017/3000, Loss: 1217.6362711588542\n",
      "Epoch 2018/3000, Loss: 1130.0232950846355\n",
      "Epoch 2019/3000, Loss: 1300.7560221354167\n",
      "Epoch 2020/3000, Loss: 1163.2671508789062\n",
      "Epoch 2021/3000, Loss: 1199.7860209147136\n",
      "Epoch 2022/3000, Loss: 1172.8044128417969\n",
      "Epoch 2023/3000, Loss: 1190.089335123698\n",
      "Epoch 2024/3000, Loss: 1232.586893717448\n",
      "Epoch 2025/3000, Loss: 1202.0101725260417\n",
      "Epoch 2026/3000, Loss: 1165.8584798177083\n",
      "Epoch 2027/3000, Loss: 1139.6368916829426\n",
      "Epoch 2028/3000, Loss: 1212.4939575195312\n",
      "Epoch 2029/3000, Loss: 1159.2962036132812\n",
      "Epoch 2030/3000, Loss: 1173.4786173502605\n",
      "Epoch 2031/3000, Loss: 1224.4261881510417\n",
      "Epoch 2032/3000, Loss: 1173.3011271158855\n",
      "Epoch 2033/3000, Loss: 1262.0476888020833\n",
      "Epoch 2034/3000, Loss: 1169.0091756184895\n",
      "Epoch 2035/3000, Loss: 1209.6505533854167\n",
      "Epoch 2036/3000, Loss: 1210.2266845703125\n",
      "Epoch 2037/3000, Loss: 1104.1885986328125\n",
      "Epoch 2038/3000, Loss: 1145.5595703125\n",
      "Epoch 2039/3000, Loss: 1262.3084309895833\n",
      "Epoch 2040/3000, Loss: 1215.2704264322917\n",
      "Epoch 2041/3000, Loss: 1141.7462463378906\n",
      "Epoch 2042/3000, Loss: 1121.9224344889324\n",
      "Epoch 2043/3000, Loss: 1273.6917724609375\n",
      "Epoch 2044/3000, Loss: 1184.432596842448\n",
      "Epoch 2045/3000, Loss: 1160.5599670410156\n",
      "Epoch 2046/3000, Loss: 1245.1371663411458\n",
      "Epoch 2047/3000, Loss: 1080.5255228678386\n",
      "Epoch 2048/3000, Loss: 1192.787821451823\n",
      "Epoch 2049/3000, Loss: 1165.493876139323\n",
      "Epoch 2050/3000, Loss: 1160.2779337565105\n",
      "Epoch 2051/3000, Loss: 1161.06005859375\n",
      "Epoch 2052/3000, Loss: 1266.9906005859375\n",
      "Epoch 2053/3000, Loss: 1132.8558959960938\n",
      "Epoch 2054/3000, Loss: 1167.4784240722656\n",
      "Epoch 2055/3000, Loss: 1222.5920817057292\n",
      "Epoch 2056/3000, Loss: 1182.7481892903645\n",
      "Epoch 2057/3000, Loss: 1180.9930826822917\n",
      "Epoch 2058/3000, Loss: 1191.893575032552\n",
      "Epoch 2059/3000, Loss: 1208.5906677246094\n",
      "Epoch 2060/3000, Loss: 1129.6946411132812\n",
      "Epoch 2061/3000, Loss: 1064.4662068684895\n",
      "Epoch 2062/3000, Loss: 1217.3067220052083\n",
      "Epoch 2063/3000, Loss: 1224.2978617350261\n",
      "Epoch 2064/3000, Loss: 1206.107421875\n",
      "Epoch 2065/3000, Loss: 1217.258544921875\n",
      "Epoch 2066/3000, Loss: 1172.7312622070312\n",
      "Epoch 2067/3000, Loss: 1263.5185953776042\n",
      "Epoch 2068/3000, Loss: 1259.8928629557292\n",
      "Epoch 2069/3000, Loss: 1157.1592203776042\n",
      "Epoch 2070/3000, Loss: 1132.7635396321614\n",
      "Epoch 2071/3000, Loss: 1118.6353759765625\n",
      "Epoch 2072/3000, Loss: 1159.7210388183594\n",
      "Epoch 2073/3000, Loss: 1180.4207560221355\n",
      "Epoch 2074/3000, Loss: 1297.7431233723958\n",
      "Epoch 2075/3000, Loss: 1155.1818237304688\n",
      "Epoch 2076/3000, Loss: 1150.9862060546875\n",
      "Epoch 2077/3000, Loss: 1191.8700561523438\n",
      "Epoch 2078/3000, Loss: 1283.3076985677083\n",
      "Epoch 2079/3000, Loss: 1186.1552734375\n",
      "Epoch 2080/3000, Loss: 1236.342753092448\n",
      "Epoch 2081/3000, Loss: 1246.036885579427\n",
      "Epoch 2082/3000, Loss: 1166.4717203776042\n",
      "Epoch 2083/3000, Loss: 1094.7451578776042\n",
      "Epoch 2084/3000, Loss: 1163.4241739908855\n",
      "Epoch 2085/3000, Loss: 1162.2342122395833\n",
      "Epoch 2086/3000, Loss: 1226.2188313802083\n",
      "Epoch 2087/3000, Loss: 1153.7041829427083\n",
      "Epoch 2088/3000, Loss: 1256.9478759765625\n",
      "Epoch 2089/3000, Loss: 1180.7372029622395\n",
      "Epoch 2090/3000, Loss: 1234.2437337239583\n",
      "Epoch 2091/3000, Loss: 1148.6239827473958\n",
      "Epoch 2092/3000, Loss: 1107.2627258300781\n",
      "Epoch 2093/3000, Loss: 1225.6676025390625\n",
      "Epoch 2094/3000, Loss: 1191.999532063802\n",
      "Epoch 2095/3000, Loss: 1230.2997945149739\n",
      "Epoch 2096/3000, Loss: 1201.572001139323\n",
      "Epoch 2097/3000, Loss: 1150.4918619791667\n",
      "Epoch 2098/3000, Loss: 1090.695780436198\n",
      "Epoch 2099/3000, Loss: 1127.9056701660156\n",
      "Epoch 2100/3000, Loss: 1174.6806233723958\n",
      "Epoch 2101/3000, Loss: 1120.5496724446614\n",
      "Epoch 2102/3000, Loss: 1136.6769409179688\n",
      "Epoch 2103/3000, Loss: 1162.6827392578125\n",
      "Epoch 2104/3000, Loss: 1119.826395670573\n",
      "Epoch 2105/3000, Loss: 1139.8168640136719\n",
      "Epoch 2106/3000, Loss: 1135.6885477701824\n",
      "Epoch 2107/3000, Loss: 1132.3467102050781\n",
      "Epoch 2108/3000, Loss: 1157.4132080078125\n",
      "Epoch 2109/3000, Loss: 1213.757344563802\n",
      "Epoch 2110/3000, Loss: 1142.6283365885417\n",
      "Epoch 2111/3000, Loss: 1167.842529296875\n",
      "Epoch 2112/3000, Loss: 1190.6207478841145\n",
      "Epoch 2113/3000, Loss: 1170.5807088216145\n",
      "Epoch 2114/3000, Loss: 1283.0950113932292\n",
      "Epoch 2115/3000, Loss: 1221.7132975260417\n",
      "Epoch 2116/3000, Loss: 1133.8125610351562\n",
      "Epoch 2117/3000, Loss: 1184.2324625651042\n",
      "Epoch 2118/3000, Loss: 1162.220438639323\n",
      "Epoch 2119/3000, Loss: 1159.86572265625\n",
      "Epoch 2120/3000, Loss: 1194.3774108886719\n",
      "Epoch 2121/3000, Loss: 1157.0991821289062\n",
      "Epoch 2122/3000, Loss: 1162.9556986490886\n",
      "Epoch 2123/3000, Loss: 1141.2296346028645\n",
      "Epoch 2124/3000, Loss: 1185.9790649414062\n",
      "Epoch 2125/3000, Loss: 1217.6097615559895\n",
      "Epoch 2126/3000, Loss: 1141.7539571126301\n",
      "Epoch 2127/3000, Loss: 1184.918721516927\n",
      "Epoch 2128/3000, Loss: 1229.9306233723958\n",
      "Epoch 2129/3000, Loss: 1147.2306111653645\n",
      "Epoch 2130/3000, Loss: 1135.3121846516926\n",
      "Epoch 2131/3000, Loss: 1231.4573669433594\n",
      "Epoch 2132/3000, Loss: 1197.354248046875\n",
      "Epoch 2133/3000, Loss: 1182.3987223307292\n",
      "Epoch 2134/3000, Loss: 1169.445576985677\n",
      "Epoch 2135/3000, Loss: 1178.6449381510417\n",
      "Epoch 2136/3000, Loss: 1222.2721659342449\n",
      "Epoch 2137/3000, Loss: 1155.010498046875\n",
      "Epoch 2138/3000, Loss: 1173.449951171875\n",
      "Epoch 2139/3000, Loss: 1210.6682535807292\n",
      "Epoch 2140/3000, Loss: 1157.9872029622395\n",
      "Epoch 2141/3000, Loss: 1089.9284566243489\n",
      "Epoch 2142/3000, Loss: 1188.365010579427\n",
      "Epoch 2143/3000, Loss: 1214.0706787109375\n",
      "Epoch 2144/3000, Loss: 1187.6496480305989\n",
      "Epoch 2145/3000, Loss: 1220.6409098307292\n",
      "Epoch 2146/3000, Loss: 1092.8888854980469\n",
      "Epoch 2147/3000, Loss: 1218.201416015625\n",
      "Epoch 2148/3000, Loss: 1218.3907775878906\n",
      "Epoch 2149/3000, Loss: 1073.929911295573\n",
      "Epoch 2150/3000, Loss: 1081.5504964192708\n",
      "Epoch 2151/3000, Loss: 1208.870869954427\n",
      "Epoch 2152/3000, Loss: 1108.528096516927\n",
      "Epoch 2153/3000, Loss: 1209.5750325520833\n",
      "Epoch 2154/3000, Loss: 1135.0341898600261\n",
      "Epoch 2155/3000, Loss: 1207.1278686523438\n",
      "Epoch 2156/3000, Loss: 1161.6824544270833\n",
      "Epoch 2157/3000, Loss: 1216.4115193684895\n",
      "Epoch 2158/3000, Loss: 1130.064208984375\n",
      "Epoch 2159/3000, Loss: 1259.7759602864583\n",
      "Epoch 2160/3000, Loss: 1193.6459350585938\n",
      "Epoch 2161/3000, Loss: 1116.1265055338542\n",
      "Epoch 2162/3000, Loss: 1223.5730590820312\n",
      "Epoch 2163/3000, Loss: 1199.1778869628906\n",
      "Epoch 2164/3000, Loss: 1201.6956990559895\n",
      "Epoch 2165/3000, Loss: 1210.2646484375\n",
      "Epoch 2166/3000, Loss: 1215.9528198242188\n",
      "Epoch 2167/3000, Loss: 1113.1993001302083\n",
      "Epoch 2168/3000, Loss: 1181.8266194661458\n",
      "Epoch 2169/3000, Loss: 1186.2636820475261\n",
      "Epoch 2170/3000, Loss: 1172.9952596028645\n",
      "Epoch 2171/3000, Loss: 1240.203633626302\n",
      "Epoch 2172/3000, Loss: 1219.9440612792969\n",
      "Epoch 2173/3000, Loss: 1239.1383666992188\n",
      "Epoch 2174/3000, Loss: 1086.8759155273438\n",
      "Epoch 2175/3000, Loss: 1211.8136393229167\n",
      "Epoch 2176/3000, Loss: 1149.6664021809895\n",
      "Epoch 2177/3000, Loss: 1060.4067281087239\n",
      "Epoch 2178/3000, Loss: 1139.3151550292969\n",
      "Epoch 2179/3000, Loss: 1116.1651814778645\n",
      "Epoch 2180/3000, Loss: 1091.8416442871094\n",
      "Epoch 2181/3000, Loss: 1173.6864318847656\n",
      "Epoch 2182/3000, Loss: 1208.4334004720051\n",
      "Epoch 2183/3000, Loss: 1204.6277974446614\n",
      "Epoch 2184/3000, Loss: 1109.5582071940105\n",
      "Epoch 2185/3000, Loss: 1120.3038533528645\n",
      "Epoch 2186/3000, Loss: 1113.1108296712239\n",
      "Epoch 2187/3000, Loss: 1086.6238199869792\n",
      "Epoch 2188/3000, Loss: 1173.8185221354167\n",
      "Epoch 2189/3000, Loss: 1158.3328552246094\n",
      "Epoch 2190/3000, Loss: 1185.416727701823\n",
      "Epoch 2191/3000, Loss: 1160.8252766927083\n",
      "Epoch 2192/3000, Loss: 1136.3391418457031\n",
      "Epoch 2193/3000, Loss: 1064.0032857259114\n",
      "Epoch 2194/3000, Loss: 1228.6512858072917\n",
      "Epoch 2195/3000, Loss: 1237.6253458658855\n",
      "Epoch 2196/3000, Loss: 1199.5574951171875\n",
      "Epoch 2197/3000, Loss: 1167.6070353190105\n",
      "Epoch 2198/3000, Loss: 1124.1122029622395\n",
      "Epoch 2199/3000, Loss: 1132.7823588053386\n",
      "Epoch 2200/3000, Loss: 1145.3209431966145\n",
      "Epoch 2201/3000, Loss: 1191.6590067545574\n",
      "Epoch 2202/3000, Loss: 1125.5194193522136\n",
      "Epoch 2203/3000, Loss: 1215.1955159505208\n",
      "Epoch 2204/3000, Loss: 1216.713846842448\n",
      "Epoch 2205/3000, Loss: 1209.7363382975261\n",
      "Epoch 2206/3000, Loss: 1209.4228108723958\n",
      "Epoch 2207/3000, Loss: 1231.0684407552083\n",
      "Epoch 2208/3000, Loss: 1227.47265625\n",
      "Epoch 2209/3000, Loss: 1115.4523824055989\n",
      "Epoch 2210/3000, Loss: 1169.543212890625\n",
      "Epoch 2211/3000, Loss: 1188.792236328125\n",
      "Epoch 2212/3000, Loss: 1134.5428059895833\n",
      "Epoch 2213/3000, Loss: 1207.2218017578125\n",
      "Epoch 2214/3000, Loss: 1155.6422932942708\n",
      "Epoch 2215/3000, Loss: 1161.6581726074219\n",
      "Epoch 2216/3000, Loss: 1114.2981669108074\n",
      "Epoch 2217/3000, Loss: 1142.7147521972656\n",
      "Epoch 2218/3000, Loss: 1124.7471516927083\n",
      "Epoch 2219/3000, Loss: 1176.0352986653645\n",
      "Epoch 2220/3000, Loss: 1172.0470072428386\n",
      "Epoch 2221/3000, Loss: 1101.1519165039062\n",
      "Epoch 2222/3000, Loss: 1209.8833414713542\n",
      "Epoch 2223/3000, Loss: 1176.069315592448\n",
      "Epoch 2224/3000, Loss: 1224.0161539713542\n",
      "Epoch 2225/3000, Loss: 1194.9040120442708\n",
      "Epoch 2226/3000, Loss: 1117.5216369628906\n",
      "Epoch 2227/3000, Loss: 1208.0693969726562\n",
      "Epoch 2228/3000, Loss: 1162.3348795572917\n",
      "Epoch 2229/3000, Loss: 1205.8773803710938\n",
      "Epoch 2230/3000, Loss: 1178.8458150227864\n",
      "Epoch 2231/3000, Loss: 1163.1722005208333\n",
      "Epoch 2232/3000, Loss: 1073.6274922688801\n",
      "Epoch 2233/3000, Loss: 1148.2320149739583\n",
      "Epoch 2234/3000, Loss: 1219.661600748698\n",
      "Epoch 2235/3000, Loss: 1219.1805419921875\n",
      "Epoch 2236/3000, Loss: 1146.2798563639324\n",
      "Epoch 2237/3000, Loss: 1120.7838439941406\n",
      "Epoch 2238/3000, Loss: 1168.5393168131511\n",
      "Epoch 2239/3000, Loss: 1194.9884440104167\n",
      "Epoch 2240/3000, Loss: 1137.9560445149739\n",
      "Epoch 2241/3000, Loss: 1199.9155883789062\n",
      "Epoch 2242/3000, Loss: 1179.001485188802\n",
      "Epoch 2243/3000, Loss: 1128.4796854654949\n",
      "Epoch 2244/3000, Loss: 1139.2796020507812\n",
      "Epoch 2245/3000, Loss: 1158.9441833496094\n",
      "Epoch 2246/3000, Loss: 1159.1808980305989\n",
      "Epoch 2247/3000, Loss: 1101.1565348307292\n",
      "Epoch 2248/3000, Loss: 1184.5115661621094\n",
      "Epoch 2249/3000, Loss: 1199.8755696614583\n",
      "Epoch 2250/3000, Loss: 1149.5812886555989\n",
      "Epoch 2251/3000, Loss: 1190.8836975097656\n",
      "Epoch 2252/3000, Loss: 1166.4956461588542\n",
      "Epoch 2253/3000, Loss: 1203.3740641276042\n",
      "Epoch 2254/3000, Loss: 1188.0016682942708\n",
      "Epoch 2255/3000, Loss: 1108.0410563151042\n",
      "Epoch 2256/3000, Loss: 1170.5139973958333\n",
      "Epoch 2257/3000, Loss: 1194.326883951823\n",
      "Epoch 2258/3000, Loss: 1120.7197672526042\n",
      "Epoch 2259/3000, Loss: 1155.540018717448\n",
      "Epoch 2260/3000, Loss: 1295.3488159179688\n",
      "Epoch 2261/3000, Loss: 1130.287862141927\n",
      "Epoch 2262/3000, Loss: 1150.5103556315105\n",
      "Epoch 2263/3000, Loss: 1152.6227620442708\n",
      "Epoch 2264/3000, Loss: 1149.425557454427\n",
      "Epoch 2265/3000, Loss: 1164.62841796875\n",
      "Epoch 2266/3000, Loss: 1235.8934936523438\n",
      "Epoch 2267/3000, Loss: 1130.4507649739583\n",
      "Epoch 2268/3000, Loss: 1130.2505696614583\n",
      "Epoch 2269/3000, Loss: 1118.3945007324219\n",
      "Epoch 2270/3000, Loss: 1186.965576171875\n",
      "Epoch 2271/3000, Loss: 1062.6612040201824\n",
      "Epoch 2272/3000, Loss: 1119.9447428385417\n",
      "Epoch 2273/3000, Loss: 1179.8502298990886\n",
      "Epoch 2274/3000, Loss: 1140.6899108886719\n",
      "Epoch 2275/3000, Loss: 1078.4052124023438\n",
      "Epoch 2276/3000, Loss: 1069.9763997395833\n",
      "Epoch 2277/3000, Loss: 1151.0553385416667\n",
      "Epoch 2278/3000, Loss: 1267.6775309244792\n",
      "Epoch 2279/3000, Loss: 1157.7728271484375\n",
      "Epoch 2280/3000, Loss: 1180.0829264322917\n",
      "Epoch 2281/3000, Loss: 1096.6501871744792\n",
      "Epoch 2282/3000, Loss: 1146.8651428222656\n",
      "Epoch 2283/3000, Loss: 1097.6705322265625\n",
      "Epoch 2284/3000, Loss: 1134.570576985677\n",
      "Epoch 2285/3000, Loss: 1206.0540262858074\n",
      "Epoch 2286/3000, Loss: 1185.6468709309895\n",
      "Epoch 2287/3000, Loss: 1104.5099589029949\n",
      "Epoch 2288/3000, Loss: 1182.0880432128906\n",
      "Epoch 2289/3000, Loss: 1172.4656778971355\n",
      "Epoch 2290/3000, Loss: 1090.0759989420574\n",
      "Epoch 2291/3000, Loss: 1143.9749348958333\n",
      "Epoch 2292/3000, Loss: 1175.8734741210938\n",
      "Epoch 2293/3000, Loss: 1263.8651529947917\n",
      "Epoch 2294/3000, Loss: 1147.6523742675781\n",
      "Epoch 2295/3000, Loss: 1161.1348673502605\n",
      "Epoch 2296/3000, Loss: 1153.773417154948\n",
      "Epoch 2297/3000, Loss: 1196.0586547851562\n",
      "Epoch 2298/3000, Loss: 1162.8269856770833\n",
      "Epoch 2299/3000, Loss: 1163.2902018229167\n",
      "Epoch 2300/3000, Loss: 1150.1307067871094\n",
      "Epoch 2301/3000, Loss: 1196.6424967447917\n",
      "Epoch 2302/3000, Loss: 1292.7259114583333\n",
      "Epoch 2303/3000, Loss: 1145.1980997721355\n",
      "Epoch 2304/3000, Loss: 1186.5804850260417\n",
      "Epoch 2305/3000, Loss: 1213.2480061848958\n",
      "Epoch 2306/3000, Loss: 1116.333984375\n",
      "Epoch 2307/3000, Loss: 1124.076436360677\n",
      "Epoch 2308/3000, Loss: 1168.444356282552\n",
      "Epoch 2309/3000, Loss: 1161.0249633789062\n",
      "Epoch 2310/3000, Loss: 1160.1717936197917\n",
      "Epoch 2311/3000, Loss: 1044.1817728678386\n",
      "Epoch 2312/3000, Loss: 1147.6789652506511\n",
      "Epoch 2313/3000, Loss: 1121.7772420247395\n",
      "Epoch 2314/3000, Loss: 1149.390401204427\n",
      "Epoch 2315/3000, Loss: 1185.9630635579426\n",
      "Epoch 2316/3000, Loss: 1135.7824300130208\n",
      "Epoch 2317/3000, Loss: 1123.3539733886719\n",
      "Epoch 2318/3000, Loss: 1200.2447509765625\n",
      "Epoch 2319/3000, Loss: 1138.0477701822917\n",
      "Epoch 2320/3000, Loss: 1131.0177917480469\n",
      "Epoch 2321/3000, Loss: 1193.817362467448\n",
      "Epoch 2322/3000, Loss: 1181.4825642903645\n",
      "Epoch 2323/3000, Loss: 1106.5272623697917\n",
      "Epoch 2324/3000, Loss: 1036.3602701822917\n",
      "Epoch 2325/3000, Loss: 1059.6070556640625\n",
      "Epoch 2326/3000, Loss: 1200.5501505533855\n",
      "Epoch 2327/3000, Loss: 1116.4403076171875\n",
      "Epoch 2328/3000, Loss: 1135.0461018880208\n",
      "Epoch 2329/3000, Loss: 1156.4376525878906\n",
      "Epoch 2330/3000, Loss: 1139.0027465820312\n",
      "Epoch 2331/3000, Loss: 1162.9747314453125\n",
      "Epoch 2332/3000, Loss: 1161.0500895182292\n",
      "Epoch 2333/3000, Loss: 1174.5450236002605\n",
      "Epoch 2334/3000, Loss: 1103.4624125162761\n",
      "Epoch 2335/3000, Loss: 1192.2339986165364\n",
      "Epoch 2336/3000, Loss: 1149.2553405761719\n",
      "Epoch 2337/3000, Loss: 1103.9105021158855\n",
      "Epoch 2338/3000, Loss: 1231.4542236328125\n",
      "Epoch 2339/3000, Loss: 1218.8369547526042\n",
      "Epoch 2340/3000, Loss: 1189.2150065104167\n",
      "Epoch 2341/3000, Loss: 1168.8993021647136\n",
      "Epoch 2342/3000, Loss: 1208.3510335286458\n",
      "Epoch 2343/3000, Loss: 1160.5673014322917\n",
      "Epoch 2344/3000, Loss: 1212.4507649739583\n",
      "Epoch 2345/3000, Loss: 1165.4470113118489\n",
      "Epoch 2346/3000, Loss: 1165.5858154296875\n",
      "Epoch 2347/3000, Loss: 1083.2232971191406\n",
      "Epoch 2348/3000, Loss: 1147.8843383789062\n",
      "Epoch 2349/3000, Loss: 1104.7970784505208\n",
      "Epoch 2350/3000, Loss: 1160.6050211588542\n",
      "Epoch 2351/3000, Loss: 1184.7983907063801\n",
      "Epoch 2352/3000, Loss: 1080.836669921875\n",
      "Epoch 2353/3000, Loss: 1146.5102945963542\n",
      "Epoch 2354/3000, Loss: 1157.3495279947917\n",
      "Epoch 2355/3000, Loss: 1228.1464436848958\n",
      "Epoch 2356/3000, Loss: 1147.5800374348958\n",
      "Epoch 2357/3000, Loss: 1161.3087768554688\n",
      "Epoch 2358/3000, Loss: 1163.2515767415364\n",
      "Epoch 2359/3000, Loss: 1120.0570576985676\n",
      "Epoch 2360/3000, Loss: 1122.7988688151042\n",
      "Epoch 2361/3000, Loss: 1202.6139119466145\n",
      "Epoch 2362/3000, Loss: 1160.281982421875\n",
      "Epoch 2363/3000, Loss: 1146.6043802897136\n",
      "Epoch 2364/3000, Loss: 1242.0534769694011\n",
      "Epoch 2365/3000, Loss: 1127.8863423665364\n",
      "Epoch 2366/3000, Loss: 1188.5992228190105\n",
      "Epoch 2367/3000, Loss: 1224.9361368815105\n",
      "Epoch 2368/3000, Loss: 1132.7825113932292\n",
      "Epoch 2369/3000, Loss: 1076.1512145996094\n",
      "Epoch 2370/3000, Loss: 1193.93408203125\n",
      "Epoch 2371/3000, Loss: 1171.1175537109375\n",
      "Epoch 2372/3000, Loss: 1159.853780110677\n",
      "Epoch 2373/3000, Loss: 1130.0661214192708\n",
      "Epoch 2374/3000, Loss: 1142.0039978027344\n",
      "Epoch 2375/3000, Loss: 1183.6710205078125\n",
      "Epoch 2376/3000, Loss: 1204.5205688476562\n",
      "Epoch 2377/3000, Loss: 1231.040995279948\n",
      "Epoch 2378/3000, Loss: 1125.8833923339844\n",
      "Epoch 2379/3000, Loss: 1056.6549479166667\n",
      "Epoch 2380/3000, Loss: 1155.2102355957031\n",
      "Epoch 2381/3000, Loss: 1171.1524963378906\n",
      "Epoch 2382/3000, Loss: 1118.9383544921875\n",
      "Epoch 2383/3000, Loss: 1196.5780843098958\n",
      "Epoch 2384/3000, Loss: 1147.8219197591145\n",
      "Epoch 2385/3000, Loss: 1066.3465372721355\n",
      "Epoch 2386/3000, Loss: 1241.8616129557292\n",
      "Epoch 2387/3000, Loss: 1245.0836385091145\n",
      "Epoch 2388/3000, Loss: 1165.3449300130208\n",
      "Epoch 2389/3000, Loss: 1121.8037719726562\n",
      "Epoch 2390/3000, Loss: 1177.1976114908855\n",
      "Epoch 2391/3000, Loss: 1151.211181640625\n",
      "Epoch 2392/3000, Loss: 1178.6595052083333\n",
      "Epoch 2393/3000, Loss: 1127.6931966145833\n",
      "Epoch 2394/3000, Loss: 1097.5897827148438\n",
      "Epoch 2395/3000, Loss: 1188.8573811848958\n",
      "Epoch 2396/3000, Loss: 1166.250244140625\n",
      "Epoch 2397/3000, Loss: 1060.5833435058594\n",
      "Epoch 2398/3000, Loss: 1187.1605733235676\n",
      "Epoch 2399/3000, Loss: 1149.7168375651042\n",
      "Epoch 2400/3000, Loss: 1092.56689453125\n",
      "Epoch 2401/3000, Loss: 1092.6339721679688\n",
      "Epoch 2402/3000, Loss: 1116.0589396158855\n",
      "Epoch 2403/3000, Loss: 1063.4117533365886\n",
      "Epoch 2404/3000, Loss: 1175.6158447265625\n",
      "Epoch 2405/3000, Loss: 1169.2766418457031\n",
      "Epoch 2406/3000, Loss: 1161.1962076822917\n",
      "Epoch 2407/3000, Loss: 1162.9039916992188\n",
      "Epoch 2408/3000, Loss: 1130.260498046875\n",
      "Epoch 2409/3000, Loss: 1014.9501546223959\n",
      "Epoch 2410/3000, Loss: 1130.7604370117188\n",
      "Epoch 2411/3000, Loss: 1167.4827982584636\n",
      "Epoch 2412/3000, Loss: 1074.604960123698\n",
      "Epoch 2413/3000, Loss: 1072.7537536621094\n",
      "Epoch 2414/3000, Loss: 1124.185282389323\n",
      "Epoch 2415/3000, Loss: 1092.3393961588542\n",
      "Epoch 2416/3000, Loss: 1090.306864420573\n",
      "Epoch 2417/3000, Loss: 1133.4481811523438\n",
      "Epoch 2418/3000, Loss: 1089.7297058105469\n",
      "Epoch 2419/3000, Loss: 1193.4159545898438\n",
      "Epoch 2420/3000, Loss: 1086.4724222819011\n",
      "Epoch 2421/3000, Loss: 1128.2108968098958\n",
      "Epoch 2422/3000, Loss: 1162.3655598958333\n",
      "Epoch 2423/3000, Loss: 1093.3644409179688\n",
      "Epoch 2424/3000, Loss: 1110.2236836751301\n",
      "Epoch 2425/3000, Loss: 1064.8508199055989\n",
      "Epoch 2426/3000, Loss: 1227.538330078125\n",
      "Epoch 2427/3000, Loss: 1146.5890706380208\n",
      "Epoch 2428/3000, Loss: 1190.1159261067708\n",
      "Epoch 2429/3000, Loss: 1164.7003580729167\n",
      "Epoch 2430/3000, Loss: 1234.5407104492188\n",
      "Epoch 2431/3000, Loss: 1080.3367309570312\n",
      "Epoch 2432/3000, Loss: 1167.7088114420574\n",
      "Epoch 2433/3000, Loss: 1177.838643391927\n",
      "Epoch 2434/3000, Loss: 1155.9876607259114\n",
      "Epoch 2435/3000, Loss: 1220.8994547526042\n",
      "Epoch 2436/3000, Loss: 1126.0021362304688\n",
      "Epoch 2437/3000, Loss: 1141.0823160807292\n",
      "Epoch 2438/3000, Loss: 1111.6506245930989\n",
      "Epoch 2439/3000, Loss: 1109.0803324381511\n",
      "Epoch 2440/3000, Loss: 1176.3585001627605\n",
      "Epoch 2441/3000, Loss: 1150.623067220052\n",
      "Epoch 2442/3000, Loss: 1128.0307108561199\n",
      "Epoch 2443/3000, Loss: 1108.6358439127605\n",
      "Epoch 2444/3000, Loss: 1118.0015055338542\n",
      "Epoch 2445/3000, Loss: 1127.5568237304688\n",
      "Epoch 2446/3000, Loss: 1061.6539713541667\n",
      "Epoch 2447/3000, Loss: 1125.171875\n",
      "Epoch 2448/3000, Loss: 1101.8184407552083\n",
      "Epoch 2449/3000, Loss: 1115.110819498698\n",
      "Epoch 2450/3000, Loss: 1176.5012613932292\n",
      "Epoch 2451/3000, Loss: 1105.6000162760417\n",
      "Epoch 2452/3000, Loss: 1206.0722045898438\n",
      "Epoch 2453/3000, Loss: 1174.404805501302\n",
      "Epoch 2454/3000, Loss: 1168.0071309407551\n",
      "Epoch 2455/3000, Loss: 1129.3782958984375\n",
      "Epoch 2456/3000, Loss: 1189.4559936523438\n",
      "Epoch 2457/3000, Loss: 1112.641825358073\n",
      "Epoch 2458/3000, Loss: 1143.1410522460938\n",
      "Epoch 2459/3000, Loss: 1141.6311442057292\n",
      "Epoch 2460/3000, Loss: 1126.8606567382812\n",
      "Epoch 2461/3000, Loss: 1143.1295471191406\n",
      "Epoch 2462/3000, Loss: 1208.0849609375\n",
      "Epoch 2463/3000, Loss: 1093.9885762532551\n",
      "Epoch 2464/3000, Loss: 1084.2393086751301\n",
      "Epoch 2465/3000, Loss: 1106.834737141927\n",
      "Epoch 2466/3000, Loss: 1164.3499755859375\n",
      "Epoch 2467/3000, Loss: 1079.0713500976562\n",
      "Epoch 2468/3000, Loss: 1187.33740234375\n",
      "Epoch 2469/3000, Loss: 1158.7343343098958\n",
      "Epoch 2470/3000, Loss: 1265.0760396321614\n",
      "Epoch 2471/3000, Loss: 1169.117655436198\n",
      "Epoch 2472/3000, Loss: 1165.2115275065105\n",
      "Epoch 2473/3000, Loss: 1160.0822855631511\n",
      "Epoch 2474/3000, Loss: 1176.4314371744792\n",
      "Epoch 2475/3000, Loss: 1123.9116516113281\n",
      "Epoch 2476/3000, Loss: 1181.1521402994792\n",
      "Epoch 2477/3000, Loss: 1220.2298583984375\n",
      "Epoch 2478/3000, Loss: 1164.3912658691406\n",
      "Epoch 2479/3000, Loss: 1117.1306966145833\n",
      "Epoch 2480/3000, Loss: 1130.5032958984375\n",
      "Epoch 2481/3000, Loss: 1168.9862874348958\n",
      "Epoch 2482/3000, Loss: 1087.085469563802\n",
      "Epoch 2483/3000, Loss: 1172.972188313802\n",
      "Epoch 2484/3000, Loss: 1175.6109822591145\n",
      "Epoch 2485/3000, Loss: 1168.6878051757812\n",
      "Epoch 2486/3000, Loss: 1135.7364298502605\n",
      "Epoch 2487/3000, Loss: 1105.0181274414062\n",
      "Epoch 2488/3000, Loss: 1090.5672505696614\n",
      "Epoch 2489/3000, Loss: 1253.1946919759114\n",
      "Epoch 2490/3000, Loss: 1139.0714009602864\n",
      "Epoch 2491/3000, Loss: 1213.6301879882812\n",
      "Epoch 2492/3000, Loss: 1091.0073140462239\n",
      "Epoch 2493/3000, Loss: 1128.8166198730469\n",
      "Epoch 2494/3000, Loss: 1134.198221842448\n",
      "Epoch 2495/3000, Loss: 1140.4640706380208\n",
      "Epoch 2496/3000, Loss: 1134.3243611653645\n",
      "Epoch 2497/3000, Loss: 1095.9871520996094\n",
      "Epoch 2498/3000, Loss: 1085.9875590006511\n",
      "Epoch 2499/3000, Loss: 1206.1719156901042\n",
      "Epoch 2500/3000, Loss: 1114.9158528645833\n",
      "Epoch 2501/3000, Loss: 1137.906229654948\n",
      "Epoch 2502/3000, Loss: 1100.623067220052\n",
      "Epoch 2503/3000, Loss: 1124.9859924316406\n",
      "Epoch 2504/3000, Loss: 1063.6396484375\n",
      "Epoch 2505/3000, Loss: 1111.8184407552083\n",
      "Epoch 2506/3000, Loss: 1194.3543904622395\n",
      "Epoch 2507/3000, Loss: 1086.7587788899739\n",
      "Epoch 2508/3000, Loss: 1052.6702575683594\n",
      "Epoch 2509/3000, Loss: 1165.2408548990886\n",
      "Epoch 2510/3000, Loss: 1235.4777119954426\n",
      "Epoch 2511/3000, Loss: 1138.0376993815105\n",
      "Epoch 2512/3000, Loss: 1101.4308776855469\n",
      "Epoch 2513/3000, Loss: 1154.4712219238281\n",
      "Epoch 2514/3000, Loss: 1151.1998189290364\n",
      "Epoch 2515/3000, Loss: 1244.0769449869792\n",
      "Epoch 2516/3000, Loss: 1137.576924641927\n",
      "Epoch 2517/3000, Loss: 1135.6785176595051\n",
      "Epoch 2518/3000, Loss: 1133.1764729817708\n",
      "Epoch 2519/3000, Loss: 1084.7101542154949\n",
      "Epoch 2520/3000, Loss: 1158.936279296875\n",
      "Epoch 2521/3000, Loss: 1163.1131388346355\n",
      "Epoch 2522/3000, Loss: 1111.3583068847656\n",
      "Epoch 2523/3000, Loss: 1136.195536295573\n",
      "Epoch 2524/3000, Loss: 1140.4905090332031\n",
      "Epoch 2525/3000, Loss: 1100.7655741373699\n",
      "Epoch 2526/3000, Loss: 1164.7841084798176\n",
      "Epoch 2527/3000, Loss: 1128.1699829101562\n",
      "Epoch 2528/3000, Loss: 1030.2576395670574\n",
      "Epoch 2529/3000, Loss: 1148.6515299479167\n",
      "Epoch 2530/3000, Loss: 1095.9878845214844\n",
      "Epoch 2531/3000, Loss: 1110.5211791992188\n",
      "Epoch 2532/3000, Loss: 1136.7282613118489\n",
      "Epoch 2533/3000, Loss: 1183.8421020507812\n",
      "Epoch 2534/3000, Loss: 1141.1334025065105\n",
      "Epoch 2535/3000, Loss: 1109.0283203125\n",
      "Epoch 2536/3000, Loss: 1094.724365234375\n",
      "Epoch 2537/3000, Loss: 1134.3009948730469\n",
      "Epoch 2538/3000, Loss: 1066.6987813313801\n",
      "Epoch 2539/3000, Loss: 1218.3953653971355\n",
      "Epoch 2540/3000, Loss: 1103.825703938802\n",
      "Epoch 2541/3000, Loss: 1185.460917154948\n",
      "Epoch 2542/3000, Loss: 1164.0682067871094\n",
      "Epoch 2543/3000, Loss: 1131.1733703613281\n",
      "Epoch 2544/3000, Loss: 1096.3334248860676\n",
      "Epoch 2545/3000, Loss: 1084.6061604817708\n",
      "Epoch 2546/3000, Loss: 1192.9370727539062\n",
      "Epoch 2547/3000, Loss: 1149.677958170573\n",
      "Epoch 2548/3000, Loss: 1077.4044494628906\n",
      "Epoch 2549/3000, Loss: 1072.4829711914062\n",
      "Epoch 2550/3000, Loss: 1028.0040079752605\n",
      "Epoch 2551/3000, Loss: 1259.3786112467449\n",
      "Epoch 2552/3000, Loss: 1138.5228881835938\n",
      "Epoch 2553/3000, Loss: 1147.3312377929688\n",
      "Epoch 2554/3000, Loss: 1116.6803792317708\n",
      "Epoch 2555/3000, Loss: 1117.2457885742188\n",
      "Epoch 2556/3000, Loss: 1143.3655700683594\n",
      "Epoch 2557/3000, Loss: 1120.0166829427083\n",
      "Epoch 2558/3000, Loss: 1158.5738728841145\n",
      "Epoch 2559/3000, Loss: 1165.2066955566406\n",
      "Epoch 2560/3000, Loss: 1116.0077006022136\n",
      "Epoch 2561/3000, Loss: 1111.5539855957031\n",
      "Epoch 2562/3000, Loss: 1170.7192077636719\n",
      "Epoch 2563/3000, Loss: 1145.0050252278645\n",
      "Epoch 2564/3000, Loss: 1106.9126586914062\n",
      "Epoch 2565/3000, Loss: 1135.9982198079426\n",
      "Epoch 2566/3000, Loss: 1122.0464375813801\n",
      "Epoch 2567/3000, Loss: 1106.6538187662761\n",
      "Epoch 2568/3000, Loss: 1012.1788228352865\n",
      "Epoch 2569/3000, Loss: 1182.1090698242188\n",
      "Epoch 2570/3000, Loss: 1071.3477783203125\n",
      "Epoch 2571/3000, Loss: 1124.7400716145833\n",
      "Epoch 2572/3000, Loss: 1102.46923828125\n",
      "Epoch 2573/3000, Loss: 1159.10498046875\n",
      "Epoch 2574/3000, Loss: 1078.0990498860676\n",
      "Epoch 2575/3000, Loss: 1152.4079182942708\n",
      "Epoch 2576/3000, Loss: 1136.7636311848958\n",
      "Epoch 2577/3000, Loss: 1010.3539835611979\n",
      "Epoch 2578/3000, Loss: 1143.5887247721355\n",
      "Epoch 2579/3000, Loss: 1200.648905436198\n",
      "Epoch 2580/3000, Loss: 1129.4894714355469\n",
      "Epoch 2581/3000, Loss: 1117.3534037272136\n",
      "Epoch 2582/3000, Loss: 1129.5635070800781\n",
      "Epoch 2583/3000, Loss: 1187.5993245442708\n",
      "Epoch 2584/3000, Loss: 1123.969706217448\n",
      "Epoch 2585/3000, Loss: 1148.2141418457031\n",
      "Epoch 2586/3000, Loss: 1083.5408223470051\n",
      "Epoch 2587/3000, Loss: 1189.5214131673176\n",
      "Epoch 2588/3000, Loss: 1163.4384256998699\n",
      "Epoch 2589/3000, Loss: 1109.0679117838542\n",
      "Epoch 2590/3000, Loss: 1067.6305033365886\n",
      "Epoch 2591/3000, Loss: 1118.5443522135417\n",
      "Epoch 2592/3000, Loss: 1118.082051595052\n",
      "Epoch 2593/3000, Loss: 1101.8968709309895\n",
      "Epoch 2594/3000, Loss: 1123.6773376464844\n",
      "Epoch 2595/3000, Loss: 1176.3362833658855\n",
      "Epoch 2596/3000, Loss: 1098.6324971516926\n",
      "Epoch 2597/3000, Loss: 1198.0501505533855\n",
      "Epoch 2598/3000, Loss: 1139.2589009602864\n",
      "Epoch 2599/3000, Loss: 1048.8124694824219\n",
      "Epoch 2600/3000, Loss: 1116.4801127115886\n",
      "Epoch 2601/3000, Loss: 1152.0099283854167\n",
      "Epoch 2602/3000, Loss: 1148.0079956054688\n",
      "Epoch 2603/3000, Loss: 1151.8963826497395\n",
      "Epoch 2604/3000, Loss: 1127.9894307454426\n",
      "Epoch 2605/3000, Loss: 1128.8675842285156\n",
      "Epoch 2606/3000, Loss: 1144.9584147135417\n",
      "Epoch 2607/3000, Loss: 1139.9964090983074\n",
      "Epoch 2608/3000, Loss: 1186.9854125976562\n",
      "Epoch 2609/3000, Loss: 1192.3125\n",
      "Epoch 2610/3000, Loss: 1084.9859822591145\n",
      "Epoch 2611/3000, Loss: 1188.117451985677\n",
      "Epoch 2612/3000, Loss: 1126.6930440266926\n",
      "Epoch 2613/3000, Loss: 1151.8454488118489\n",
      "Epoch 2614/3000, Loss: 1117.9342447916667\n",
      "Epoch 2615/3000, Loss: 1104.938212076823\n",
      "Epoch 2616/3000, Loss: 1126.4465433756511\n",
      "Epoch 2617/3000, Loss: 1150.9833374023438\n",
      "Epoch 2618/3000, Loss: 1166.185811360677\n",
      "Epoch 2619/3000, Loss: 1152.74072265625\n",
      "Epoch 2620/3000, Loss: 1128.2593078613281\n",
      "Epoch 2621/3000, Loss: 1139.7623596191406\n",
      "Epoch 2622/3000, Loss: 1153.7376098632812\n",
      "Epoch 2623/3000, Loss: 1158.8837280273438\n",
      "Epoch 2624/3000, Loss: 1119.1418762207031\n",
      "Epoch 2625/3000, Loss: 1157.5527852376301\n",
      "Epoch 2626/3000, Loss: 1106.987548828125\n",
      "Epoch 2627/3000, Loss: 1173.6574300130208\n",
      "Epoch 2628/3000, Loss: 1124.1027526855469\n",
      "Epoch 2629/3000, Loss: 1162.8247172037761\n",
      "Epoch 2630/3000, Loss: 1087.7338562011719\n",
      "Epoch 2631/3000, Loss: 1165.2518819173176\n",
      "Epoch 2632/3000, Loss: 1146.3279622395833\n",
      "Epoch 2633/3000, Loss: 1166.2960510253906\n",
      "Epoch 2634/3000, Loss: 1129.4987386067708\n",
      "Epoch 2635/3000, Loss: 1146.3565979003906\n",
      "Epoch 2636/3000, Loss: 1172.6334431966145\n",
      "Epoch 2637/3000, Loss: 1159.0109558105469\n",
      "Epoch 2638/3000, Loss: 1151.2414143880208\n",
      "Epoch 2639/3000, Loss: 1133.7504781087239\n",
      "Epoch 2640/3000, Loss: 1156.3460591634114\n",
      "Epoch 2641/3000, Loss: 1097.4032084147136\n",
      "Epoch 2642/3000, Loss: 1086.2821756998699\n",
      "Epoch 2643/3000, Loss: 1113.0894063313801\n",
      "Epoch 2644/3000, Loss: 1107.1119283040364\n",
      "Epoch 2645/3000, Loss: 1045.6687316894531\n",
      "Epoch 2646/3000, Loss: 1108.1862691243489\n",
      "Epoch 2647/3000, Loss: 1110.2779947916667\n",
      "Epoch 2648/3000, Loss: 1066.6793619791667\n",
      "Epoch 2649/3000, Loss: 1202.0350341796875\n",
      "Epoch 2650/3000, Loss: 1062.141866048177\n",
      "Epoch 2651/3000, Loss: 1105.1092325846355\n",
      "Epoch 2652/3000, Loss: 1146.6266988118489\n",
      "Epoch 2653/3000, Loss: 1263.5452270507812\n",
      "Epoch 2654/3000, Loss: 1136.6489868164062\n",
      "Epoch 2655/3000, Loss: 1155.4304606119792\n",
      "Epoch 2656/3000, Loss: 1098.4179585774739\n",
      "Epoch 2657/3000, Loss: 1104.7521667480469\n",
      "Epoch 2658/3000, Loss: 1136.3197428385417\n",
      "Epoch 2659/3000, Loss: 1096.731669108073\n",
      "Epoch 2660/3000, Loss: 1177.5820719401042\n",
      "Epoch 2661/3000, Loss: 1121.7344665527344\n",
      "Epoch 2662/3000, Loss: 1139.655008951823\n",
      "Epoch 2663/3000, Loss: 1221.7486775716145\n",
      "Epoch 2664/3000, Loss: 1202.2718912760417\n",
      "Epoch 2665/3000, Loss: 1133.4064636230469\n",
      "Epoch 2666/3000, Loss: 1086.2829488118489\n",
      "Epoch 2667/3000, Loss: 1083.6136678059895\n",
      "Epoch 2668/3000, Loss: 1120.3014729817708\n",
      "Epoch 2669/3000, Loss: 1098.093526204427\n",
      "Epoch 2670/3000, Loss: 1170.282938639323\n",
      "Epoch 2671/3000, Loss: 1130.7577616373699\n",
      "Epoch 2672/3000, Loss: 1105.0890808105469\n",
      "Epoch 2673/3000, Loss: 1139.8775939941406\n",
      "Epoch 2674/3000, Loss: 1135.4544270833333\n",
      "Epoch 2675/3000, Loss: 1049.5814717610676\n",
      "Epoch 2676/3000, Loss: 1083.6964721679688\n",
      "Epoch 2677/3000, Loss: 1105.0353190104167\n",
      "Epoch 2678/3000, Loss: 1172.2507934570312\n",
      "Epoch 2679/3000, Loss: 1210.4201558430989\n",
      "Epoch 2680/3000, Loss: 1123.472900390625\n",
      "Epoch 2681/3000, Loss: 1193.5945739746094\n",
      "Epoch 2682/3000, Loss: 1066.0210978190105\n",
      "Epoch 2683/3000, Loss: 1103.6615804036458\n",
      "Epoch 2684/3000, Loss: 1137.5941060384114\n",
      "Epoch 2685/3000, Loss: 1132.6757405598958\n",
      "Epoch 2686/3000, Loss: 1158.5963948567708\n",
      "Epoch 2687/3000, Loss: 1104.8512471516926\n",
      "Epoch 2688/3000, Loss: 1159.7630818684895\n",
      "Epoch 2689/3000, Loss: 1112.2531026204426\n",
      "Epoch 2690/3000, Loss: 1115.9443868001301\n",
      "Epoch 2691/3000, Loss: 1194.8801879882812\n",
      "Epoch 2692/3000, Loss: 1090.6555786132812\n",
      "Epoch 2693/3000, Loss: 1114.6230977376301\n",
      "Epoch 2694/3000, Loss: 1147.9078979492188\n",
      "Epoch 2695/3000, Loss: 1074.6261393229167\n",
      "Epoch 2696/3000, Loss: 1099.1307373046875\n",
      "Epoch 2697/3000, Loss: 1086.0700988769531\n",
      "Epoch 2698/3000, Loss: 1061.6918334960938\n",
      "Epoch 2699/3000, Loss: 1075.6182963053386\n",
      "Epoch 2700/3000, Loss: 1051.4847310384114\n",
      "Epoch 2701/3000, Loss: 1188.4312540690105\n",
      "Epoch 2702/3000, Loss: 1170.188212076823\n",
      "Epoch 2703/3000, Loss: 1043.8436177571614\n",
      "Epoch 2704/3000, Loss: 1111.3994038899739\n",
      "Epoch 2705/3000, Loss: 1091.427998860677\n",
      "Epoch 2706/3000, Loss: 1128.0724792480469\n",
      "Epoch 2707/3000, Loss: 1222.8453369140625\n",
      "Epoch 2708/3000, Loss: 1111.9767049153645\n",
      "Epoch 2709/3000, Loss: 1187.625732421875\n",
      "Epoch 2710/3000, Loss: 1077.140645345052\n",
      "Epoch 2711/3000, Loss: 1116.1851196289062\n",
      "Epoch 2712/3000, Loss: 1208.1977132161458\n",
      "Epoch 2713/3000, Loss: 1176.2943929036458\n",
      "Epoch 2714/3000, Loss: 1118.8703104654949\n",
      "Epoch 2715/3000, Loss: 1137.9573160807292\n",
      "Epoch 2716/3000, Loss: 1115.4696858723958\n",
      "Epoch 2717/3000, Loss: 1139.5348205566406\n",
      "Epoch 2718/3000, Loss: 1157.556660970052\n",
      "Epoch 2719/3000, Loss: 1028.38818359375\n",
      "Epoch 2720/3000, Loss: 1082.1866658528645\n",
      "Epoch 2721/3000, Loss: 1139.3062642415364\n",
      "Epoch 2722/3000, Loss: 1102.4004516601562\n",
      "Epoch 2723/3000, Loss: 1192.6412455240886\n",
      "Epoch 2724/3000, Loss: 1144.1060485839844\n",
      "Epoch 2725/3000, Loss: 1168.2369791666667\n",
      "Epoch 2726/3000, Loss: 1068.076904296875\n",
      "Epoch 2727/3000, Loss: 1010.2767639160156\n",
      "Epoch 2728/3000, Loss: 1075.9868062337239\n",
      "Epoch 2729/3000, Loss: 1107.7593282063801\n",
      "Epoch 2730/3000, Loss: 1196.1696268717449\n",
      "Epoch 2731/3000, Loss: 1136.8241780598958\n",
      "Epoch 2732/3000, Loss: 1085.2599589029949\n",
      "Epoch 2733/3000, Loss: 1131.5585123697917\n",
      "Epoch 2734/3000, Loss: 1102.3953959147136\n",
      "Epoch 2735/3000, Loss: 1082.8877258300781\n",
      "Epoch 2736/3000, Loss: 1088.1502482096355\n",
      "Epoch 2737/3000, Loss: 1159.0294291178386\n",
      "Epoch 2738/3000, Loss: 1085.1706034342449\n",
      "Epoch 2739/3000, Loss: 1172.9891967773438\n",
      "Epoch 2740/3000, Loss: 1082.3021036783855\n",
      "Epoch 2741/3000, Loss: 1108.7108561197917\n",
      "Epoch 2742/3000, Loss: 1040.2178141276042\n",
      "Epoch 2743/3000, Loss: 1072.5959879557292\n",
      "Epoch 2744/3000, Loss: 1027.624735514323\n",
      "Epoch 2745/3000, Loss: 1077.4375915527344\n",
      "Epoch 2746/3000, Loss: 1061.3118794759114\n",
      "Epoch 2747/3000, Loss: 1132.4346313476562\n",
      "Epoch 2748/3000, Loss: 1119.1863199869792\n",
      "Epoch 2749/3000, Loss: 1025.9970397949219\n",
      "Epoch 2750/3000, Loss: 1116.530497233073\n",
      "Epoch 2751/3000, Loss: 1115.9561360677083\n",
      "Epoch 2752/3000, Loss: 1180.6603291829426\n",
      "Epoch 2753/3000, Loss: 1067.0410766601562\n",
      "Epoch 2754/3000, Loss: 1108.3593037923176\n",
      "Epoch 2755/3000, Loss: 1120.0696207682292\n",
      "Epoch 2756/3000, Loss: 1200.9192301432292\n",
      "Epoch 2757/3000, Loss: 1102.4069010416667\n",
      "Epoch 2758/3000, Loss: 1120.1478271484375\n",
      "Epoch 2759/3000, Loss: 1137.4094645182292\n",
      "Epoch 2760/3000, Loss: 1100.3276672363281\n",
      "Epoch 2761/3000, Loss: 1150.4226684570312\n",
      "Epoch 2762/3000, Loss: 1131.1723124186199\n",
      "Epoch 2763/3000, Loss: 1041.7158203125\n",
      "Epoch 2764/3000, Loss: 1134.3795267740886\n",
      "Epoch 2765/3000, Loss: 1122.8834228515625\n",
      "Epoch 2766/3000, Loss: 1076.7367146809895\n",
      "Epoch 2767/3000, Loss: 1190.0604451497395\n",
      "Epoch 2768/3000, Loss: 1097.7869466145833\n",
      "Epoch 2769/3000, Loss: 1104.4341430664062\n",
      "Epoch 2770/3000, Loss: 1063.020772298177\n",
      "Epoch 2771/3000, Loss: 1143.8225708007812\n",
      "Epoch 2772/3000, Loss: 1051.6280110677083\n",
      "Epoch 2773/3000, Loss: 1131.0611165364583\n",
      "Epoch 2774/3000, Loss: 1086.1610616048176\n",
      "Epoch 2775/3000, Loss: 1058.4354451497395\n",
      "Epoch 2776/3000, Loss: 1111.7890014648438\n",
      "Epoch 2777/3000, Loss: 1094.5710754394531\n",
      "Epoch 2778/3000, Loss: 1108.7087097167969\n",
      "Epoch 2779/3000, Loss: 1130.6764729817708\n",
      "Epoch 2780/3000, Loss: 1065.2766825358074\n",
      "Epoch 2781/3000, Loss: 1138.3157552083333\n",
      "Epoch 2782/3000, Loss: 1103.7613118489583\n",
      "Epoch 2783/3000, Loss: 1068.9144083658855\n",
      "Epoch 2784/3000, Loss: 1128.5121256510417\n",
      "Epoch 2785/3000, Loss: 1094.8516743977864\n",
      "Epoch 2786/3000, Loss: 1171.8387552897136\n",
      "Epoch 2787/3000, Loss: 1134.6919352213542\n",
      "Epoch 2788/3000, Loss: 1092.0291951497395\n",
      "Epoch 2789/3000, Loss: 1182.8414916992188\n",
      "Epoch 2790/3000, Loss: 1084.6579996744792\n",
      "Epoch 2791/3000, Loss: 1081.5055440266926\n",
      "Epoch 2792/3000, Loss: 1123.065450032552\n",
      "Epoch 2793/3000, Loss: 1138.6370035807292\n",
      "Epoch 2794/3000, Loss: 1136.1122334798176\n",
      "Epoch 2795/3000, Loss: 1122.4021708170574\n",
      "Epoch 2796/3000, Loss: 1147.9365844726562\n",
      "Epoch 2797/3000, Loss: 1030.2252909342449\n",
      "Epoch 2798/3000, Loss: 1198.432108561198\n",
      "Epoch 2799/3000, Loss: 1259.445048014323\n",
      "Epoch 2800/3000, Loss: 1063.8823547363281\n",
      "Epoch 2801/3000, Loss: 1070.7103780110676\n",
      "Epoch 2802/3000, Loss: 1045.5197652180989\n",
      "Epoch 2803/3000, Loss: 1155.8718668619792\n",
      "Epoch 2804/3000, Loss: 1112.9554850260417\n",
      "Epoch 2805/3000, Loss: 1087.8287251790364\n",
      "Epoch 2806/3000, Loss: 1042.0477294921875\n",
      "Epoch 2807/3000, Loss: 1075.4569803873699\n",
      "Epoch 2808/3000, Loss: 997.9352315266927\n",
      "Epoch 2809/3000, Loss: 1032.9647928873699\n",
      "Epoch 2810/3000, Loss: 1016.2748311360677\n",
      "Epoch 2811/3000, Loss: 1085.7256469726562\n",
      "Epoch 2812/3000, Loss: 1114.4374593098958\n",
      "Epoch 2813/3000, Loss: 1140.6217244466145\n",
      "Epoch 2814/3000, Loss: 1119.099100748698\n",
      "Epoch 2815/3000, Loss: 1056.7510884602864\n",
      "Epoch 2816/3000, Loss: 1161.5183715820312\n",
      "Epoch 2817/3000, Loss: 1152.500732421875\n",
      "Epoch 2818/3000, Loss: 1091.8021443684895\n",
      "Epoch 2819/3000, Loss: 1077.1243387858074\n",
      "Epoch 2820/3000, Loss: 1110.4186197916667\n",
      "Epoch 2821/3000, Loss: 1198.6860656738281\n",
      "Epoch 2822/3000, Loss: 1098.7464904785156\n",
      "Epoch 2823/3000, Loss: 1096.4471740722656\n",
      "Epoch 2824/3000, Loss: 1194.39501953125\n",
      "Epoch 2825/3000, Loss: 1129.4368387858074\n",
      "Epoch 2826/3000, Loss: 1098.9560445149739\n",
      "Epoch 2827/3000, Loss: 1213.3489176432292\n",
      "Epoch 2828/3000, Loss: 1200.4554036458333\n",
      "Epoch 2829/3000, Loss: 1128.167500813802\n",
      "Epoch 2830/3000, Loss: 1096.1139424641926\n",
      "Epoch 2831/3000, Loss: 1138.0960489908855\n",
      "Epoch 2832/3000, Loss: 1096.1451009114583\n",
      "Epoch 2833/3000, Loss: 1087.6286926269531\n",
      "Epoch 2834/3000, Loss: 1146.775390625\n",
      "Epoch 2835/3000, Loss: 1058.3284912109375\n",
      "Epoch 2836/3000, Loss: 1158.6435139973958\n",
      "Epoch 2837/3000, Loss: 1140.4460042317708\n",
      "Epoch 2838/3000, Loss: 1062.148193359375\n",
      "Epoch 2839/3000, Loss: 1159.8604736328125\n",
      "Epoch 2840/3000, Loss: 1174.0587259928386\n",
      "Epoch 2841/3000, Loss: 1092.3553568522136\n",
      "Epoch 2842/3000, Loss: 1093.6751098632812\n",
      "Epoch 2843/3000, Loss: 1114.9819641113281\n",
      "Epoch 2844/3000, Loss: 1013.6731567382812\n",
      "Epoch 2845/3000, Loss: 1075.9266153971355\n",
      "Epoch 2846/3000, Loss: 1098.84521484375\n",
      "Epoch 2847/3000, Loss: 1086.8120218912761\n",
      "Epoch 2848/3000, Loss: 1087.9145100911458\n",
      "Epoch 2849/3000, Loss: 1163.6175740559895\n",
      "Epoch 2850/3000, Loss: 1160.1733296712239\n",
      "Epoch 2851/3000, Loss: 1197.5379130045574\n",
      "Epoch 2852/3000, Loss: 1097.5341695149739\n",
      "Epoch 2853/3000, Loss: 1036.5492655436199\n",
      "Epoch 2854/3000, Loss: 1107.1380920410156\n",
      "Epoch 2855/3000, Loss: 1132.6563110351562\n",
      "Epoch 2856/3000, Loss: 1117.1054382324219\n",
      "Epoch 2857/3000, Loss: 1166.1853637695312\n",
      "Epoch 2858/3000, Loss: 1125.4801228841145\n",
      "Epoch 2859/3000, Loss: 1080.685038248698\n",
      "Epoch 2860/3000, Loss: 1103.3267618815105\n",
      "Epoch 2861/3000, Loss: 1087.3846232096355\n",
      "Epoch 2862/3000, Loss: 1060.0765482584636\n",
      "Epoch 2863/3000, Loss: 1041.2216288248699\n",
      "Epoch 2864/3000, Loss: 1095.4036153157551\n",
      "Epoch 2865/3000, Loss: 1081.4022827148438\n",
      "Epoch 2866/3000, Loss: 1118.3629455566406\n",
      "Epoch 2867/3000, Loss: 1116.3711344401042\n",
      "Epoch 2868/3000, Loss: 1028.3089294433594\n",
      "Epoch 2869/3000, Loss: 1141.8062032063801\n",
      "Epoch 2870/3000, Loss: 1127.6891479492188\n",
      "Epoch 2871/3000, Loss: 1053.7283630371094\n",
      "Epoch 2872/3000, Loss: 1063.5829264322917\n",
      "Epoch 2873/3000, Loss: 1088.0202128092449\n",
      "Epoch 2874/3000, Loss: 1114.1825561523438\n",
      "Epoch 2875/3000, Loss: 1131.5220336914062\n",
      "Epoch 2876/3000, Loss: 1154.0967814127605\n",
      "Epoch 2877/3000, Loss: 1089.6746215820312\n",
      "Epoch 2878/3000, Loss: 1271.0508626302083\n",
      "Epoch 2879/3000, Loss: 1128.1038818359375\n",
      "Epoch 2880/3000, Loss: 1190.1966349283855\n",
      "Epoch 2881/3000, Loss: 1065.6100769042969\n",
      "Epoch 2882/3000, Loss: 977.315419514974\n",
      "Epoch 2883/3000, Loss: 1078.5620218912761\n",
      "Epoch 2884/3000, Loss: 1110.5356343587239\n",
      "Epoch 2885/3000, Loss: 1126.6195475260417\n",
      "Epoch 2886/3000, Loss: 1058.9898173014324\n",
      "Epoch 2887/3000, Loss: 1126.1194356282551\n",
      "Epoch 2888/3000, Loss: 1035.0711568196614\n",
      "Epoch 2889/3000, Loss: 1154.7121988932292\n",
      "Epoch 2890/3000, Loss: 1093.0403442382812\n",
      "Epoch 2891/3000, Loss: 1121.5228474934895\n",
      "Epoch 2892/3000, Loss: 1138.8252360026042\n",
      "Epoch 2893/3000, Loss: 1215.4837036132812\n",
      "Epoch 2894/3000, Loss: 1090.2120361328125\n",
      "Epoch 2895/3000, Loss: 1125.7900899251301\n",
      "Epoch 2896/3000, Loss: 1069.5487060546875\n",
      "Epoch 2897/3000, Loss: 1064.5328369140625\n",
      "Epoch 2898/3000, Loss: 1108.2013448079426\n",
      "Epoch 2899/3000, Loss: 1092.2178344726562\n",
      "Epoch 2900/3000, Loss: 1077.3531901041667\n",
      "Epoch 2901/3000, Loss: 1075.4846700032551\n",
      "Epoch 2902/3000, Loss: 1017.4167887369791\n",
      "Epoch 2903/3000, Loss: 1066.7897338867188\n",
      "Epoch 2904/3000, Loss: 1084.1070353190105\n",
      "Epoch 2905/3000, Loss: 1040.4305318196614\n",
      "Epoch 2906/3000, Loss: 1085.0545043945312\n",
      "Epoch 2907/3000, Loss: 1091.8289082845051\n",
      "Epoch 2908/3000, Loss: 1111.1287841796875\n",
      "Epoch 2909/3000, Loss: 1040.3084513346355\n",
      "Epoch 2910/3000, Loss: 1159.0834045410156\n",
      "Epoch 2911/3000, Loss: 1133.3172200520833\n",
      "Epoch 2912/3000, Loss: 1157.4920654296875\n",
      "Epoch 2913/3000, Loss: 1097.3898315429688\n",
      "Epoch 2914/3000, Loss: 1100.31787109375\n",
      "Epoch 2915/3000, Loss: 958.7108662923177\n",
      "Epoch 2916/3000, Loss: 1154.6174926757812\n",
      "Epoch 2917/3000, Loss: 1126.5775248209636\n",
      "Epoch 2918/3000, Loss: 1136.9259338378906\n",
      "Epoch 2919/3000, Loss: 1035.3271077473958\n",
      "Epoch 2920/3000, Loss: 1103.708251953125\n",
      "Epoch 2921/3000, Loss: 1072.7538655598958\n",
      "Epoch 2922/3000, Loss: 1182.0852457682292\n",
      "Epoch 2923/3000, Loss: 1144.4291178385417\n",
      "Epoch 2924/3000, Loss: 1077.1759134928386\n",
      "Epoch 2925/3000, Loss: 1105.1544087727864\n",
      "Epoch 2926/3000, Loss: 1149.1938374837239\n",
      "Epoch 2927/3000, Loss: 1097.4278259277344\n",
      "Epoch 2928/3000, Loss: 1172.6497802734375\n",
      "Epoch 2929/3000, Loss: 1224.254638671875\n",
      "Epoch 2930/3000, Loss: 1129.1453653971355\n",
      "Epoch 2931/3000, Loss: 1125.0287984212239\n",
      "Epoch 2932/3000, Loss: 1122.3991292317708\n",
      "Epoch 2933/3000, Loss: 1171.2572631835938\n",
      "Epoch 2934/3000, Loss: 1154.2569885253906\n",
      "Epoch 2935/3000, Loss: 1056.6944274902344\n",
      "Epoch 2936/3000, Loss: 1158.1179300944011\n",
      "Epoch 2937/3000, Loss: 1134.0434265136719\n",
      "Epoch 2938/3000, Loss: 1083.6241963704426\n",
      "Epoch 2939/3000, Loss: 1069.9996846516926\n",
      "Epoch 2940/3000, Loss: 1152.0334777832031\n",
      "Epoch 2941/3000, Loss: 1136.9870198567708\n",
      "Epoch 2942/3000, Loss: 1041.117451985677\n",
      "Epoch 2943/3000, Loss: 1136.2313130696614\n",
      "Epoch 2944/3000, Loss: 1015.2879028320312\n",
      "Epoch 2945/3000, Loss: 1142.9130757649739\n",
      "Epoch 2946/3000, Loss: 1076.480712890625\n",
      "Epoch 2947/3000, Loss: 1103.3238220214844\n",
      "Epoch 2948/3000, Loss: 1104.8210550944011\n",
      "Epoch 2949/3000, Loss: 1127.7763061523438\n",
      "Epoch 2950/3000, Loss: 1067.8687642415364\n",
      "Epoch 2951/3000, Loss: 1167.4607543945312\n",
      "Epoch 2952/3000, Loss: 1062.0536499023438\n",
      "Epoch 2953/3000, Loss: 1068.1480712890625\n",
      "Epoch 2954/3000, Loss: 1116.2483317057292\n",
      "Epoch 2955/3000, Loss: 1085.1127522786458\n",
      "Epoch 2956/3000, Loss: 1049.0677185058594\n",
      "Epoch 2957/3000, Loss: 1005.0526123046875\n",
      "Epoch 2958/3000, Loss: 1098.893534342448\n",
      "Epoch 2959/3000, Loss: 1057.4470621744792\n",
      "Epoch 2960/3000, Loss: 1081.8020121256511\n",
      "Epoch 2961/3000, Loss: 1115.5912272135417\n",
      "Epoch 2962/3000, Loss: 1093.5592956542969\n",
      "Epoch 2963/3000, Loss: 1080.1685282389324\n",
      "Epoch 2964/3000, Loss: 1101.7464803059895\n",
      "Epoch 2965/3000, Loss: 1121.8965148925781\n",
      "Epoch 2966/3000, Loss: 1062.2029520670574\n",
      "Epoch 2967/3000, Loss: 1042.3582560221355\n",
      "Epoch 2968/3000, Loss: 1079.708984375\n",
      "Epoch 2969/3000, Loss: 1023.4743143717448\n",
      "Epoch 2970/3000, Loss: 1142.5670369466145\n",
      "Epoch 2971/3000, Loss: 1018.1330057779948\n",
      "Epoch 2972/3000, Loss: 1098.4680887858074\n",
      "Epoch 2973/3000, Loss: 1101.8513895670574\n",
      "Epoch 2974/3000, Loss: 1038.3644917805989\n",
      "Epoch 2975/3000, Loss: 1081.5818888346355\n",
      "Epoch 2976/3000, Loss: 1104.989969889323\n",
      "Epoch 2977/3000, Loss: 1141.5555521647136\n",
      "Epoch 2978/3000, Loss: 1062.9294535319011\n",
      "Epoch 2979/3000, Loss: 1059.9708760579426\n",
      "Epoch 2980/3000, Loss: 1121.7238667805989\n",
      "Epoch 2981/3000, Loss: 1122.773213704427\n",
      "Epoch 2982/3000, Loss: 1132.2454833984375\n",
      "Epoch 2983/3000, Loss: 1080.6448771158855\n",
      "Epoch 2984/3000, Loss: 1079.3592529296875\n",
      "Epoch 2985/3000, Loss: 1158.4726053873699\n",
      "Epoch 2986/3000, Loss: 1046.6022542317708\n",
      "Epoch 2987/3000, Loss: 1154.1698303222656\n",
      "Epoch 2988/3000, Loss: 1136.5411173502605\n",
      "Epoch 2989/3000, Loss: 1106.9543762207031\n",
      "Epoch 2990/3000, Loss: 1075.1230367024739\n",
      "Epoch 2991/3000, Loss: 1058.2710571289062\n",
      "Epoch 2992/3000, Loss: 1146.6367289225261\n",
      "Epoch 2993/3000, Loss: 1067.9305216471355\n",
      "Epoch 2994/3000, Loss: 1077.6340230305989\n",
      "Epoch 2995/3000, Loss: 1143.4525553385417\n",
      "Epoch 2996/3000, Loss: 1123.0245564778645\n",
      "Epoch 2997/3000, Loss: 1149.119140625\n",
      "Epoch 2998/3000, Loss: 1060.8781840006511\n",
      "Epoch 2999/3000, Loss: 1138.0244750976562\n",
      "Epoch 3000/3000, Loss: 1063.7529602050781\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 4587.52783203125\n",
      "Test R2: 0.5711012118342235\n",
      "Test Bias (%): 4.419382095336914\n",
      "Test RMSE W/m2: 36.92056655883789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rouhi\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([714])) that is different to the input size (torch.Size([714, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "## pytorch method of doing the same thing \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error as MAPE\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_units=[128, 128, 128, 128, 128, 64], output_units=1, dropout_rate=0.5):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Define the layers dynamically based on the hidden_units list\n",
    "        layers = []\n",
    "        in_features = input_shape\n",
    "        for hidden in hidden_units:\n",
    "            layers.append(nn.Linear(in_features, hidden))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            in_features = hidden\n",
    "\n",
    "        # Add the final output layer\n",
    "        layers.append(nn.Linear(in_features, output_units))\n",
    "\n",
    "        # Use nn.Sequential to combine the layers\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x.to(torch.float32))\n",
    "        x = self.network(x)\n",
    "        return x\n",
    "# def custom_loss_function(predictions, targets, inputs, alpha=0.5):\n",
    "#     mse_loss = nn.MSELoss()(predictions, targets)\n",
    "#     print(\"MSE LOSS=\",mse_loss)\n",
    "#     print(predictions.shape)\n",
    "#     physics_loss = torch.sum(torch.abs(inputs[:, 14].unsqueeze(1)-inputs[:, 15].unsqueeze(1)-(inputs[:, 17].unsqueeze(1)*inputs[:, 18].unsqueeze(1)/predictions)))\n",
    "#     normalized_physics_loss = physics_loss / torch.mean(torch.abs(inputs[:, 14] - inputs[:, 15] - (inputs[:, 17] * inputs[:, 18] / torch.mean(predictions))))\n",
    "\n",
    "#     print(\"Additional LOSS=\",normalized_physics_loss)\n",
    "\n",
    "#     total_loss = mse_loss + alpha * normalized_physics_loss\n",
    "#     return total_loss\n",
    "\n",
    "def train_model(model, criterion, optimizer, x_train, y_train, epochs=3000, batch_size=400):\n",
    "    model.train()\n",
    "    dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    epoch_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "            # loss = criterion(outputs, batch_y, batch_x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Calculate the average loss for the epoch\n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss}\")\n",
    "\n",
    "    return epoch_losses\n",
    "\n",
    "\n",
    "def evaluate_model(model, x_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(x_test)\n",
    "        loss = criterion(outputs, y_test)\n",
    "    return loss.item()\n",
    "\n",
    "def r2_calc(model, x_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(x_test).squeeze()\n",
    "    return r2_score(y_test.numpy(), predictions.numpy())\n",
    "\n",
    "def bias_calc(model, x_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(x_test).squeeze()\n",
    "    return MAPE(y_test.numpy(), predictions.numpy())\n",
    "def rmse_calc(model, x_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(x_test).squeeze()\n",
    "    return rmse(y_test.numpy(), predictions.numpy())\n",
    "# Example usage\n",
    "# Assuming x_train, y_train, x_test, y_test are your training and testing data\n",
    "# and have been converted to torch tensors.\n",
    "\n",
    "input_shape = X_train.shape[1]\n",
    "model = NeuralNetwork(input_shape)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.0005)\n",
    "# optimizer=optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "# train_model(model, custom_loss_function, optimizer, X_train, train_labels)\n",
    "epoch_losses = train_model(model, criterion, optimizer, X_train, train_labels)\n",
    "# Plotting the loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(epoch_losses) + 1), epoch_losses, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss vs. Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "test_loss = evaluate_model(model, X_test, test_labels)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test R2: {r2_calc(model, X_test, test_labels)}\")\n",
    "print(f\"Test Bias (%): {bias_calc(model, X_test, test_labels)}\")\n",
    "print(f\"Test RMSE W/m2: {rmse_calc(model, X_test, test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing with OpenET\n",
    "## Let's compare the results of OpenET vs GEESEBAL vs ML model\n",
    "## Read Open ET results\n",
    "os.chdir(\"D:\\\\Backup\\\\Rouhin_Lenovo\\\\US_project\\\\Untitled_Folder\\\\Data\\\\Volk_merged\\\\ML_datasets\\\\Open_ET\\\\Sen1\")\n",
    "file_list_openet=os.listdir()\n",
    "val_openet=[]\n",
    "for i in range(len(file_list_openet)):\n",
    "    for j in range(len(val)):\n",
    "        if file_list_openet[i].split('.')[0]==val[j][\"Site_name\"].iloc[0]:\n",
    "            tmp=pd.read_csv(file_list_openet[i],parse_dates=[\"Date\"])\n",
    "            val_openet.append(pd.merge(val[j].drop(columns={\"Unnamed: 0_x\"}),tmp,on=\"Date\",how=\"left\"))\n",
    "for i in range(len(val_openet)):\n",
    "    val_openet[i][\"LE_openet\"]=val_openet[i][\"ETa\"]*28.36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(277, 31)\n",
      "(277,)\n"
     ]
    }
   ],
   "source": [
    "## Testing with OpenET \n",
    "features=features=['ALFA', 'Tao_sw_1', 'Rs_down',\n",
    "       'Rl_down', 'Rl_up', 'Rn','VV_norm', \n",
    "       'VH_norm', 'VV_red', 'VV_green', 'VV_blue', 'VV_NIR', 'VV_SWIR1',\n",
    "       'VV_SWIR2', 'VV_NDVI', 'VV_NDWI', 'VH_red', 'VH_green', 'VH_blue',\n",
    "       'VH_NIR', 'VH_SWIR1', 'VH_SWIR2', 'VH_NDVI', 'VH_NDWI', 'VH_LST',\n",
    "       'VV_LST', 'VH-VV',\"Ginst\",\"Hinst\",\"LEinst\",\"Gap\",\"LE_Daily_model\",\"LE_openet\"]\n",
    "labels=[\"LE_closed\"]\n",
    "# ml_df=df[[\"B\",\"R\",\"GR\",\"NIR\",\"SWIR_1\",\"SWIR_2\",\"NDVI\",\"NDWI\",\"ALFA\",\"Rs_down\",\"Rl_down\",\"Rl_up\",\"Rn\",\"Ginst\",\"Hinst\",\"LEinst\",\"LE_closed\",\"LE_Daily_model\"]]\n",
    "# ml_df=ml_df.dropna()\n",
    "# print(ml_df.shape)\n",
    "# features=ml_df[[\"B\",\"R\",\"GR\",\"NIR\",\"SWIR_1\",\"SWIR_2\",\"NDVI\",\"NDWI\",\"ALFA\",\"Rs_down\",\"Rl_down\",\"Rl_up\",\"Rn\",\"Ginst\",\"Hinst\",\"LEinst\",\"LE_Daily_model\"]]\n",
    "# labels=ml_df[\"LE_closed\"]\n",
    "# print(features.shape)\n",
    "## Create a train set to include the geesebal output \n",
    "# train_features_all=pd.concat(train)[features].dropna()\n",
    "test_features_all=pd.concat(val_openet)[features].dropna()\n",
    "# train_features=pd.concat(train)[features].dropna().drop(columns=[\"LE_Daily_model\",\"LE_openet\"])\n",
    "train_features\n",
    "test_features=pd.concat(val_openet)[features].dropna().drop(columns=[\"LE_Daily_model\",\"LE_openet\"])\n",
    "X_test = scaler.transform(test_features)\n",
    "print(X_test.shape)\n",
    "# ## labels\n",
    "test_labels=np.array(pd.concat(val_openet)[pd.concat(val_openet)[\"LE_openet\"].notna()][labels]).squeeze()\n",
    "print(test_labels.shape)\n",
    "# ## labels ## For pytorch \n",
    "X_test=torch.tensor(X_test).to(torch.float32)\n",
    "test_labels=torch.tensor(test_labels).to(torch.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 5410.23779296875\n",
      "Test R2: 0.3523276467439741\n",
      "Test Bias (%): 0.45855191349983215\n",
      "Test RMSE W/m2: 48.292579650878906\n",
      "GEESEBAL accuracy (RMSE)= 59.9497665345855\n",
      "GEESEBAL R2 = 0.0019103599461512966\n",
      "GEESEBAL MAPE = 0.6541946691289369\n",
      "OpenET accuracy (RMSE)= 41.362911343892364\n",
      "OpenET R2 = 0.5248652947604846\n",
      "OpenET MAPE = 0.5041483704220069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rouhi\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([277])) that is different to the input size (torch.Size([277, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "## For pytorch \n",
    "test_loss = evaluate_model(model, X_test, test_labels)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test R2: {r2_calc(model, X_test, test_labels)}\")\n",
    "print(f\"Test Bias (%): {bias_calc(model, X_test, test_labels)}\")\n",
    "print(f\"Test RMSE W/m2: {rmse_calc(model, X_test, test_labels)}\")\n",
    "##--------\n",
    "print(\"GEESEBAL accuracy (RMSE)=\",rmse(test_labels,test_features_all[\"LE_Daily_model\"]) )\n",
    "print(\"GEESEBAL R2 =\",r2_score(test_labels,test_features_all[\"LE_Daily_model\"]) )\n",
    "print(\"GEESEBAL MAPE =\",MAPE(test_labels,test_features_all[\"LE_Daily_model\"]) )\n",
    "print(\"OpenET accuracy (RMSE)=\",rmse(test_labels,test_features_all[\"LE_openet\"]) )\n",
    "print(\"OpenET R2 =\",r2_score(test_labels,test_features_all[\"LE_openet\"]) )\n",
    "print(\"OpenET MAPE =\",MAPE(test_labels,test_features_all[\"LE_openet\"]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(val)):\n",
    "    val[i].to_csv(\"D:\\\\Backup\\\\Rouhin_Lenovo\\\\US_project\\\\Untitled_Folder\\\\Data\\\\Volk_merged\\\\ML_datasets\\\\Val_Data_for_OpenET\\\\Sen1\\\\\"+val[i][\"Site_name\"].iloc[0]+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get a df of train and val station data \n",
    "lat=[]\n",
    "lon=[]\n",
    "elev=[]\n",
    "shape=[]\n",
    "name=[]\n",
    "lc=[]\n",
    "for i in range(len(train)):\n",
    "    lat.append(train[i][\"Lat_volk\"].iloc[0])\n",
    "    lon.append(train[i][\"Lon_volk\"].iloc[0])\n",
    "    elev.append(train[i][\"Elev_volk\"].iloc[0])\n",
    "    lc.append(train[i][\"Land_cover_volk\"].iloc[0])\n",
    "    name.append(train[i][\"Site_name\"].iloc[0])\n",
    "    shape.append(train[i].shape[0])\n",
    "summary_df_train=pd.DataFrame()\n",
    "summary_df_train[\"Lat\"]=lat\n",
    "summary_df_train[\"Lon\"]=lon\n",
    "summary_df_train[\"Elev\"]=elev\n",
    "summary_df_train[\"lc\"]=lc\n",
    "summary_df_train[\"Name\"]=name\n",
    "summary_df_train[\"Points\"]=shape\n",
    "###-----------------\n",
    "lat=[]\n",
    "lon=[]\n",
    "elev=[]\n",
    "shape=[]\n",
    "name=[]\n",
    "lc=[]\n",
    "for i in range(len(val)):\n",
    "    lat.append(val[i][\"Lat_volk\"].iloc[0])\n",
    "    lon.append(val[i][\"Lon_volk\"].iloc[0])\n",
    "    elev.append(val[i][\"Elev_volk\"].iloc[0])\n",
    "    lc.append(val[i][\"Land_cover_volk\"].iloc[0])\n",
    "    name.append(val[i][\"Site_name\"].iloc[0])\n",
    "    shape.append(val[i].shape[0])\n",
    "summary_df_val=pd.DataFrame()\n",
    "summary_df_val[\"Lat\"]=lat\n",
    "summary_df_val[\"Lon\"]=lon\n",
    "summary_df_val[\"Elev\"]=elev\n",
    "summary_df_val[\"lc\"]=lc\n",
    "summary_df_val[\"Name\"]=name\n",
    "summary_df_val[\"Points\"]=shape\n",
    "# summary_df_train.to_csv(\"D:\\\\Backup\\\\Rouhin_Lenovo\\\\US_project\\\\Untitled_Folder\\\\Data\\\\Volk_merged\\\\ML_datasets\\\\Train_test_sites\\\\ML_Sen1\\\\train_seed_sen_3.csv\")\n",
    "# summary_df_val.to_csv(\"D:\\\\Backup\\\\Rouhin_Lenovo\\\\US_project\\\\Untitled_Folder\\\\Data\\\\Volk_merged\\\\ML_datasets\\\\Train_test_sites\\\\ML_Sen1\\\\val_seed_sen_3.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
